# ã‚­ãƒ£ãƒã‚¯ãƒ©å£²ä¸Šäºˆæ¸¬ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ§‹ç¯‰ã‚¬ã‚¤ãƒ‰

æ¥å®¢æ¥­ã«ãŠã‘ã‚‹å£²ä¸Šåˆ†æã¨ã‚­ãƒ£ã‚¹ãƒˆç®¡ç†ã¯ã€çµŒå–¶ã®é‡è¦ãªè¦ç´ ã§ã™ã€‚ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€Excelãƒ‡ãƒ¼ã‚¿ã‚’åŸºã«ã—ãŸå£²ä¸Šäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã¨ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®ä½œã‚Šæ–¹ã‚’å®Ÿè·µçš„ã«è§£èª¬ã—ã¾ã™ã€‚

## ç›®æ¬¡
1. [ãƒ‡ãƒ¼ã‚¿è¨­è¨ˆã¨Excelå…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ](#1-ãƒ‡ãƒ¼ã‚¿è¨­è¨ˆã¨excelå…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ)
2. [Pythonã§ã®ãƒ‡ãƒ¼ã‚¿å‡¦ç†](#2-pythonã§ã®ãƒ‡ãƒ¼ã‚¿å‡¦ç†)
3. [å£²ä¸Šäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰](#3-å£²ä¸Šäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰)
4. [ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®ä½œæˆ](#4-ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®ä½œæˆ)
5. [é‹ç”¨ã¨ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹](#5-é‹ç”¨ã¨ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹)

---

## 1. ãƒ‡ãƒ¼ã‚¿è¨­è¨ˆã¨Excelå…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

### åŸºæœ¬ãƒ‡ãƒ¼ã‚¿æ§‹é€ 

#### ã‚­ãƒ£ã‚¹ãƒˆãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆcast_master.xlsxï¼‰
```
| ã‚­ãƒ£ã‚¹ãƒˆID | åå‰ | å…¥åº—æ—¥ | ãƒ©ãƒ³ã‚¯ | åŸºæœ¬æ™‚çµ¦ | çµŒé¨“å¹´æ•° | å¹´é½¢ | ã‚¿ã‚¤ãƒ— |
|-----------|------|--------|--------|----------|----------|------|--------|
| C001      | èŠ±éŸ³ | 2023-01-15 | S | 3000 | 2.5 | 22 | å¯æ„›ã„ç³» |
| C002      | ç¾å’² | 2022-06-01 | A | 2500 | 1.8 | 24 | ãŠå§‰ã•ã‚“ç³» |
| C003      | éº—å¥ˆ | 2023-03-10 | B | 2000 | 0.8 | 20 | æ¸…æ¥šç³» |
```

#### æ—¥æ¬¡å£²ä¸Šãƒ‡ãƒ¼ã‚¿ï¼ˆdaily_sales.xlsxï¼‰
```
| æ—¥ä»˜ | ã‚­ãƒ£ã‚¹ãƒˆID | å‡ºå‹¤æ™‚é–“ | é€€å‹¤æ™‚é–“ | æ¥å®¢æ™‚é–“ | åŒä¼´æ•° | æŒ‡åæ•° | ãƒ‰ãƒªãƒ³ã‚¯æ•° | ãƒœãƒˆãƒ«æ•° | å£²ä¸Šé‡‘é¡ | å¤©æ°— | æ›œæ—¥ | ã‚¤ãƒ™ãƒ³ãƒˆ |
|------|-----------|----------|----------|----------|--------|--------|------------|----------|----------|------|------|----------|
| 2024-01-15 | C001 | 20:00 | 01:00 | 5.0 | 1 | 3 | 12 | 2 | 180000 | æ™´ã‚Œ | æœˆ | æ–°å¹´ä¼š |
| 2024-01-15 | C002 | 19:30 | 01:30 | 6.0 | 0 | 5 | 15 | 1 | 220000 | æ™´ã‚Œ | æœˆ | æ–°å¹´ä¼š |
```

#### é¡§å®¢ãƒ‡ãƒ¼ã‚¿ï¼ˆcustomer_data.xlsxï¼‰
```
| é¡§å®¢ID | å¹´é½¢ | è·æ¥­ | å¹´åãƒ¬ãƒ³ã‚¸ | æ¥åº—é »åº¦ | å¹³å‡å˜ä¾¡ | å¥½ã¿ã‚¿ã‚¤ãƒ— | æ‹…å½“ã‚­ãƒ£ã‚¹ãƒˆ |
|--------|------|------|------------|----------|----------|------------|--------------|
| CU001 | 45 | ä¼šç¤¾å“¡ | 600-800ä¸‡ | æœˆ2å› | 50000 | ãŠå§‰ã•ã‚“ç³» | C002 |
| CU002 | 38 | çµŒå–¶è€… | 1000ä¸‡ä»¥ä¸Š | é€±1å› | 120000 | å¯æ„›ã„ç³» | C001 |
```

### Excelãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ä½œæˆ

```python
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import openpyxl
from openpyxl.styles import Font, PatternFill, Alignment

def create_excel_templates():
    """å£²ä¸Šç®¡ç†ç”¨ã®Excelãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆ"""
    
    # ã‚­ãƒ£ã‚¹ãƒˆãƒã‚¹ã‚¿ãƒ¼ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
    cast_columns = [
        'ã‚­ãƒ£ã‚¹ãƒˆID', 'åå‰', 'å…¥åº—æ—¥', 'ãƒ©ãƒ³ã‚¯', 'åŸºæœ¬æ™‚çµ¦', 
        'çµŒé¨“å¹´æ•°', 'å¹´é½¢', 'ã‚¿ã‚¤ãƒ—', 'ç‰¹æŠ€', 'å‡ºèº«åœ°'
    ]
    
    # æ—¥æ¬¡å£²ä¸Šã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
    daily_columns = [
        'æ—¥ä»˜', 'ã‚­ãƒ£ã‚¹ãƒˆID', 'å‡ºå‹¤æ™‚é–“', 'é€€å‹¤æ™‚é–“', 'æ¥å®¢æ™‚é–“',
        'åŒä¼´æ•°', 'æŒ‡åæ•°', 'ãƒ‰ãƒªãƒ³ã‚¯æ•°', 'ãƒœãƒˆãƒ«æ•°', 'å£²ä¸Šé‡‘é¡',
        'å¤©æ°—', 'æ›œæ—¥', 'ã‚¤ãƒ™ãƒ³ãƒˆ', 'å®¢å±¤', 'å‚™è€ƒ'
    ]
    
    # é¡§å®¢ãƒ‡ãƒ¼ã‚¿ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
    customer_columns = [
        'é¡§å®¢ID', 'åå‰', 'å¹´é½¢', 'è·æ¥­', 'å¹´åãƒ¬ãƒ³ã‚¸', 'æ¥åº—é »åº¦',
        'å¹³å‡å˜ä¾¡', 'å¥½ã¿ã‚¿ã‚¤ãƒ—', 'æ‹…å½“ã‚­ãƒ£ã‚¹ãƒˆ', 'åˆå›æ¥åº—æ—¥', 'æœ€çµ‚æ¥åº—æ—¥'
    ]
    
    # Excelãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    with pd.ExcelWriter('ã‚­ãƒ£ãƒã‚¯ãƒ©å£²ä¸Šç®¡ç†ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ.xlsx', engine='openpyxl') as writer:
        # ç©ºã®DataFrameã‚’ä½œæˆã—ã¦ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¨ã—ã¦ä¿å­˜
        pd.DataFrame(columns=cast_columns).to_excel(writer, sheet_name='ã‚­ãƒ£ã‚¹ãƒˆãƒã‚¹ã‚¿ãƒ¼', index=False)
        pd.DataFrame(columns=daily_columns).to_excel(writer, sheet_name='æ—¥æ¬¡å£²ä¸Š', index=False)
        pd.DataFrame(columns=customer_columns).to_excel(writer, sheet_name='é¡§å®¢ãƒ‡ãƒ¼ã‚¿', index=False)
        
        # å…¥åŠ›ä¾‹ã‚·ãƒ¼ãƒˆã‚‚ä½œæˆ
        create_sample_data().to_excel(writer, sheet_name='å…¥åŠ›ä¾‹', index=False)
    
    print("âœ… Excelãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆã—ã¾ã—ãŸ")

def create_sample_data():
    """ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ"""
    
    # ã‚µãƒ³ãƒ—ãƒ«ã®æ—¥æ¬¡å£²ä¸Šãƒ‡ãƒ¼ã‚¿
    sample_data = []
    cast_ids = ['C001', 'C002', 'C003', 'C004', 'C005']
    
    for i in range(30):  # 30æ—¥åˆ†ã®ã‚µãƒ³ãƒ—ãƒ«
        date = datetime.now() - timedelta(days=i)
        
        for cast_id in np.random.choice(cast_ids, size=np.random.randint(2, 4), replace=False):
            sample_data.append({
                'æ—¥ä»˜': date.strftime('%Y-%m-%d'),
                'ã‚­ãƒ£ã‚¹ãƒˆID': cast_id,
                'å‡ºå‹¤æ™‚é–“': '20:00',
                'é€€å‹¤æ™‚é–“': '01:00',
                'æ¥å®¢æ™‚é–“': round(np.random.normal(4.5, 1.5), 1),
                'åŒä¼´æ•°': np.random.poisson(0.3),
                'æŒ‡åæ•°': np.random.poisson(2.5),
                'ãƒ‰ãƒªãƒ³ã‚¯æ•°': np.random.poisson(8),
                'ãƒœãƒˆãƒ«æ•°': np.random.poisson(1.2),
                'å£²ä¸Šé‡‘é¡': np.random.normal(150000, 50000),
                'å¤©æ°—': np.random.choice(['æ™´ã‚Œ', 'æ›‡ã‚Š', 'é›¨']),
                'æ›œæ—¥': date.strftime('%a'),
                'ã‚¤ãƒ™ãƒ³ãƒˆ': np.random.choice(['ãªã—', 'å¿˜å¹´ä¼š', 'æ­“é€è¿ä¼š', 'ã‚¯ãƒªã‚¹ãƒã‚¹'], p=[0.7, 0.1, 0.1, 0.1])
            })
    
    return pd.DataFrame(sample_data)

# ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½œæˆ
create_excel_templates()
```

---

## 2. Pythonã§ã®ãƒ‡ãƒ¼ã‚¿å‡¦ç†

### ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
import warnings
warnings.filterwarnings('ignore')

class CabaretDataProcessor:
    def __init__(self):
        self.scaler = StandardScaler()
        self.label_encoders = {}
        
    def load_excel_data(self, file_path):
        """Excelãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å…¨ã‚·ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿"""
        try:
            # å…¨ã‚·ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿
            excel_file = pd.ExcelFile(file_path)
            data_dict = {}
            
            for sheet_name in excel_file.sheet_names:
                data_dict[sheet_name] = pd.read_excel(file_path, sheet_name=sheet_name)
                print(f"âœ… {sheet_name}ã‚·ãƒ¼ãƒˆèª­ã¿è¾¼ã¿å®Œäº†: {data_dict[sheet_name].shape}")
            
            return data_dict
            
        except Exception as e:
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def preprocess_daily_sales(self, df):
        """æ—¥æ¬¡å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†"""
        
        df = df.copy()
        
        # æ—¥ä»˜å‹ã«å¤‰æ›
        df['æ—¥ä»˜'] = pd.to_datetime(df['æ—¥ä»˜'])
        
        # æ™‚é–“ç³»ç‰¹å¾´é‡ã®ä½œæˆ
        df['å¹´'] = df['æ—¥ä»˜'].dt.year
        df['æœˆ'] = df['æ—¥ä»˜'].dt.month
        df['æ—¥'] = df['æ—¥ä»˜'].dt.day
        df['æ›œæ—¥ç•ªå·'] = df['æ—¥ä»˜'].dt.dayofweek  # 0=æœˆæ›œæ—¥
        df['é€±æœ«ãƒ•ãƒ©ã‚°'] = (df['æ›œæ—¥ç•ªå·'] >= 5).astype(int)
        
        # æœˆæœ«ãƒ»æœˆåˆãƒ•ãƒ©ã‚°
        df['æœˆæœ«ãƒ•ãƒ©ã‚°'] = (df['æ—¥ä»˜'].dt.day >= 25).astype(int)
        df['æœˆåˆãƒ•ãƒ©ã‚°'] = (df['æ—¥ä»˜'].dt.day <= 5).astype(int)
        
        # å£²ä¸ŠåŠ¹ç‡æŒ‡æ¨™
        df['æ™‚é–“å˜ä¾¡'] = df['å£²ä¸Šé‡‘é¡'] / (df['æ¥å®¢æ™‚é–“'] + 0.1)  # ã‚¼ãƒ­é™¤ç®—å¯¾ç­–
        df['æŒ‡åå˜ä¾¡'] = df['å£²ä¸Šé‡‘é¡'] / (df['æŒ‡åæ•°'] + 1)
        
        # æ¥å®¢å¯†åº¦
        df['ãƒ‰ãƒªãƒ³ã‚¯å¯†åº¦'] = df['ãƒ‰ãƒªãƒ³ã‚¯æ•°'] / (df['æ¥å®¢æ™‚é–“'] + 0.1)
        df['æŒ‡åå¯†åº¦'] = df['æŒ‡åæ•°'] / (df['æ¥å®¢æ™‚é–“'] + 0.1)
        
        # ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        categorical_columns = ['å¤©æ°—', 'ã‚¤ãƒ™ãƒ³ãƒˆ', 'ã‚­ãƒ£ã‚¹ãƒˆID']
        
        for col in categorical_columns:
            if col in df.columns:
                if col not in self.label_encoders:
                    self.label_encoders[col] = LabelEncoder()
                    df[f'{col}_encoded'] = self.label_encoders[col].fit_transform(df[col].astype(str))
                else:
                    df[f'{col}_encoded'] = self.label_encoders[col].transform(df[col].astype(str))
        
        print("âœ… æ—¥æ¬¡å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†å®Œäº†")
        return df
    
    def create_aggregated_features(self, df):
        """é›†ç´„ç‰¹å¾´é‡ã‚’ä½œæˆ"""
        
        # ã‚­ãƒ£ã‚¹ãƒˆåˆ¥ã®çµ±è¨ˆé‡
        cast_stats = df.groupby('ã‚­ãƒ£ã‚¹ãƒˆID').agg({
            'å£²ä¸Šé‡‘é¡': ['mean', 'std', 'max', 'count'],
            'æŒ‡åæ•°': ['mean', 'sum'],
            'åŒä¼´æ•°': ['mean', 'sum'],
            'æ¥å®¢æ™‚é–“': ['mean'],
            'æ™‚é–“å˜ä¾¡': ['mean']
        }).round(2)
        
        # ã‚«ãƒ©ãƒ åã‚’å¹³å¦åŒ–
        cast_stats.columns = ['_'.join(col).strip() for col in cast_stats.columns]
        cast_stats = cast_stats.reset_index()
        
        # å…ƒãƒ‡ãƒ¼ã‚¿ã«ãƒãƒ¼ã‚¸
        df = df.merge(cast_stats, on='ã‚­ãƒ£ã‚¹ãƒˆID', how='left', suffixes=('', '_avg'))
        
        # éå»Næ—¥é–“ã®å£²ä¸Šãƒˆãƒ¬ãƒ³ãƒ‰
        df = df.sort_values(['ã‚­ãƒ£ã‚¹ãƒˆID', 'æ—¥ä»˜'])
        df['å£²ä¸Š_7æ—¥å¹³å‡'] = df.groupby('ã‚­ãƒ£ã‚¹ãƒˆID')['å£²ä¸Šé‡‘é¡'].rolling(window=7, min_periods=1).mean().reset_index(0, drop=True)
        df['å£²ä¸Š_30æ—¥å¹³å‡'] = df.groupby('ã‚­ãƒ£ã‚¹ãƒˆID')['å£²ä¸Šé‡‘é¡'].rolling(window=30, min_periods=1).mean().reset_index(0, drop=True)
        
        print("âœ… é›†ç´„ç‰¹å¾´é‡ã®ä½œæˆå®Œäº†")
        return df

# ä½¿ç”¨ä¾‹
processor = CabaretDataProcessor()

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§å‹•ä½œç¢ºèª
sample_df = create_sample_data()
processed_df = processor.preprocess_daily_sales(sample_df)
final_df = processor.create_aggregated_features(processed_df)

print("ğŸ“Š å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª:")
print(final_df.head())
print(f"ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {final_df.shape}")
```

---

## 3. å£²ä¸Šäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰

### äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ

```python
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.ensemble import GradientBoostingRegressor
import joblib

class SalesPredictionModel:
    def __init__(self):
        self.models = {}
        self.feature_importance = {}
        self.scaler = StandardScaler()
        
    def prepare_features(self, df):
        """äºˆæ¸¬ç”¨ã®ç‰¹å¾´é‡ã‚’æº–å‚™"""
        
        # äºˆæ¸¬ã«ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ã‚’é¸æŠ
        feature_columns = [
            'æ¥å®¢æ™‚é–“', 'æŒ‡åæ•°', 'åŒä¼´æ•°', 'ãƒ‰ãƒªãƒ³ã‚¯æ•°', 'ãƒœãƒˆãƒ«æ•°',
            'æ›œæ—¥ç•ªå·', 'é€±æœ«ãƒ•ãƒ©ã‚°', 'æœˆæœ«ãƒ•ãƒ©ã‚°', 'æœˆåˆãƒ•ãƒ©ã‚°',
            'å¤©æ°—_encoded', 'ã‚¤ãƒ™ãƒ³ãƒˆ_encoded', 'ã‚­ãƒ£ã‚¹ãƒˆID_encoded',
            'å£²ä¸Šé‡‘é¡_mean', 'æŒ‡åæ•°_mean', 'æ™‚é–“å˜ä¾¡_mean',
            'å£²ä¸Š_7æ—¥å¹³å‡', 'å£²ä¸Š_30æ—¥å¹³å‡'
        ]
        
        # åˆ©ç”¨å¯èƒ½ãªç‰¹å¾´é‡ã®ã¿ã‚’é¸æŠ
        available_features = [col for col in feature_columns if col in df.columns]
        
        X = df[available_features].copy()
        
        # æ¬ æå€¤ã‚’å¹³å‡å€¤ã§è£œå®Œ
        X = X.fillna(X.mean())
        
        return X, available_features
    
    def train_models(self, df, target_column='å£²ä¸Šé‡‘é¡'):
        """è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’"""
        
        X, feature_names = self.prepare_features(df)
        y = df[target_column]
        
        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # ç‰¹å¾´é‡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©
        models_config = {
            'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
            'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
            'Linear Regression': LinearRegression()
        }
        
        results = {}
        
        for name, model in models_config.items():
            print(f"\nğŸ¤– {name}ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ä¸­...")
            
            # ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®è¦å¦ã‚’åˆ¤æ–­
            if name == 'Linear Regression':
                model.fit(X_train_scaled, y_train)
                y_pred = model.predict(X_test_scaled)
            else:
                model.fit(X_train, y_train)
                y_pred = model.predict(X_test)
            
            # è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
            mse = mean_squared_error(y_test, y_pred)
            rmse = np.sqrt(mse)
            mae = mean_absolute_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)
            
            results[name] = {
                'model': model,
                'rmse': rmse,
                'mae': mae,
                'r2': r2,
                'predictions': y_pred
            }
            
            # ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆRandom Forestã¨Gradient Boostingã®ã¿ï¼‰
            if hasattr(model, 'feature_importances_'):
                self.feature_importance[name] = dict(
                    zip(feature_names, model.feature_importances_)
                )
            
            print(f"âœ… RMSE: {rmse:,.0f}, MAE: {mae:,.0f}, RÂ²: {r2:.3f}")
        
        self.models = results
        self.feature_names = feature_names
        
        # æœ€é©ãƒ¢ãƒ‡ãƒ«ã®é¸æŠï¼ˆRÂ²ãŒæœ€é«˜ã®ã‚‚ã®ï¼‰
        best_model_name = max(results.keys(), key=lambda k: results[k]['r2'])
        self.best_model = results[best_model_name]['model']
        self.best_model_name = best_model_name
        
        print(f"\nğŸ† æœ€é©ãƒ¢ãƒ‡ãƒ«: {best_model_name} (RÂ²: {results[best_model_name]['r2']:.3f})")
        
        return results
    
    def predict_sales(self, new_data):
        """æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦å£²ä¸Šäºˆæ¸¬"""
        
        X_new, _ = self.prepare_features(new_data)
        
        if self.best_model_name == 'Linear Regression':
            X_new_scaled = self.scaler.transform(X_new)
            predictions = self.best_model.predict(X_new_scaled)
        else:
            predictions = self.best_model.predict(X_new)
        
        return predictions
    
    def save_model(self, file_path):
        """ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜"""
        joblib.dump({
            'model': self.best_model,
            'scaler': self.scaler,
            'feature_names': self.feature_names,
            'model_name': self.best_model_name
        }, file_path)
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {file_path}")

# ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã®å®Ÿè¡Œ
predictor = SalesPredictionModel()
model_results = predictor.train_models(final_df)

# ç‰¹å¾´é‡é‡è¦åº¦ã®å¯è¦–åŒ–
def plot_feature_importance(predictor, model_name='Random Forest'):
    """ç‰¹å¾´é‡é‡è¦åº¦ã‚’å¯è¦–åŒ–"""
    
    if model_name in predictor.feature_importance:
        importance_dict = predictor.feature_importance[model_name]
        
        # é‡è¦åº¦é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_features = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)
        
        features, importance = zip(*sorted_features[:10])  # ä¸Šä½10å€‹
        
        plt.figure(figsize=(10, 6))
        plt.barh(range(len(features)), importance)
        plt.yticks(range(len(features)), features)
        plt.xlabel('é‡è¦åº¦')
        plt.title(f'{model_name} - ç‰¹å¾´é‡é‡è¦åº¦ Top 10')
        plt.gca().invert_yaxis()
        plt.tight_layout()
        plt.show()

plot_feature_importance(predictor)
```

### äºˆæ¸¬ç²¾åº¦ã®æ¤œè¨¼

```python
def validate_predictions(predictor, df):
    """äºˆæ¸¬ç²¾åº¦ã‚’è©³ç´°ã«æ¤œè¨¼"""
    
    # å®Ÿéš›ã®äºˆæ¸¬å®Ÿè¡Œ
    predictions = predictor.predict_sales(df)
    actual = df['å£²ä¸Šé‡‘é¡'].values
    
    # æ®‹å·®åˆ†æ
    residuals = actual - predictions
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # äºˆæ¸¬å€¤ vs å®Ÿæ¸¬å€¤
    axes[0, 0].scatter(predictions, actual, alpha=0.6)
    axes[0, 0].plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'r--', lw=2)
    axes[0, 0].set_xlabel('äºˆæ¸¬å€¤')
    axes[0, 0].set_ylabel('å®Ÿæ¸¬å€¤')
    axes[0, 0].set_title('äºˆæ¸¬å€¤ vs å®Ÿæ¸¬å€¤')
    
    # æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ
    axes[0, 1].scatter(predictions, residuals, alpha=0.6)
    axes[0, 1].axhline(y=0, color='r', linestyle='--')
    axes[0, 1].set_xlabel('äºˆæ¸¬å€¤')
    axes[0, 1].set_ylabel('æ®‹å·®')
    axes[0, 1].set_title('æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ')
    
    # æ®‹å·®ã®åˆ†å¸ƒ
    axes[1, 0].hist(residuals, bins=30, alpha=0.7)
    axes[1, 0].set_xlabel('æ®‹å·®')
    axes[1, 0].set_ylabel('é »åº¦')
    axes[1, 0].set_title('æ®‹å·®ã®åˆ†å¸ƒ')
    
    # æ™‚ç³»åˆ—ã§ã®äºˆæ¸¬ç²¾åº¦
    if 'æ—¥ä»˜' in df.columns:
        df_with_pred = df.copy()
        df_with_pred['äºˆæ¸¬å€¤'] = predictions
        daily_actual = df_with_pred.groupby('æ—¥ä»˜')['å£²ä¸Šé‡‘é¡'].sum()
        daily_pred = df_with_pred.groupby('æ—¥ä»˜')['äºˆæ¸¬å€¤'].sum()
        
        axes[1, 1].plot(daily_actual.index, daily_actual.values, label='å®Ÿæ¸¬å€¤', marker='o')
        axes[1, 1].plot(daily_pred.index, daily_pred.values, label='äºˆæ¸¬å€¤', marker='s')
        axes[1, 1].set_xlabel('æ—¥ä»˜')
        axes[1, 1].set_ylabel('æ—¥æ¬¡å£²ä¸Šåˆè¨ˆ')
        axes[1, 1].set_title('æ—¥æ¬¡å£²ä¸Šï¼šå®Ÿæ¸¬å€¤ vs äºˆæ¸¬å€¤')
        axes[1, 1].legend()
        axes[1, 1].tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    plt.show()
    
    # ç²¾åº¦æŒ‡æ¨™ã®è¡¨ç¤º
    mse = mean_squared_error(actual, predictions)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(actual, predictions)
    r2 = r2_score(actual, predictions)
    
    print("ğŸ“Š äºˆæ¸¬ç²¾åº¦æŒ‡æ¨™:")
    print(f"  RMSE: Â¥{rmse:,.0f}")
    print(f"  MAE: Â¥{mae:,.0f}")
    print(f"  RÂ²: {r2:.3f}")
    print(f"  å¹³å‡çµ¶å¯¾èª¤å·®ç‡: {(mae/actual.mean())*100:.1f}%")

validate_predictions(predictor, final_df)
```

---

## 4. ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®ä½œæˆ

### Streamlitã‚’ä½¿ç”¨ã—ãŸWebãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

```python
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import datetime

def create_dashboard():
    """Streamlitãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    
    st.set_page_config(page_title="ã‚­ãƒ£ãƒã‚¯ãƒ©å£²ä¸Šåˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰", layout="wide")
    
    st.title("ğŸ¾ å£²ä¸Šåˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")
    st.sidebar.title("è¨­å®šãƒ¡ãƒ‹ãƒ¥ãƒ¼")
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã®è¨­å®š
    st.sidebar.header("ğŸ“… æœŸé–“è¨­å®š")
    start_date = st.sidebar.date_input("é–‹å§‹æ—¥", datetime.date.today() - datetime.timedelta(days=30))
    end_date = st.sidebar.date_input("çµ‚äº†æ—¥", datetime.date.today())
    
    # ã‚­ãƒ£ã‚¹ãƒˆé¸æŠ
    st.sidebar.header("ğŸ‘¥ ã‚­ãƒ£ã‚¹ãƒˆé¸æŠ")
    available_casts = ['å…¨ä½“', 'C001', 'C002', 'C003', 'C004', 'C005']
    selected_cast = st.sidebar.selectbox("ã‚­ãƒ£ã‚¹ãƒˆ", available_casts)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯å¤–éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
    @st.cache_data
    def load_data():
        return create_sample_data()
    
    df = load_data()
    df['æ—¥ä»˜'] = pd.to_datetime(df['æ—¥ä»˜'])
    
    # æœŸé–“ã§ãƒ•ã‚£ãƒ«ã‚¿
    mask = (df['æ—¥ä»˜'] >= pd.to_datetime(start_date)) & (df['æ—¥ä»˜'] <= pd.to_datetime(end_date))
    filtered_df = df[mask]
    
    # ã‚­ãƒ£ã‚¹ãƒˆã§ãƒ•ã‚£ãƒ«ã‚¿
    if selected_cast != 'å…¨ä½“':
        filtered_df = filtered_df[filtered_df['ã‚­ãƒ£ã‚¹ãƒˆID'] == selected_cast]
    
    # ãƒ¡ã‚¤ãƒ³è¡¨ç¤ºã‚¨ãƒªã‚¢
    col1, col2, col3, col4 = st.columns(4)
    
    # KPIãƒ¡ãƒˆãƒªã‚¯ã‚¹
    with col1:
        total_sales = filtered_df['å£²ä¸Šé‡‘é¡'].sum()
        st.metric("ç·å£²ä¸Š", f"Â¥{total_sales:,.0f}")
    
    with col2:
        avg_sales = filtered_df['å£²ä¸Šé‡‘é¡'].mean()
        st.metric("å¹³å‡æ—¥æ¬¡å£²ä¸Š", f"Â¥{avg_sales:,.0f}")
    
    with col3:
        total_customers = filtered_df['æŒ‡åæ•°'].sum()
        st.metric("ç·æŒ‡åæ•°", f"{total_customers:,.0f}")
    
    with col4:
        avg_hourly_rate = (filtered_df['å£²ä¸Šé‡‘é¡'] / filtered_df['æ¥å®¢æ™‚é–“']).mean()
        st.metric("å¹³å‡æ™‚é–“å˜ä¾¡", f"Â¥{avg_hourly_rate:,.0f}")
    
    # ã‚°ãƒ©ãƒ•ã‚¨ãƒªã‚¢
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“ˆ å£²ä¸Šãƒˆãƒ¬ãƒ³ãƒ‰", "ğŸ‘¥ ã‚­ãƒ£ã‚¹ãƒˆåˆ†æ", "ğŸ¯ äºˆæ¸¬", "ğŸ“Š è©³ç´°åˆ†æ"])
    
    with tab1:
        create_sales_trend_chart(filtered_df)
    
    with tab2:
        create_cast_analysis_chart(filtered_df)
    
    with tab3:
        create_prediction_interface(predictor)
    
    with tab4:
        create_detailed_analysis(filtered_df)

def create_sales_trend_chart(df):
    """å£²ä¸Šãƒˆãƒ¬ãƒ³ãƒ‰ãƒãƒ£ãƒ¼ãƒˆã‚’ä½œæˆ"""
    
    st.subheader("ğŸ“ˆ æ—¥æ¬¡å£²ä¸Šãƒˆãƒ¬ãƒ³ãƒ‰")
    
    # æ—¥æ¬¡å£²ä¸Šé›†è¨ˆ
    daily_sales = df.groupby('æ—¥ä»˜')['å£²ä¸Šé‡‘é¡'].sum().reset_index()
    
    fig = px.line(daily_sales, x='æ—¥ä»˜', y='å£²ä¸Šé‡‘é¡', 
                  title="æ—¥æ¬¡å£²ä¸Šæ¨ç§»",
                  labels={'å£²ä¸Šé‡‘é¡': 'å£²ä¸Šé‡‘é¡ (Â¥)', 'æ—¥ä»˜': 'æ—¥ä»˜'})
    
    fig.update_layout(height=400)
    st.plotly_chart(fig, use_container_width=True)
    
    # æ›œæ—¥åˆ¥å£²ä¸Š
    df['æ›œæ—¥'] = df['æ—¥ä»˜'].dt.day_name()
    weekday_sales = df.groupby('æ›œæ—¥')['å£²ä¸Šé‡‘é¡'].mean().reindex([
        'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'
    ])
    
    fig2 = px.bar(x=weekday_sales.index, y=weekday_sales.values,
                  title="æ›œæ—¥åˆ¥å¹³å‡å£²ä¸Š",
                  labels={'x': 'æ›œæ—¥', 'y': 'å¹³å‡å£²ä¸Š (Â¥)'})
    
    fig2.update_layout(height=400)
    st.plotly_chart(fig2, use_container_width=True)

def create_cast_analysis_chart(df):
    """ã‚­ãƒ£ã‚¹ãƒˆåˆ†æãƒãƒ£ãƒ¼ãƒˆã‚’ä½œæˆ"""
    
    st.subheader("ğŸ‘¥ ã‚­ãƒ£ã‚¹ãƒˆåˆ¥åˆ†æ")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # ã‚­ãƒ£ã‚¹ãƒˆåˆ¥å£²ä¸Š
        cast_sales = df.groupby('ã‚­ãƒ£ã‚¹ãƒˆID')['å£²ä¸Šé‡‘é¡'].sum().sort_values(ascending=False)
        
        fig = px.bar(x=cast_sales.index, y=cast_sales.values,
                     title="ã‚­ãƒ£ã‚¹ãƒˆåˆ¥ç·å£²ä¸Š",
                     labels={'x': 'ã‚­ãƒ£ã‚¹ãƒˆID', 'y': 'ç·å£²ä¸Š (Â¥)'})
        
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        # ã‚­ãƒ£ã‚¹ãƒˆåˆ¥æ™‚é–“å˜ä¾¡
        cast_hourly = (df.groupby('ã‚­ãƒ£ã‚¹ãƒˆID')['å£²ä¸Šé‡‘é¡'].sum() / 
                      df.groupby('ã‚­ãƒ£ã‚¹ãƒˆID')['æ¥å®¢æ™‚é–“'].sum()).sort_values(ascending=False)
        
        fig2 = px.bar(x=cast_hourly.index, y=cast_hourly.values,
                      title="ã‚­ãƒ£ã‚¹ãƒˆåˆ¥æ™‚é–“å˜ä¾¡",
                      labels={'x': 'ã‚­ãƒ£ã‚¹ãƒˆID', 'y': 'æ™‚é–“å˜ä¾¡ (Â¥/h)'})
        
        st.plotly_chart(fig2, use_container_width=True)
    
    # æ•£å¸ƒå›³ï¼šæŒ‡åæ•° vs å£²ä¸Š
    fig3 = px.scatter(df, x='æŒ‡åæ•°', y='å£²ä¸Šé‡‘é¡', color='ã‚­ãƒ£ã‚¹ãƒˆID',
                      title="æŒ‡åæ•° vs å£²ä¸Šé‡‘é¡",
                      labels={'æŒ‡åæ•°': 'æŒ‡åæ•°', 'å£²ä¸Šé‡‘é¡': 'å£²ä¸Šé‡‘é¡ (Â¥)'})
    
    st.plotly_chart(fig3, use_container_width=True)

def create_prediction_interface(predictor):
    """äºˆæ¸¬ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’ä½œæˆ"""
    
    st.subheader("ğŸ¯ å£²ä¸Šäºˆæ¸¬")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("### äºˆæ¸¬æ¡ä»¶ã‚’å…¥åŠ›")
        
        # å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
        cast_id = st.selectbox("ã‚­ãƒ£ã‚¹ãƒˆID", ['C001', 'C002', 'C003', 'C004', 'C005'])
        work_hours = st.slider("æ¥å®¢æ™‚é–“", 1.0, 8.0, 5.0, 0.5)
        appointments = st.slider("æŒ‡åæ•°", 0, 10, 3)
        companion = st.slider("åŒä¼´æ•°", 0, 3, 0)
        drinks = st.slider("ãƒ‰ãƒªãƒ³ã‚¯æ•°", 0, 20, 8)
        bottles = st.slider("ãƒœãƒˆãƒ«æ•°", 0, 5, 1)
        
        weather = st.selectbox("å¤©æ°—", ['æ™´ã‚Œ', 'æ›‡ã‚Š', 'é›¨'])
        is_weekend = st.checkbox("é€±æœ«")
        has_event = st.selectbox("ã‚¤ãƒ™ãƒ³ãƒˆ", ['ãªã—', 'å¿˜å¹´ä¼š', 'æ­“é€è¿ä¼š', 'ã‚¯ãƒªã‚¹ãƒã‚¹'])
        
        if st.button("å£²ä¸Šäºˆæ¸¬å®Ÿè¡Œ"):
            # äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
            prediction_data = pd.DataFrame({
                'ã‚­ãƒ£ã‚¹ãƒˆID': [cast_id],
                'æ¥å®¢æ™‚é–“': [work_hours],
                'æŒ‡åæ•°': [appointments],
                'åŒä¼´æ•°': [companion],
                'ãƒ‰ãƒªãƒ³ã‚¯æ•°': [drinks],
                'ãƒœãƒˆãƒ«æ•°': [bottles],
                'å¤©æ°—': [weather],
                'ã‚¤ãƒ™ãƒ³ãƒˆ': [has_event],
                'é€±æœ«ãƒ•ãƒ©ã‚°': [1 if is_weekend else 0],
                'æœˆæœ«ãƒ•ãƒ©ã‚°': [0],  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                'æœˆåˆãƒ•ãƒ©ã‚°': [0],  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                'æ›œæ—¥ç•ªå·': [5 if is_weekend else 2]  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            })
            
            # ãƒ€ãƒŸãƒ¼ã®äºˆæ¸¬ï¼ˆå®Ÿéš›ã«ã¯å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼‰
            base_sales = work_hours * 25000  # åŸºæœ¬æ™‚é–“å˜ä¾¡
            appointment_bonus = appointments * 15000
            companion_bonus = companion * 30000
            drink_bonus = drinks * 2000
            bottle_bonus = bottles * 20000
            
            weather_multiplier = {'æ™´ã‚Œ': 1.1, 'æ›‡ã‚Š': 1.0, 'é›¨': 0.9}[weather]
            weekend_multiplier = 1.2 if is_weekend else 1.0
            event_multiplier = {'ãªã—': 1.0, 'å¿˜å¹´ä¼š': 1.3, 'æ­“é€è¿ä¼š': 1.2, 'ã‚¯ãƒªã‚¹ãƒã‚¹': 1.4}[has_event]
            
            predicted_sales = (base_sales + appointment_bonus + companion_bonus + 
                             drink_bonus + bottle_bonus) * weather_multiplier * weekend_multiplier * event_multiplier
            
            st.session_state.predicted_sales = predicted_sales
    
    with col2:
        st.write("### äºˆæ¸¬çµæœ")
        
        if 'predicted_sales' in st.session_state:
            predicted_value = st.session_state.predicted_sales
            
            # äºˆæ¸¬çµæœã®è¡¨ç¤º
            st.metric("äºˆæ¸¬å£²ä¸Š", f"Â¥{predicted_value:,.0f}")
            
            # äºˆæ¸¬ã®å†…è¨³ã‚’è¡¨ç¤º
            st.write("#### å£²ä¸Šå†…è¨³")
            breakdown = {
                'åŸºæœ¬å£²ä¸Š': work_hours * 25000,
                'æŒ‡åãƒœãƒ¼ãƒŠã‚¹': appointments * 15000,
                'åŒä¼´ãƒœãƒ¼ãƒŠã‚¹': companion * 30000,
                'ãƒ‰ãƒªãƒ³ã‚¯ãƒœãƒ¼ãƒŠã‚¹': drinks * 2000,
                'ãƒœãƒˆãƒ«ãƒœãƒ¼ãƒŠã‚¹': bottles * 20000
            }
            
            for item, amount in breakdown.items():
                st.write(f"- {item}: Â¥{amount:,.0f}")
            
            # äºˆæ¸¬ä¿¡é ¼åŒºé–“
            confidence_lower = predicted_value * 0.85
            confidence_upper = predicted_value * 1.15
            
            st.write(f"#### äºˆæ¸¬ç¯„å›²")
            st.write(f"- ä¸‹é™: Â¥{confidence_lower:,.0f}")
            st.write(f"- ä¸Šé™: Â¥{confidence_upper:,.0f}")
            
            # ã‚¢ãƒ‰ãƒã‚¤ã‚¹ç”Ÿæˆ
            st.write("#### ğŸ’¡ å£²ä¸Šå‘ä¸Šã‚¢ãƒ‰ãƒã‚¤ã‚¹")
            
            if appointments < 3:
                st.write("- ğŸ“ æŒ‡åã‚’å¢—ã‚„ã™ã“ã¨ã§å£²ä¸Šå¤§å¹…ã‚¢ãƒƒãƒ—ãŒæœŸå¾…ã§ãã¾ã™")
            if companion == 0:
                st.write("- ğŸ½ï¸ åŒä¼´ã‚’å–ã‚‹ã“ã¨ã§é«˜é¡å£²ä¸Šã«ã¤ãªãŒã‚Šã¾ã™")
            if bottles < 2:
                st.write("- ğŸ¾ ãƒœãƒˆãƒ«ã‚ªãƒ¼ãƒ€ãƒ¼ã®ææ¡ˆã§å˜ä¾¡ã‚¢ãƒƒãƒ—ã‚’å›³ã‚Šã¾ã—ã‚‡ã†")

def create_detailed_analysis(df):
    """è©³ç´°åˆ†æã‚’ä½œæˆ"""
    
    st.subheader("ğŸ“Š è©³ç´°åˆ†æ")
    
    tab1, tab2, tab3 = st.tabs(["ç›¸é–¢åˆ†æ", "æ™‚é–“åˆ†æ", "é¡§å®¢åˆ†æ"])
    
    with tab1:
        st.write("### å£²ä¸Šç›¸é–¢åˆ†æ")
        
        # æ•°å€¤åˆ—ã®ã¿ã‚’é¸æŠ
        numeric_cols = ['å£²ä¸Šé‡‘é¡', 'æ¥å®¢æ™‚é–“', 'æŒ‡åæ•°', 'åŒä¼´æ•°', 'ãƒ‰ãƒªãƒ³ã‚¯æ•°', 'ãƒœãƒˆãƒ«æ•°']
        correlation_data = df[numeric_cols].corr()
        
        fig = px.imshow(correlation_data, 
                       text_auto=True,
                       aspect="auto",
                       title="å£²ä¸Šé–¢é€£æŒ‡æ¨™ã®ç›¸é–¢ä¿‚æ•°")
        
        st.plotly_chart(fig, use_container_width=True)
        
        # ç›¸é–¢ã®è§£é‡ˆ
        st.write("#### ç›¸é–¢åˆ†æã®è§£é‡ˆ")
        st.write("- **æŒ‡åæ•°**ã¨å£²ä¸Šã®ç›¸é–¢ãŒæœ€ã‚‚é«˜ã„å ´åˆã€æŒ‡åç²å¾—ãŒé‡è¦")
        st.write("- **åŒä¼´æ•°**ã¨ã®ç›¸é–¢ãŒé«˜ã„å ´åˆã€åŒä¼´å–¶æ¥­ã«æ³¨åŠ›")
        st.write("- **æ¥å®¢æ™‚é–“**ã¨ã®ç›¸é–¢ã§åŠ¹ç‡æ€§ã‚’åˆ¤æ–­")
    
    with tab2:
        st.write("### æ™‚é–“å¸¯åˆ¥åˆ†æ")
        
        # æ¥å®¢æ™‚é–“ã®åˆ†å¸ƒ
        fig1 = px.histogram(df, x='æ¥å®¢æ™‚é–“', nbins=20,
                           title="æ¥å®¢æ™‚é–“ã®åˆ†å¸ƒ")
        st.plotly_chart(fig1, use_container_width=True)
        
        # æ™‚é–“å˜ä¾¡ã®åˆ†æ
        df['æ™‚é–“å˜ä¾¡'] = df['å£²ä¸Šé‡‘é¡'] / df['æ¥å®¢æ™‚é–“']
        fig2 = px.box(df, x='ã‚­ãƒ£ã‚¹ãƒˆID', y='æ™‚é–“å˜ä¾¡',
                     title="ã‚­ãƒ£ã‚¹ãƒˆåˆ¥æ™‚é–“å˜ä¾¡ã®åˆ†å¸ƒ")
        st.plotly_chart(fig2, use_container_width=True)
    
    with tab3:
        st.write("### é¡§å®¢è¡Œå‹•åˆ†æ")
        
        # æŒ‡åæ•°ã¨ãƒ‰ãƒªãƒ³ã‚¯æ•°ã®é–¢ä¿‚
        fig1 = px.scatter(df, x='æŒ‡åæ•°', y='ãƒ‰ãƒªãƒ³ã‚¯æ•°', 
                         size='å£²ä¸Šé‡‘é¡', color='ã‚­ãƒ£ã‚¹ãƒˆID',
                         title="æŒ‡åæ•° vs ãƒ‰ãƒªãƒ³ã‚¯æ•°ï¼ˆãƒãƒ–ãƒ«ã‚µã‚¤ã‚º=å£²ä¸Šï¼‰")
        st.plotly_chart(fig1, use_container_width=True)
        
        # åŒä¼´ã¨å£²ä¸Šã®é–¢ä¿‚
        companion_sales = df.groupby('åŒä¼´æ•°')['å£²ä¸Šé‡‘é¡'].mean()
        fig2 = px.bar(x=companion_sales.index, y=companion_sales.values,
                     title="åŒä¼´æ•°åˆ¥å¹³å‡å£²ä¸Š")
        st.plotly_chart(fig2, use_container_width=True)

# ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®å®Ÿè¡Œéƒ¨åˆ†
if __name__ == "__main__":
    create_dashboard()
```

### Excelå½¢å¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ©Ÿèƒ½

```python
def create_excel_report(df, file_path="å£²ä¸Šåˆ†æãƒ¬ãƒãƒ¼ãƒˆ.xlsx"):
    """Excelå½¢å¼ã®è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    
    with pd.ExcelWriter(file_path, engine='openpyxl') as writer:
        
        # 1. ã‚µãƒãƒªãƒ¼ã‚·ãƒ¼ãƒˆ
        summary_data = {
            'æŒ‡æ¨™': ['ç·å£²ä¸Š', 'å¹³å‡æ—¥æ¬¡å£²ä¸Š', 'ç·æŒ‡åæ•°', 'å¹³å‡æ™‚é–“å˜ä¾¡', 'å‡ºå‹¤æ—¥æ•°'],
            'å€¤': [
                f"Â¥{df['å£²ä¸Šé‡‘é¡'].sum():,.0f}",
                f"Â¥{df['å£²ä¸Šé‡‘é¡'].mean():,.0f}",
                f"{df['æŒ‡åæ•°'].sum():,.0f}",
                f"Â¥{(df['å£²ä¸Šé‡‘é¡']/df['æ¥å®¢æ™‚é–“']).mean():,.0f}",
                f"{df['æ—¥ä»˜'].nunique()}æ—¥"
            ]
        }
        
        pd.DataFrame(summary_data).to_excel(writer, sheet_name='ã‚µãƒãƒªãƒ¼', index=False)
        
        # 2. æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿
        daily_summary = df.groupby('æ—¥ä»˜').agg({
            'å£²ä¸Šé‡‘é¡': 'sum',
            'æŒ‡åæ•°': 'sum',
            'åŒä¼´æ•°': 'sum',
            'æ¥å®¢æ™‚é–“': 'sum',
            'ã‚­ãƒ£ã‚¹ãƒˆID': 'nunique'
        }).round(0)
        daily_summary.columns = ['æ—¥æ¬¡å£²ä¸Š', 'æŒ‡åæ•°', 'åŒä¼´æ•°', 'ç·æ¥å®¢æ™‚é–“', 'å‡ºå‹¤ã‚­ãƒ£ã‚¹ãƒˆæ•°']
        daily_summary.to_excel(writer, sheet_name='æ—¥æ¬¡ã‚µãƒãƒªãƒ¼')
        
        # 3. ã‚­ãƒ£ã‚¹ãƒˆåˆ¥åˆ†æ
        cast_summary = df.groupby('ã‚­ãƒ£ã‚¹ãƒˆID').agg({
            'å£²ä¸Šé‡‘é¡': ['sum', 'mean'],
            'æŒ‡åæ•°': ['sum', 'mean'],
            'åŒä¼´æ•°': 'sum',
            'æ¥å®¢æ™‚é–“': 'sum'
        }).round(0)
        
        cast_summary.columns = ['ç·å£²ä¸Š', 'å¹³å‡å£²ä¸Š', 'ç·æŒ‡å', 'å¹³å‡æŒ‡å', 'ç·åŒä¼´', 'ç·æ¥å®¢æ™‚é–“']
        cast_summary['æ™‚é–“å˜ä¾¡'] = (cast_summary['ç·å£²ä¸Š'] / cast_summary['ç·æ¥å®¢æ™‚é–“']).round(0)
        cast_summary.to_excel(writer, sheet_name='ã‚­ãƒ£ã‚¹ãƒˆåˆ†æ')
        
        # 4. è©³ç´°ãƒ‡ãƒ¼ã‚¿
        df.to_excel(writer, sheet_name='è©³ç´°ãƒ‡ãƒ¼ã‚¿', index=False)
        
        # 5. äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚µãƒ³ãƒ—ãƒ«ï¼‰
        prediction_scenarios = pd.DataFrame({
            'ã‚·ãƒŠãƒªã‚ª': ['åŸºæœ¬', 'å¥½èª¿', 'æœ€é«˜'],
            'æ¥å®¢æ™‚é–“': [4, 5, 6],
            'æŒ‡åæ•°': [2, 4, 6],
            'åŒä¼´æ•°': [0, 1, 2],
            'äºˆæƒ³å£²ä¸Š': [100000, 180000, 280000]
        })
        prediction_scenarios.to_excel(writer, sheet_name='å£²ä¸Šäºˆæ¸¬', index=False)
    
    print(f"âœ… Excelãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {file_path}")

# ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã®å®Ÿè¡Œ
create_excel_report(final_df)
```

---

## 5. é‹ç”¨ã¨ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹

### è‡ªå‹•åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```python
import schedule
import time
from datetime import datetime
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from email.mime.base import MIMEBase
from email import encoders

class AutomationManager:
    def __init__(self, data_path, model_path):
        self.data_path = data_path
        self.model_path = model_path
        self.processor = CabaretDataProcessor()
        
    def daily_data_update(self):
        """æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿æ›´æ–°å‡¦ç†"""
        
        try:
            print(f"ğŸ“… {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - æ—¥æ¬¡æ›´æ–°é–‹å§‹")
            
            # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
            new_data = self.processor.load_excel_data(self.data_path)
            
            if new_data:
                # ãƒ‡ãƒ¼ã‚¿å‡¦ç†
                processed_data = self.processor.preprocess_daily_sales(new_data['æ—¥æ¬¡å£²ä¸Š'])
                
                # ç•°å¸¸å€¤ãƒã‚§ãƒƒã‚¯
                anomalies = self.detect_anomalies(processed_data)
                
                if anomalies:
                    self.send_alert(f"ç•°å¸¸å€¤æ¤œå‡º: {len(anomalies)}ä»¶")
                
                print("âœ… æ—¥æ¬¡æ›´æ–°å®Œäº†")
                
        except Exception as e:
            print(f"âŒ æ—¥æ¬¡æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            self.send_alert(f"æ—¥æ¬¡æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
    
    def weekly_model_retrain(self):
        """é€±æ¬¡ãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’"""
        
        try:
            print("ğŸ¤– é€±æ¬¡ãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’é–‹å§‹")
            
            # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            data = self.processor.load_excel_data(self.data_path)
            processed_data = self.processor.preprocess_daily_sales(data['æ—¥æ¬¡å£²ä¸Š'])
            
            # ãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’
            predictor = SalesPredictionModel()
            results = predictor.train_models(processed_data)
            
            # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            predictor.save_model(self.model_path)
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            best_r2 = max([result['r2'] for result in results.values()])
            
            report = f"é€±æ¬¡ãƒ¢ãƒ‡ãƒ«æ›´æ–°å®Œäº†\næœ€é«˜RÂ²ã‚¹ã‚³ã‚¢: {best_r2:.3f}"
            self.send_report(report)
            
            print("âœ… é€±æ¬¡ãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’å®Œäº†")
            
        except Exception as e:
            print(f"âŒ é€±æ¬¡å†å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
            self.send_alert(f"é€±æ¬¡å†å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
    
    def detect_anomalies(self, df, threshold=2.5):
        """ç•°å¸¸å€¤æ¤œå‡º"""
        
        anomalies = []
        
        # å£²ä¸Šé‡‘é¡ã®ç•°å¸¸å€¤
        sales_mean = df['å£²ä¸Šé‡‘é¡'].mean()
        sales_std = df['å£²ä¸Šé‡‘é¡'].std()
        
        sales_anomalies = df[
            (df['å£²ä¸Šé‡‘é¡'] > sales_mean + threshold * sales_std) |
            (df['å£²ä¸Šé‡‘é¡'] < sales_mean - threshold * sales_std)
        ]
        
        for _, row in sales_anomalies.iterrows():
            anomalies.append({
                'type': 'å£²ä¸Šç•°å¸¸',
                'date': row['æ—¥ä»˜'],
                'cast': row['ã‚­ãƒ£ã‚¹ãƒˆID'],
                'value': row['å£²ä¸Šé‡‘é¡'],
                'threshold': f"å¹³å‡Â±{threshold}Ïƒ"
            })
        
        return anomalies
    
    def send_alert(self, message):
        """ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡"""
        print(f"ğŸš¨ ã‚¢ãƒ©ãƒ¼ãƒˆ: {message}")
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ãƒ¡ãƒ¼ãƒ«ã‚„Slackãªã©ã§é€šçŸ¥
    
    def send_report(self, message):
        """ãƒ¬ãƒãƒ¼ãƒˆé€ä¿¡"""
        print(f"ğŸ“Š ãƒ¬ãƒãƒ¼ãƒˆ: {message}")
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ãƒ¡ãƒ¼ãƒ«ã§å®šæœŸãƒ¬ãƒãƒ¼ãƒˆã‚’é€ä¿¡
    
    def setup_schedule(self):
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®š"""
        
        # æ¯æ—¥åˆå‰6æ™‚ã«ãƒ‡ãƒ¼ã‚¿æ›´æ–°
        schedule.every().day.at("06:00").do(self.daily_data_update)
        
        # æ¯é€±æœˆæ›œæ—¥åˆå‰3æ™‚ã«ãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’
        schedule.every().monday.at("03:00").do(self.weekly_model_retrain)
        
        print("â° ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®šå®Œäº†")
        
        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè¡Œ
        while True:
            schedule.run_pending()
            time.sleep(60)  # 1åˆ†é–“éš”ã§ãƒã‚§ãƒƒã‚¯

# è‡ªå‹•åŒ–ã®å®Ÿè¡Œä¾‹
# automation = AutomationManager("data.xlsx", "model.pkl")
# automation.setup_schedule()
```

### ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯

```python
class DataQualityChecker:
    def __init__(self):
        self.quality_rules = {
            'å£²ä¸Šé‡‘é¡': {'min': 0, 'max': 1000000},
            'æ¥å®¢æ™‚é–“': {'min': 0, 'max': 12},
            'æŒ‡åæ•°': {'min': 0, 'max': 50},
            'åŒä¼´æ•°': {'min': 0, 'max': 10}
        }
    
    def check_data_quality(self, df):
        """ãƒ‡ãƒ¼ã‚¿å“è³ªã‚’ãƒã‚§ãƒƒã‚¯"""
        
        issues = []
        
        # 1. å¿…é ˆåˆ—ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        required_columns = ['æ—¥ä»˜', 'ã‚­ãƒ£ã‚¹ãƒˆID', 'å£²ä¸Šé‡‘é¡', 'æ¥å®¢æ™‚é–“']
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            issues.append(f"å¿…é ˆåˆ—ãŒä¸è¶³: {missing_columns}")
        
        # 2. æ¬ æå€¤ãƒã‚§ãƒƒã‚¯
        missing_data = df[required_columns].isnull().sum()
        missing_issues = missing_data[missing_data > 0]
        
        if len(missing_issues) > 0:
            issues.append(f"æ¬ æå€¤ã‚ã‚Š: {missing_issues.to_dict()}")
        
        # 3. å€¤ã®ç¯„å›²ãƒã‚§ãƒƒã‚¯
        for column, rules in self.quality_rules.items():
            if column in df.columns:
                out_of_range = df[
                    (df[column] < rules['min']) | (df[column] > rules['max'])
                ]
                
                if len(out_of_range) > 0:
                    issues.append(f"{column}ã®ç¯„å›²å¤–ãƒ‡ãƒ¼ã‚¿: {len(out_of_range)}ä»¶")
        
        # 4. é‡è¤‡ãƒ‡ãƒ¼ã‚¿ãƒã‚§ãƒƒã‚¯
        duplicates = df.duplicated(subset=['æ—¥ä»˜', 'ã‚­ãƒ£ã‚¹ãƒˆID']).sum()
        if duplicates > 0:
            issues.append(f"é‡è¤‡ãƒ‡ãƒ¼ã‚¿: {duplicates}ä»¶")
        
        # 5. è«–ç†çš„æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        logical_issues = df[df['å£²ä¸Šé‡‘é¡'] < df['æ¥å®¢æ™‚é–“'] * 1000]  # æ™‚çµ¦1000å††æœªæº€
        if len(logical_issues) > 0:
            issues.append(f"è«–ç†çš„ã«ä¸æ•´åˆ: {len(logical_issues)}ä»¶ï¼ˆæ¥µç«¯ã«ä½ã„æ™‚é–“å˜ä¾¡ï¼‰")
        
        return issues
    
    def generate_quality_report(self, df):
        """å“è³ªãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        
        issues = self.check_data_quality(df)
        
        report = {
            'total_records': len(df),
            'quality_issues': issues,
            'data_completeness': {
                col: f"{(1 - df[col].isnull().mean()) * 100:.1f}%"
                for col in df.columns if df[col].dtype in ['float64', 'int64']
            },
            'summary': "OK" if len(issues) == 0 else f"{len(issues)}ä»¶ã®å•é¡Œã‚ã‚Š"
        }
        
        return report

# å“è³ªãƒã‚§ãƒƒã‚¯ã®å®Ÿè¡Œ
quality_checker = DataQualityChecker()
quality_report = quality_checker.generate_quality_report(final_df)

print("ğŸ“‹ ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒãƒ¼ãƒˆ:")
print(f"ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {quality_report['total_records']}")
print(f"å“è³ªçŠ¶æ³: {quality_report['summary']}")

if quality_report['quality_issues']:
    print("\nâš ï¸ å“è³ªå•é¡Œ:")
    for issue in quality_report['quality_issues']:
        print(f"  - {issue}")

print(f"\nâœ… ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§:")
for col, completeness in quality_report['data_completeness'].items():
    print(f"  {col}: {completeness}")
```

---

## ğŸ“‹ é‹ç”¨ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### æ—¥æ¬¡é‹ç”¨ã‚¿ã‚¹ã‚¯
- [ ] å‰æ—¥ã®å£²ä¸Šãƒ‡ãƒ¼ã‚¿å…¥åŠ›ç¢ºèª
- [ ] ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
- [ ] ç•°å¸¸å€¤ãƒ»å¤–ã‚Œå€¤ã®ç¢ºèª
- [ ] ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®å‹•ä½œç¢ºèª
- [ ] é‡è¦æŒ‡æ¨™ã®å‰æ—¥æ¯”è¼ƒ

### é€±æ¬¡é‹ç”¨ã‚¿ã‚¹ã‚¯
- [ ] é€±æ¬¡å£²ä¸Šã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
- [ ] ã‚­ãƒ£ã‚¹ãƒˆåˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
- [ ] äºˆæ¸¬ç²¾åº¦ã®æ¤œè¨¼
- [ ] ãƒ¢ãƒ‡ãƒ«ã®å†å­¦ç¿’å®Ÿè¡Œ
- [ ] ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—

### æœˆæ¬¡é‹ç”¨ã‚¿ã‚¹ã‚¯
- [ ] æœˆæ¬¡å£²ä¸Šåˆ†æãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
- [ ] å­£ç¯€æ€§ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
- [ ] äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦è©•ä¾¡
- [ ] ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®ç‚¹æ¤œ
- [ ] æ”¹å–„ææ¡ˆã®æ¤œè¨

---

## ğŸš€ ç™ºå±•çš„ãªæ©Ÿèƒ½è¿½åŠ 

### é«˜åº¦ãªäºˆæ¸¬æ©Ÿèƒ½

```python
def advanced_prediction_features():
    """é«˜åº¦ãªäºˆæ¸¬æ©Ÿèƒ½ã®ä¾‹"""
    
    features = {
        "ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬": "ç¾åœ¨ã®çŠ¶æ³ã«åŸºã¥ãå³åº§ã®å£²ä¸Šäºˆæ¸¬",
        "ã‚·ãƒŠãƒªã‚ªåˆ†æ": "ç•°ãªã‚‹æ¡ä»¶ä¸‹ã§ã®å£²ä¸Šã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³", 
        "æœ€é©åŒ–ææ¡ˆ": "å£²ä¸Šæœ€å¤§åŒ–ã®ãŸã‚ã®è¡Œå‹•ææ¡ˆ",
        "éœ€è¦äºˆæ¸¬": "æ›œæ—¥ãƒ»å­£ç¯€ã‚’è€ƒæ…®ã—ãŸéœ€è¦äºˆæ¸¬",
        "ä¾¡æ ¼æœ€é©åŒ–": "ã‚µãƒ¼ãƒ“ã‚¹ä¾¡æ ¼ã®æœ€é©åŒ–ææ¡ˆ"
    }
    
    print("ğŸ”® è¿½åŠ å¯èƒ½ãªé«˜åº¦æ©Ÿèƒ½:")
    for feature, description in features.items():
        print(f"  ğŸ“ˆ {feature}: {description}")
    
    return features

advanced_prediction_features()
```

### ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¨ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼

```python
def implement_security_measures():
    """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–ã®å®Ÿè£…ä¾‹"""
    
    security_measures = [
        "ãƒ‡ãƒ¼ã‚¿æš—å·åŒ–: å€‹äººæƒ…å ±ã®æš—å·åŒ–ä¿å­˜",
        "ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡: ãƒ¦ãƒ¼ã‚¶ãƒ¼æ¨©é™ã®ç®¡ç†",
        "ç›£æŸ»ãƒ­ã‚°: æ“ä½œå±¥æ­´ã®è¨˜éŒ²",
        "ãƒ‡ãƒ¼ã‚¿ãƒã‚¹ã‚­ãƒ³ã‚°: è¡¨ç¤ºæ™‚ã®å€‹äººæƒ…å ±éš è”½",
        "ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: å®šæœŸçš„ãªãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"
    ]
    
    print("ğŸ”’ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–:")
    for measure in security_measures:
        print(f"  ğŸ›¡ï¸ {measure}")
    
    return security_measures

implement_security_measures()
```

---

## ğŸ¯ ã¾ã¨ã‚

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€ã‚­ãƒ£ãƒã‚¯ãƒ©ã®å£²ä¸Šäºˆæ¸¬ã¨ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ§‹ç¯‰ã«ã¤ã„ã¦ä»¥ä¸‹ã®å†…å®¹ã‚’è§£èª¬ã—ã¾ã—ãŸï¼š

### âœ… é”æˆã§ãã‚‹ã“ã¨
1. **Excelå…¥åŠ›ã‹ã‚‰ã®è‡ªå‹•ãƒ‡ãƒ¼ã‚¿å‡¦ç†**
2. **æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹å£²ä¸Šäºˆæ¸¬**  
3. **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰**
4. **è‡ªå‹•ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ**
5. **å“è³ªç®¡ç†ã¨ã‚¢ãƒ©ãƒ¼ãƒˆæ©Ÿèƒ½**

### ğŸ’¡ ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤
- **ãƒ‡ãƒ¼ã‚¿é§†å‹•ã®æ„æ€æ±ºå®š**æ”¯æ´
- **ã‚­ãƒ£ã‚¹ãƒˆç®¡ç†ã®åŠ¹ç‡åŒ–**
- **å£²ä¸Šå‘ä¸Šæ–½ç­–ã®åŠ¹æœæ¸¬å®š**
- **ãƒªã‚¹ã‚¯ç®¡ç†ã¨ç•°å¸¸æ¤œçŸ¥**
- **çµŒå–¶æˆ¦ç•¥ã®ç«‹æ¡ˆæ”¯æ´**

### ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—
1. å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã§ã®ãƒ†ã‚¹ãƒˆé‹ç”¨
2. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®åé›†
3. æ©Ÿèƒ½ã®æ®µéšçš„æ‹¡å¼µ
4. ä»–åº—èˆ—ã¸ã®å±•é–‹æ¤œè¨
5. AIæ©Ÿèƒ½ã®ã•ã‚‰ãªã‚‹é«˜åº¦åŒ–

ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã§ã€ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ãŸåŠ¹ç‡çš„ãªåº—èˆ—é‹å–¶ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

---

*âš ï¸ æ³¨æ„: å€‹äººæƒ…å ±ã®å–ã‚Šæ‰±ã„ã«ã¯ååˆ†æ³¨æ„ã—ã€é–¢é€£æ³•è¦ã‚’éµå®ˆã—ã¦ãã ã•ã„ã€‚*

```python
print("ğŸ‰ å£²ä¸Šäºˆæ¸¬ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ§‹ç¯‰å®Œäº†ï¼")
print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ–ãƒ³ãªçµŒå–¶ã‚’å§‹ã‚ã¾ã—ã‚‡ã†ï¼")
```
