# ã€æ–°äººã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å‘ã‘ã€‘TableauÃ—ExcelÃ—Pythonã§å­¦ã¶å›å¸°åˆ†æãƒ»æ™‚ç³»åˆ—åˆ†æå®Œå…¨ã‚¬ã‚¤ãƒ‰

## ã¯ã˜ã‚ã«

ã€Œå›å¸°åˆ†æã£ã¦ä½•ï¼Ÿã€ã€Œæ™‚ç³»åˆ—åˆ†æã¯é›£ã—ãã†...ã€

ãã‚“ãªå¿ƒé…ã¯ä¸è¦ã§ã™ï¼ã“ã®è¨˜äº‹ã§ã¯ã€**çµ±è¨ˆå­¦ã®çŸ¥è­˜ã‚¼ãƒ­**ã‹ã‚‰å§‹ã‚ã¦ã€Tableauã€Excelã€Pythonã‚’ä½¿ã£ãŸå®Ÿè·µçš„ãªå›å¸°åˆ†æãƒ»æ™‚ç³»åˆ—åˆ†æã‚’ãƒã‚¹ã‚¿ãƒ¼ã§ãã¾ã™ã€‚

**30åˆ†ã§åŸºæœ¬ç†è§£ â†’ 2æ™‚é–“ã§å®Ÿè·µ â†’ 1æ—¥ã§æ¥­å‹™æ´»ç”¨**ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã€èª°ã§ã‚‚åˆ†æã®ãƒ—ãƒ­ã«ãªã‚Œã¾ã™ã€‚

---

## 1. å›å¸°åˆ†æãƒ»æ™‚ç³»åˆ—åˆ†æã£ã¦ä½•ï¼Ÿã€è¶…åŸºç¤ç·¨ã€‘

### ğŸ¯ å›å¸°åˆ†æã¨ã¯ï¼Ÿ

**ä¸€è¨€ã§è¨€ã†ã¨ï¼šã€ŒXãŒYã«ã©ã†å½±éŸ¿ã™ã‚‹ã‹ã€ã‚’æ•°å€¤åŒ–ã™ã‚‹åˆ†æ**

```
èº«è¿‘ãªä¾‹ï¼š
ãƒ»æ°—æ¸© â†’ ã‚¢ã‚¤ã‚¹ã‚¯ãƒªãƒ¼ãƒ ã®å£²ä¸Š
ãƒ»åºƒå‘Šè²» â†’ å•†å“ã®å£²ä¸Š  
ãƒ»å‹‰å¼·æ™‚é–“ â†’ ãƒ†ã‚¹ãƒˆã®ç‚¹æ•°
ãƒ»åº—èˆ—é¢ç© â†’ æ¥å®¢æ•°

ğŸ“Š å›å¸°åˆ†æã§ã‚ã‹ã‚‹ã“ã¨ï¼š
1. é–¢ä¿‚ã®å¼·ã•ï¼ˆç›¸é–¢ä¿‚æ•°ï¼‰
2. å½±éŸ¿ã®å¤§ãã•ï¼ˆå›å¸°ä¿‚æ•°ï¼‰
3. å°†æ¥ã®äºˆæ¸¬å€¤
4. çµ±è¨ˆçš„æœ‰æ„æ€§
```

### ğŸ“ˆ æ™‚ç³»åˆ—åˆ†æã¨ã¯ï¼Ÿ

**ä¸€è¨€ã§è¨€ã†ã¨ï¼šã€Œæ™‚é–“ã®æµã‚Œã«ã‚ˆã‚‹å¤‰åŒ–ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã€ã‚’åˆ†æ**

```
èº«è¿‘ãªä¾‹ï¼š
ãƒ»æœˆåˆ¥å£²ä¸Šã®æ¨ç§»
ãƒ»æ ªä¾¡ã®å€¤å‹•ã
ãƒ»æ°—æ¸©ã®å­£ç¯€å¤‰å‹•
ãƒ»ã‚µã‚¤ãƒˆã‚¢ã‚¯ã‚»ã‚¹æ•°ã®æ—¥æ¬¡å¤‰åŒ–

ğŸ” æ™‚ç³»åˆ—åˆ†æã§ã‚ã‹ã‚‹ã“ã¨ï¼š
1. ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆä¸Šæ˜‡ãƒ»ä¸‹é™å‚¾å‘ï¼‰
2. å­£ç¯€æ€§ï¼ˆå®šæœŸçš„ãªå¤‰å‹•ï¼‰
3. å‘¨æœŸæ€§ï¼ˆç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
4. æœªæ¥ã®äºˆæ¸¬å€¤
```

### ğŸ’¡ ãªãœé‡è¦ãªã®ï¼Ÿ

**ãƒ“ã‚¸ãƒã‚¹ã§ã®æ´»ç”¨ä¾‹**
- **å£²ä¸Šäºˆæ¸¬**ï¼šæ¥æœˆã®å£²ä¸Šã‚’äºˆæ¸¬ã—ã¦åœ¨åº«èª¿æ•´
- **ä¾¡æ ¼æœ€é©åŒ–**ï¼šä¾¡æ ¼å¤‰æ›´ãŒå£²ä¸Šã«ä¸ãˆã‚‹å½±éŸ¿ã‚’åˆ†æ
- **ãƒªã‚¹ã‚¯ç®¡ç†**ï¼šç•°å¸¸å€¤ã‚„æ€¥å¤‰ã‚’æ—©æœŸç™ºè¦‹
- **æ„æ€æ±ºå®šæ”¯æ´**ï¼šãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ãŸæˆ¦ç•¥ç«‹æ¡ˆ

---

## 2. å¿…è¦ãªç’°å¢ƒã¨ãƒ‡ãƒ¼ã‚¿æº–å‚™

### ğŸ”§ ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

**å‰æï¼šTableau Desktopã€Pythonã€ExcelãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿**

**è¿½åŠ ã§å¿…è¦ãªPythonãƒ©ã‚¤ãƒ–ãƒ©ãƒª**
```bash
# ã‚³ãƒãƒ³ãƒ‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§å®Ÿè¡Œ
pip install pandas numpy matplotlib seaborn
pip install scikit-learn statsmodels
pip install tabpy-client
pip install openpyxl xlsxwriter
```

**TabPyæ¥ç¶šç¢ºèª**
```bash
# TabPyã‚µãƒ¼ãƒãƒ¼èµ·å‹•
tabpy

# ãƒ–ãƒ©ã‚¦ã‚¶ã§ç¢ºèª
# http://localhost:9004 ã«ã‚¢ã‚¯ã‚»ã‚¹
```

### ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ

**Excel ãƒ•ã‚¡ã‚¤ãƒ«ï¼š`regression_timeseries_data.xlsx`**

**Sheet1: å›å¸°åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿**
```
æ—¥ä»˜        å£²ä¸Šé‡‘é¡    åºƒå‘Šè²»    æ°—æ¸©    é™æ°´é‡    æ›œæ—¥    åº—èˆ—é¢ç©
2023-01-01  850000     200000   8.5     0.0      æ—¥æ›œ    120
2023-01-02  920000     180000   9.2     0.0      æœˆæ›œ    120  
2023-01-03  780000     150000   7.8     5.2      ç«æ›œ    120
2023-01-04  1100000    300000   10.1    0.0      æ°´æ›œ    120
2023-01-05  980000     250000   9.8     0.0      æœ¨æ›œ    120
...ï¼ˆ365æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿ï¼‰
```

**Sheet2: æ™‚ç³»åˆ—åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿**
```
å¹´æœˆ        æœˆæ¬¡å£²ä¸Š    é¡§å®¢æ•°    æ–°è¦é¡§å®¢   ãƒªãƒ”ãƒ¼ãƒˆå®¢
2020-01     25000000   12500    3000      9500
2020-02     22000000   11000    2800      8200
2020-03     28000000   14000    3200      10800
...ï¼ˆ48ãƒ¶æœˆåˆ†ã®ãƒ‡ãƒ¼ã‚¿ï¼‰
```

### ğŸ ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆç”¨Pythonã‚³ãƒ¼ãƒ‰

```python
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import random

def generate_sample_data():
    """
    åˆ†æç”¨ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•ç”Ÿæˆ
    """
    # æ—¥ä»˜ç¯„å›²ã®è¨­å®š
    start_date = datetime(2023, 1, 1)
    end_date = datetime(2023, 12, 31)
    date_range = pd.date_range(start_date, end_date, freq='D')
    
    # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
    df = pd.DataFrame({'æ—¥ä»˜': date_range})
    
    # æ›œæ—¥ã®è¿½åŠ 
    df['æ›œæ—¥'] = df['æ—¥ä»˜'].dt.day_name()
    df['æ›œæ—¥_æ•°å€¤'] = df['æ—¥ä»˜'].dt.dayofweek
    
    # æœˆãƒ»å­£ç¯€ã®è¿½åŠ 
    df['æœˆ'] = df['æ—¥ä»˜'].dt.month
    df['å­£ç¯€'] = df['æœˆ'].map({
        12: 'å†¬', 1: 'å†¬', 2: 'å†¬',
        3: 'æ˜¥', 4: 'æ˜¥', 5: 'æ˜¥',
        6: 'å¤', 7: 'å¤', 8: 'å¤',
        9: 'ç§‹', 10: 'ç§‹', 11: 'ç§‹'
    })
    
    # åŸºæœ¬çš„ãªå¤–éƒ¨è¦å› ãƒ‡ãƒ¼ã‚¿
    np.random.seed(42)  # å†ç¾æ€§ã®ãŸã‚
    df['æ°—æ¸©'] = 15 + 10 * np.sin(2 * np.pi * df['æœˆ'] / 12) + np.random.normal(0, 3, len(df))
    df['é™æ°´é‡'] = np.maximum(0, np.random.exponential(2, len(df)))
    df['åºƒå‘Šè²»'] = np.random.uniform(100000, 400000, len(df))
    
    # å£²ä¸Šãƒ‡ãƒ¼ã‚¿ï¼ˆè¤‡æ•°è¦å› ã®å½±éŸ¿ã‚’å«ã‚€ï¼‰
    base_sales = 800000  # ãƒ™ãƒ¼ã‚¹å£²ä¸Š
    
    # å„è¦å› ã®å½±éŸ¿
    temp_effect = (df['æ°—æ¸©'] - 15) * 20000  # æ°—æ¸©ã®å½±éŸ¿
    rain_effect = -df['é™æ°´é‡'] * 10000      # é›¨ã®å½±éŸ¿
    ad_effect = df['åºƒå‘Šè²»'] * 1.5           # åºƒå‘ŠåŠ¹æœ
    
    # æ›œæ—¥åŠ¹æœ
    weekday_effect = df['æ›œæ—¥_æ•°å€¤'].map({
        0: 50000,   # æœˆæ›œ
        1: 0,       # ç«æ›œ
        2: 0,       # æ°´æ›œ  
        3: 30000,   # æœ¨æ›œ
        4: 80000,   # é‡‘æ›œ
        5: 150000,  # åœŸæ›œ
        6: 120000   # æ—¥æ›œ
    })
    
    # å­£ç¯€åŠ¹æœ
    season_effect = df['å­£ç¯€'].map({
        'æ˜¥': 100000, 'å¤': 150000, 'ç§‹': 50000, 'å†¬': 200000
    })
    
    # ãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¤ã‚º
    noise = np.random.normal(0, 100000, len(df))
    
    # æœ€çµ‚å£²ä¸Šã®è¨ˆç®—
    df['å£²ä¸Šé‡‘é¡'] = (base_sales + temp_effect + rain_effect + 
                   ad_effect + weekday_effect + season_effect + noise)
    df['å£²ä¸Šé‡‘é¡'] = np.maximum(df['å£²ä¸Šé‡‘é¡'], 100000)  # æœ€ä½å£²ä¸Šä¿è¨¼
    
    # æ•´æ•°ã«å¤‰æ›
    df['å£²ä¸Šé‡‘é¡'] = df['å£²ä¸Šé‡‘é¡'].astype(int)
    df['åºƒå‘Šè²»'] = df['åºƒå‘Šè²»'].astype(int)
    
    # é¡§å®¢æ•°ï¼ˆå£²ä¸Šã«é€£å‹•ï¼‰
    df['é¡§å®¢æ•°'] = (df['å£²ä¸Šé‡‘é¡'] / 25000 + np.random.normal(0, 10, len(df))).astype(int)
    df['é¡§å®¢æ•°'] = np.maximum(df['é¡§å®¢æ•°'], 10)
    
    # åº—èˆ—é¢ç©ï¼ˆå›ºå®šï¼‰
    df['åº—èˆ—é¢ç©'] = 120
    
    print("ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†ï¼")
    print(f"ãƒ‡ãƒ¼ã‚¿æœŸé–“: {start_date.strftime('%Y-%m-%d')} - {end_date.strftime('%Y-%m-%d')}")
    print(f"ãƒ‡ãƒ¼ã‚¿è¡Œæ•°: {len(df)}")
    
    return df

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã¨ä¿å­˜
sample_data = generate_sample_data()

# Excelãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
with pd.ExcelWriter('regression_timeseries_data.xlsx') as writer:
    sample_data.to_excel(writer, sheet_name='daily_data', index=False)
    
    # æœˆæ¬¡ãƒ‡ãƒ¼ã‚¿ã‚‚ä½œæˆ
    monthly_data = sample_data.groupby([sample_data['æ—¥ä»˜'].dt.year, 
                                       sample_data['æ—¥ä»˜'].dt.month]).agg({
        'å£²ä¸Šé‡‘é¡': 'sum',
        'é¡§å®¢æ•°': 'sum', 
        'åºƒå‘Šè²»': 'sum'
    }).reset_index()
    monthly_data['å¹´æœˆ'] = monthly_data['æ—¥ä»˜'].astype(str) + '-' + monthly_data['æ—¥ä»˜'].astype(str).str.zfill(2)
    
    monthly_data.to_excel(writer, sheet_name='monthly_data', index=False)

print("Excelãƒ•ã‚¡ã‚¤ãƒ« 'regression_timeseries_data.xlsx' ãŒä½œæˆã•ã‚Œã¾ã—ãŸï¼")
```

---

## 3. å›å¸°åˆ†æå®Ÿè·µç·¨ï¼šå£²ä¸Šã«å½±éŸ¿ã™ã‚‹è¦å› ã‚’åˆ†æã—ã‚ˆã†

### ğŸ“Š Step 1: Excelãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª

ã¾ãšã€ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’Excelã§é–‹ã„ã¦ç¢ºèªã—ã¾ã—ã‚‡ã†ã€‚

**ç¢ºèªãƒã‚¤ãƒ³ãƒˆ**
- ãƒ‡ãƒ¼ã‚¿ã«æ¬ æå€¤ãŒãªã„ã‹
- æ•°å€¤ãƒ‡ãƒ¼ã‚¿ãŒæ­£ã—ãèªè­˜ã•ã‚Œã¦ã„ã‚‹ã‹
- æ—¥ä»˜å½¢å¼ãŒçµ±ä¸€ã•ã‚Œã¦ã„ã‚‹ã‹

### ğŸ” Step 2: Tableauã§ã®åŸºæœ¬çš„ãªé–¢ä¿‚æ€§ç¢ºèª

**2.1 Tableauã«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿**
1. Tableau Desktopèµ·å‹•
2. ã€ŒMicrosoft Excelã€ã‚’é¸æŠ
3. ã€Œregression_timeseries_data.xlsxã€ã‚’é–‹ã
4. ã€Œdaily_dataã€ã‚·ãƒ¼ãƒˆã‚’é¸æŠ

**2.2 æ•£å¸ƒå›³ã§é–¢ä¿‚æ€§ã‚’å¯è¦–åŒ–**

**æ°—æ¸© vs å£²ä¸Šã®é–¢ä¿‚**
- Xè»¸ï¼šæ°—æ¸©
- Yè»¸ï¼šå£²ä¸Šé‡‘é¡
- ãƒãƒ¼ã‚¯ï¼šå††
- ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³è¿½åŠ ï¼ˆå³ã‚¯ãƒªãƒƒã‚¯ â†’ ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³è¿½åŠ ï¼‰

```
ğŸ“ˆ æœŸå¾…ã•ã‚Œã‚‹çµæœï¼š
ãƒ»æ°—æ¸©ãŒé«˜ã„ã»ã©å£²ä¸ŠãŒå¢—åŠ ã™ã‚‹å‚¾å‘
ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã®å‚¾ããŒæ­£ã®å€¤
ãƒ»RÂ²å€¤ãŒ0.3-0.7ç¨‹åº¦ï¼ˆä¸­ç¨‹åº¦ã®ç›¸é–¢ï¼‰
```

**åºƒå‘Šè²» vs å£²ä¸Šã®é–¢ä¿‚**
- Xè»¸ï¼šåºƒå‘Šè²»  
- Yè»¸ï¼šå£²ä¸Šé‡‘é¡
- è‰²ï¼šæœˆï¼ˆå­£ç¯€æ€§ã‚‚åŒæ™‚ã«ç¢ºèªï¼‰

### ğŸ Step 3: Pythonã§è©³ç´°ãªå›å¸°åˆ†æ

**3.1 å˜å›å¸°åˆ†æï¼ˆ1ã¤ã®å¤‰æ•°ã®å½±éŸ¿ï¼‰**

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error
import statsmodels.api as sm

def simple_regression_analysis(df, x_col, y_col):
    """
    å˜å›å¸°åˆ†æã‚’å®Ÿè¡Œ
    """
    # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
    X = df[x_col].values.reshape(-1, 1)
    y = df[y_col].values
    
    # æ¬ æå€¤ã®é™¤å»
    mask = ~(np.isnan(X.flatten()) | np.isnan(y))
    X = X[mask]
    y = y[mask]
    
    # å›å¸°ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
    model = LinearRegression()
    model.fit(X, y)
    
    # äºˆæ¸¬
    y_pred = model.predict(X)
    
    # çµ±è¨ˆæŒ‡æ¨™ã®è¨ˆç®—
    r2 = r2_score(y, y_pred)
    rmse = np.sqrt(mean_squared_error(y, y_pred))
    
    # statsmodelsã§è©³ç´°ãªçµ±è¨ˆæƒ…å ±
    X_with_const = sm.add_constant(X.flatten())
    sm_model = sm.OLS(y, X_with_const).fit()
    
    # çµæœã®è¡¨ç¤º
    print(f"\n=== {x_col} â†’ {y_col} ã®å›å¸°åˆ†æçµæœ ===")
    print(f"å›å¸°ä¿‚æ•°: {model.coef_[0]:.2f}")
    print(f"åˆ‡ç‰‡: {model.intercept_:.2f}")
    print(f"æ±ºå®šä¿‚æ•° (RÂ²): {r2:.4f}")
    print(f"RMSE: {rmse:.2f}")
    print(f"på€¤: {sm_model.pvalues[1]:.6f}")
    
    # è§£é‡ˆã®è‡ªå‹•ç”Ÿæˆ
    if sm_model.pvalues[1] < 0.05:
        significance = "çµ±è¨ˆçš„ã«æœ‰æ„"
    else:
        significance = "çµ±è¨ˆçš„ã«æœ‰æ„ã§ã¯ãªã„"
    
    print(f"\nğŸ“Š çµæœã®è§£é‡ˆ:")
    print(f"ãƒ»{x_col}ãŒ1å˜ä½å¢—åŠ ã™ã‚‹ã¨ã€{y_col}ã¯{model.coef_[0]:.2f}å¤‰åŒ–")
    print(f"ãƒ»ã“ã®é–¢ä¿‚ã¯{significance}ã§ã™")
    print(f"ãƒ»{x_col}ã§{y_col}ã®{r2*100:.1f}%ã‚’èª¬æ˜å¯èƒ½")
    
    # å¯è¦–åŒ–
    plt.figure(figsize=(10, 6))
    plt.scatter(X.flatten(), y, alpha=0.6, label='å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿')
    plt.plot(X.flatten(), y_pred, 'r-', linewidth=2, label=f'å›å¸°ç›´ç·š (RÂ²={r2:.3f})')
    plt.xlabel(x_col)
    plt.ylabel(y_col)
    plt.title(f'{x_col} vs {y_col} ã®å›å¸°åˆ†æ')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
    
    return model, r2, rmse, sm_model

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
df = pd.read_excel('regression_timeseries_data.xlsx', sheet_name='daily_data')

# å„è¦å› ã®åˆ†æå®Ÿè¡Œ
temp_model, temp_r2, temp_rmse, temp_sm = simple_regression_analysis(
    df, 'æ°—æ¸©', 'å£²ä¸Šé‡‘é¡'
)

ad_model, ad_r2, ad_rmse, ad_sm = simple_regression_analysis(
    df, 'åºƒå‘Šè²»', 'å£²ä¸Šé‡‘é¡'
)
```

**3.2 é‡å›å¸°åˆ†æï¼ˆè¤‡æ•°å¤‰æ•°ã®åŒæ™‚å½±éŸ¿ï¼‰**

```python
def multiple_regression_analysis(df):
    """
    é‡å›å¸°åˆ†æã‚’å®Ÿè¡Œ
    """
    # èª¬æ˜å¤‰æ•°ã®é¸æŠ
    feature_columns = ['æ°—æ¸©', 'é™æ°´é‡', 'åºƒå‘Šè²»', 'æ›œæ—¥_æ•°å€¤', 'æœˆ']
    X = df[feature_columns]
    y = df['å£²ä¸Šé‡‘é¡']
    
    # æ¬ æå€¤ã®å‡¦ç†
    X = X.fillna(X.mean())
    
    # å›å¸°ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
    model = LinearRegression()
    model.fit(X, y)
    
    # äºˆæ¸¬
    y_pred = model.predict(X)
    
    # çµ±è¨ˆæŒ‡æ¨™
    r2 = r2_score(y, y_pred)
    rmse = np.sqrt(mean_squared_error(y, y_pred))
    
    # statsmodelsã§è©³ç´°åˆ†æ
    X_with_const = sm.add_constant(X)
    sm_model = sm.OLS(y, X_with_const).fit()
    
    print("\n=== é‡å›å¸°åˆ†æçµæœ ===")
    print(f"æ±ºå®šä¿‚æ•° (RÂ²): {r2:.4f}")
    print(f"èª¿æ•´æ¸ˆã¿æ±ºå®šä¿‚æ•°: {sm_model.rsquared_adj:.4f}")
    print(f"RMSE: {rmse:.2f}")
    print(f"Fçµ±è¨ˆé‡: {sm_model.fvalue:.2f}")
    print(f"Fçµ±è¨ˆé‡ã®på€¤: {sm_model.f_pvalue:.6f}")
    
    print("\nğŸ“Š å„å¤‰æ•°ã®å½±éŸ¿åº¦:")
    for i, col in enumerate(feature_columns):
        coef = model.coef_[i]
        p_value = sm_model.pvalues[i+1]  # +1ã¯å®šæ•°é …ã®ãŸã‚
        significance = "***" if p_value < 0.001 else "**" if p_value < 0.01 else "*" if p_value < 0.05 else ""
        
        print(f"{col:10}: ä¿‚æ•° {coef:10.2f}, på€¤ {p_value:.6f} {significance}")
    
    print("\nâ­ æœ‰æ„æ°´æº–: *** p<0.001, ** p<0.01, * p<0.05")
    
    # å¤‰æ•°ã®é‡è¦åº¦å¯è¦–åŒ–
    plt.figure(figsize=(12, 8))
    
    # ä¿‚æ•°ã®å¯è¦–åŒ–
    plt.subplot(2, 2, 1)
    coefficients = model.coef_
    plt.bar(range(len(feature_columns)), coefficients)
    plt.xticks(range(len(feature_columns)), feature_columns, rotation=45)
    plt.title('å›å¸°ä¿‚æ•°')
    plt.ylabel('ä¿‚æ•°ã®å€¤')
    
    # på€¤ã®å¯è¦–åŒ–
    plt.subplot(2, 2, 2)
    p_values = sm_model.pvalues[1:]  # å®šæ•°é …ã‚’é™¤ã
    colors = ['red' if p < 0.05 else 'blue' for p in p_values]
    plt.bar(range(len(feature_columns)), -np.log10(p_values), color=colors)
    plt.xticks(range(len(feature_columns)), feature_columns, rotation=45)
    plt.title('çµ±è¨ˆçš„æœ‰æ„æ€§ (-log10(på€¤))')
    plt.axhline(-np.log10(0.05), color='red', linestyle='--', label='æœ‰æ„æ°´æº– 0.05')
    plt.legend()
    
    # å®Ÿæ¸¬å€¤ vs äºˆæ¸¬å€¤
    plt.subplot(2, 2, 3)
    plt.scatter(y, y_pred, alpha=0.6)
    plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)
    plt.xlabel('å®Ÿéš›ã®å£²ä¸Š')
    plt.ylabel('äºˆæ¸¬å£²ä¸Š')
    plt.title(f'å®Ÿæ¸¬å€¤ vs äºˆæ¸¬å€¤ (RÂ²={r2:.3f})')
    
    # æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ
    plt.subplot(2, 2, 4)
    residuals = y - y_pred
    plt.scatter(y_pred, residuals, alpha=0.6)
    plt.axhline(0, color='red', linestyle='--')
    plt.xlabel('äºˆæ¸¬å€¤')
    plt.ylabel('æ®‹å·®')
    plt.title('æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ')
    
    plt.tight_layout()
    plt.show()
    
    return model, r2, sm_model

# é‡å›å¸°åˆ†æã®å®Ÿè¡Œ
multi_model, multi_r2, multi_sm = multiple_regression_analysis(df)
```

### ğŸ¯ Step 4: Tableauã§Pythonçµæœã‚’è¡¨ç¤º

**è¨ˆç®—ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼šPythoné‡å›å¸°äºˆæ¸¬**
```
SCRIPT_REAL("
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

# å¼•æ•°ã®å—ã‘å–ã‚Š
temp = _arg1
rain = _arg2
ad_cost = _arg3
dayofweek = _arg4
month = _arg5

# ç‰¹å¾´é‡ã®çµåˆ
X = np.column_stack([temp, rain, ad_cost, dayofweek, month])

# äº‹å‰ã«å­¦ç¿’ã—ãŸä¿‚æ•°ã‚’ä½¿ç”¨ï¼ˆå®Ÿéš›ã¯å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼‰
# ã“ã“ã§ã¯ä¾‹ã¨ã—ã¦ä¿‚æ•°ã‚’ç›´æ¥æŒ‡å®š
coefficients = [20000, -10000, 1.5, 30000, 50000]
intercept = 800000

# äºˆæ¸¬ã®è¨ˆç®—
predictions = intercept + np.sum(X * coefficients, axis=1)

return predictions
",
[æ°—æ¸©], [é™æ°´é‡], [åºƒå‘Šè²»], 
DATEPART('weekday', [æ—¥ä»˜]) - 1,
DATEPART('month', [æ—¥ä»˜]))
```

**äºˆæ¸¬ç²¾åº¦ã®è¡¨ç¤º**
```
SCRIPT_REAL("
import numpy as np

actual = _arg1
predicted = _arg2

# RÂ²ã®è¨ˆç®—
ss_res = np.sum((actual - predicted) ** 2)
ss_tot = np.sum((actual - np.mean(actual)) ** 2)
r2 = 1 - (ss_res / ss_tot)

return r2
", SUM([å£²ä¸Šé‡‘é¡]), SUM([Pythoné‡å›å¸°äºˆæ¸¬]))
```

---

## 4. æ™‚ç³»åˆ—åˆ†æå®Ÿè·µç·¨ï¼šå£²ä¸Šãƒˆãƒ¬ãƒ³ãƒ‰ã¨å­£ç¯€æ€§ã‚’åˆ†æã—ã‚ˆã†

### ğŸ“ˆ Step 1: æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬ç†è§£

**æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®4ã¤ã®æˆåˆ†**
```
ğŸ“Š 1. ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆTrendï¼‰
   é•·æœŸçš„ãªå¢—åŠ ãƒ»æ¸›å°‘å‚¾å‘

ğŸ“Š 2. å­£ç¯€æ€§ï¼ˆSeasonalityï¼‰  
   å®šæœŸçš„ãªå‘¨æœŸå¤‰å‹•ï¼ˆå¹´ãƒ»æœˆãƒ»é€±ãƒ»æ—¥ï¼‰

ğŸ“Š 3. å‘¨æœŸæ€§ï¼ˆCyclicalï¼‰
   ä¸è¦å‰‡ãªé•·æœŸå‘¨æœŸï¼ˆæ™¯æ°—å¾ªç’°ãªã©ï¼‰

ğŸ“Š 4. ãƒã‚¤ã‚ºï¼ˆNoise/Randomï¼‰
   èª¬æ˜ã§ããªã„ä¸è¦å‰‡å¤‰å‹•
```

### ğŸ Step 2: Pythonã§æ™‚ç³»åˆ—åˆ†è§£

```python
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.arima.model import ARIMA
import warnings
warnings.filterwarnings('ignore')

def time_series_decomposition(df, date_col, value_col, freq=12):
    """
    æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’æˆåˆ†åˆ†è§£
    """
    # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
    ts_data = df.set_index(date_col)[value_col]
    ts_data.index = pd.to_datetime(ts_data.index)
    ts_data = ts_data.sort_index()
    
    # æ™‚ç³»åˆ—åˆ†è§£
    decomposition = seasonal_decompose(ts_data, model='additive', period=freq)
    
    # çµæœã®å¯è¦–åŒ–
    fig, axes = plt.subplots(4, 1, figsize=(15, 12))
    
    # å…ƒãƒ‡ãƒ¼ã‚¿
    decomposition.observed.plot(ax=axes[0], title='å…ƒãƒ‡ãƒ¼ã‚¿ï¼ˆå£²ä¸Šæ¨ç§»ï¼‰')
    axes[0].set_ylabel('å£²ä¸Šé‡‘é¡')
    
    # ãƒˆãƒ¬ãƒ³ãƒ‰
    decomposition.trend.plot(ax=axes[1], title='ãƒˆãƒ¬ãƒ³ãƒ‰æˆåˆ†', color='red')
    axes[1].set_ylabel('ãƒˆãƒ¬ãƒ³ãƒ‰')
    
    # å­£ç¯€æ€§
    decomposition.seasonal.plot(ax=axes[2], title='å­£ç¯€æ€§æˆåˆ†', color='green')
    axes[2].set_ylabel('å­£ç¯€æ€§')
    
    # æ®‹å·®ï¼ˆãƒã‚¤ã‚ºï¼‰
    decomposition.resid.plot(ax=axes[3], title='æ®‹å·®æˆåˆ†ï¼ˆãƒã‚¤ã‚ºï¼‰', color='purple')
    axes[3].set_ylabel('æ®‹å·®')
    
    plt.tight_layout()
    plt.show()
    
    # æˆåˆ†ã®çµ±è¨ˆã‚µãƒãƒªãƒ¼
    print("=== æ™‚ç³»åˆ—åˆ†è§£çµæœ ===")
    print(f"ãƒˆãƒ¬ãƒ³ãƒ‰æˆåˆ†ã®å‚¾ã: {np.polyfit(range(len(decomposition.trend.dropna())), decomposition.trend.dropna(), 1)[0]:.2f}")
    print(f"å­£ç¯€æ€§ã®æŒ¯å¹…: {decomposition.seasonal.max() - decomposition.seasonal.min():.2f}")
    print(f"æ®‹å·®ã®æ¨™æº–åå·®: {decomposition.resid.std():.2f}")
    
    return decomposition

def stationarity_test(ts_data):
    """
    å®šå¸¸æ€§ã®æ¤œå®šï¼ˆADFæ¤œå®šï¼‰
    """
    result = adfuller(ts_data.dropna())
    
    print("\n=== å®šå¸¸æ€§æ¤œå®šï¼ˆADFæ¤œå®šï¼‰===")
    print(f"ADFçµ±è¨ˆé‡: {result[0]:.6f}")
    print(f"på€¤: {result[1]:.6f}")
    print(f"ä½¿ç”¨ãƒ©ã‚°æ•°: {result[2]}")
    print(f"è¦³æ¸¬æ•°: {result[3]}")
    
    if result[1] <= 0.05:
        print("âœ… ãƒ‡ãƒ¼ã‚¿ã¯å®šå¸¸ã§ã™ï¼ˆp < 0.05ï¼‰")
    else:
        print("âš ï¸ ãƒ‡ãƒ¼ã‚¿ã¯éå®šå¸¸ã§ã™ï¼ˆp >= 0.05ï¼‰")
        print("   â†’ å·®åˆ†ã‚’å–ã‚‹ã‹ã€ãƒˆãƒ¬ãƒ³ãƒ‰é™¤å»ãŒå¿…è¦")
    
    return result

# æœˆæ¬¡ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
monthly_df = pd.read_excel('regression_timeseries_data.xlsx', sheet_name='monthly_data')

# æ™‚ç³»åˆ—åˆ†è§£ã®å®Ÿè¡Œ
decomp_result = time_series_decomposition(
    monthly_df, 'å¹´æœˆ', 'å£²ä¸Šé‡‘é¡', freq=12
)

# å®šå¸¸æ€§æ¤œå®š
ts_sales = monthly_df.set_index('å¹´æœˆ')['å£²ä¸Šé‡‘é¡']
ts_sales.index = pd.to_datetime(ts_sales.index)
stationarity_result = stationarity_test(ts_sales)
```

### ğŸ“Š Step 3: ARIMAãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹äºˆæ¸¬

```python
def arima_forecast(ts_data, forecast_periods=12):
    """
    ARIMAãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹æ™‚ç³»åˆ—äºˆæ¸¬
    """
    # æœ€é©ãªARIMAãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è‡ªå‹•é¸æŠ
    from statsmodels.tsa.arima.model import ARIMA
    from itertools import product
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç¯„å›²ã®è¨­å®š
    p_values = range(0, 3)
    d_values = range(0, 2)  
    q_values = range(0, 3)
    
    best_aic = np.inf
    best_params = None
    best_model = None
    
    print("ARIMAãƒ¢ãƒ‡ãƒ«ã®æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¢ç´¢ä¸­...")
    
    for p, d, q in product(p_values, d_values, q_values):
        try:
            model = ARIMA(ts_data, order=(p, d, q))
            fitted_model = model.fit()
            
            if fitted_model.aic < best_aic:
                best_aic = fitted_model.aic
                best_params = (p, d, q)
                best_model = fitted_model
                
        except:
            continue
    
    print(f"æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: ARIMA{best_params}")
    print(f"AIC: {best_aic:.2f}")
    
    # äºˆæ¸¬ã®å®Ÿè¡Œ
    forecast = best_model.forecast(steps=forecast_periods)
    forecast_ci = best_model.get_forecast(steps=forecast_periods).conf_int()
    
    # çµæœã®å¯è¦–åŒ–
    plt.figure(figsize=(15, 8))
    
    # éå»ãƒ‡ãƒ¼ã‚¿
    plt.plot(ts_data.index, ts_data.values, label='å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿', color='blue')
    
    # äºˆæ¸¬å€¤
    forecast_index = pd.date_range(start=ts_data.index[-1] + pd.DateOffset(months=1), 
                                  periods=forecast_periods, freq='MS')
    plt.plot(forecast_index, forecast, label='äºˆæ¸¬å€¤', color='red', linestyle='--')
    
    # ä¿¡é ¼åŒºé–“
    plt.fill_between(forecast_index, 
                    forecast_ci.iloc[:, 0], 
                    forecast_ci.iloc[:, 1], 
                    color='red', alpha=0.2, label='95%ä¿¡é ¼åŒºé–“')
    
    plt.title('ARIMAäºˆæ¸¬çµæœ')
    plt.xlabel('æ—¥ä»˜')
    plt.ylabel('å£²ä¸Šé‡‘é¡')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
    
    # äºˆæ¸¬çµæœã®è¡¨ç¤º
    print("\n=== äºˆæ¸¬çµæœ ===")
    for i, (date, value, lower, upper) in enumerate(
        zip(forecast_index, forecast, forecast_ci.iloc[:, 0], forecast_ci.iloc[:, 1])):
        print(f"{date.strftime('%Y-%m')}: {value:,.0f} ({lower:,.0f} - {upper:,.0f})")
    
    return best_model, forecast, forecast_ci, forecast_index

# ARIMAäºˆæ¸¬ã®å®Ÿè¡Œ
arima_model, forecast_values, forecast_ci, forecast_dates = arima_forecast(ts_sales)
```

### ğŸ¯ Step 4: Tableauã§æ™‚ç³»åˆ—åˆ†æçµæœã‚’è¡¨ç¤º

**è¨ˆç®—ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼šãƒˆãƒ¬ãƒ³ãƒ‰æˆåˆ†**
```
SCRIPT_REAL("
import pandas as pd
import numpy as np
from statsmodels.tsa.seasonal import seasonal_decompose

# ãƒ‡ãƒ¼ã‚¿ã®å—ã‘å–ã‚Š
dates = _arg1
values = _arg2

# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ä½œæˆ
df = pd.DataFrame({'date': dates, 'value': values})
df['date'] = pd.to_datetime(df['date'])
df = df.set_index('date').sort_index()

# æ™‚ç³»åˆ—åˆ†è§£
decomposition = seasonal_decompose(df['value'], model='additive', period=12)

# ãƒˆãƒ¬ãƒ³ãƒ‰æˆåˆ†ã‚’è¿”ã™
return decomposition.trend.fillna(method='bfill').fillna(method='ffill').values
", [æ—¥ä»˜], SUM([å£²ä¸Šé‡‘é¡]))
```

**è¨ˆç®—ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼šå­£ç¯€æ€§æˆåˆ†**
```
SCRIPT_REAL("
import pandas as pd
import numpy as np
from statsmodels.tsa.seasonal import seasonal_decompose

# ãƒ‡ãƒ¼ã‚¿ã®å—ã‘å–ã‚Š
dates = _arg1
values = _arg2

# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ä½œæˆ
df = pd.DataFrame({'date': dates, 'value': values})
df['date'] = pd.to_datetime(df['date'])
df = df.set_index('date').sort_index()

# æ™‚ç³»åˆ—åˆ†è§£
decomposition = seasonal_decompose(df['value'], model='additive', period=12)

# å­£ç¯€æ€§æˆåˆ†ã‚’è¿”ã™
return decomposition.seasonal.values
", [æ—¥ä»˜], SUM([å£²ä¸Šé‡‘é¡]))
```

**è¨ˆç®—ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼šARIMAäºˆæ¸¬**
```
SCRIPT_REAL("
import pandas as pd
import numpy as np
from statsmodels.tsa.arima.model import ARIMA

# ãƒ‡ãƒ¼ã‚¿ã®å—ã‘å–ã‚Š
dates = _arg1
values = _arg2

# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ä½œæˆ
df = pd.DataFrame({'date': dates, 'value': values})
df['date'] = pd.to_datetime(df['date'])
df = df.set_index('date').sort_index()

# ARIMAãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
try:
    model = ARIMA(df['value'], order=(1, 1, 1))
    fitted_model = model.fit()
    
    # 1æœŸå…ˆäºˆæ¸¬
    forecast = fitted_model.forecast(steps=1)[0]
    
    return forecast
except:
    # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç§»å‹•å¹³å‡ã‚’è¿”ã™
    return np.mean(values[-12:])
    
", [æ—¥ä»˜], SUM([å£²ä¸Šé‡‘é¡]))
```

---

## 5. é«˜åº¦ãªåˆ†æï¼šçµ„ã¿åˆã‚ã›æŠ€

### ğŸš€ Step 1: å›å¸°åˆ†æã¨æ™‚ç³»åˆ—åˆ†æã®çµ±åˆ

```python
def integrated_forecast_model(df):
    """
    å›å¸°åˆ†æã¨æ™‚ç³»åˆ—åˆ†æã‚’çµ±åˆã—ãŸäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«
    """
    # 1. å¤–éƒ¨è¦å› ã‚’è€ƒæ…®ã—ãŸå›å¸°ãƒ¢ãƒ‡ãƒ«
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.model_selection import train_test_split
    
    # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
    df['æ—¥ä»˜'] = pd.to_datetime(df['æ—¥ä»˜'])
    df['å¹´'] = df['æ—¥ä»˜'].dt.year
    df['æœˆ'] = df['æ—¥ä»˜'].dt.month
    df['æ—¥'] = df['æ—¥ä»˜'].dt.day
    df['æ›œæ—¥'] = df['æ—¥ä»˜'].dt.dayofweek
    df['å››åŠæœŸ'] = df['æ—¥ä»˜'].dt.quarter
    
    # ãƒ©ã‚°å¤‰æ•°ã®ä½œæˆï¼ˆéå»ã®å£²ä¸Šã®å½±éŸ¿ï¼‰
    df = df.sort_values('æ—¥ä»˜')
    for lag in [1, 7, 30]:
        df[f'å£²ä¸Š_lag{lag}'] = df['å£²ä¸Šé‡‘é¡'].shift(lag)
    
    # ç§»å‹•å¹³å‡ç‰¹å¾´é‡
    for window in [7, 30]:
        df[f'å£²ä¸Š_ma{window}'] = df['å£²ä¸Šé‡‘é¡'].rolling(window=window).mean()
    
    # æ¬ æå€¤ã‚’é™¤å»
    df_clean = df.dropna()
    
    # ç‰¹å¾´é‡é¸æŠ
    feature_cols = ['æ°—æ¸©', 'é™æ°´é‡', 'åºƒå‘Šè²»', 'å¹´', 'æœˆ', 'æ—¥', 'æ›œæ—¥', 'å››åŠæœŸ',
                   'å£²ä¸Š_lag1', 'å£²ä¸Š_lag7', 'å£²ä¸Š_lag30', 'å£²ä¸Š_ma7', 'å£²ä¸Š_ma30']
    
    X = df_clean[feature_cols]
    y = df_clean['å£²ä¸Šé‡‘é¡']
    
    # è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Random Forestãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
    rf_model.fit(X_train, y_train)
    
    # äºˆæ¸¬ç²¾åº¦ã®è©•ä¾¡
    train_score = rf_model.score(X_train, y_train)
    test_score = rf_model.score(X_test, y_test)
    
    print("=== çµ±åˆäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«çµæœ ===")
    print(f"è¨“ç·´ç²¾åº¦ (RÂ²): {train_score:.4f}")
    print(f"ãƒ†ã‚¹ãƒˆç²¾åº¦ (RÂ²): {test_score:.4f}")
    
    # ç‰¹å¾´é‡é‡è¦åº¦
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': rf_model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("\nç‰¹å¾´é‡é‡è¦åº¦:")
    for idx, row in feature_importance.head(10).iterrows():
        print(f"{row['feature']:15}: {row['importance']:.4f}")
    
    # å¯è¦–åŒ–
    plt.figure(figsize=(15, 10))
    
    # ç‰¹å¾´é‡é‡è¦åº¦
    plt.subplot(2, 2, 1)
    top_features = feature_importance.head(10)
    plt.barh(range(len(top_features)), top_features['importance'])
    plt.yticks(range(len(top_features)), top_features['feature'])
    plt.title('ç‰¹å¾´é‡é‡è¦åº¦')
    plt.xlabel('é‡è¦åº¦')
    
    # äºˆæ¸¬ vs å®Ÿæ¸¬
    y_pred = rf_model.predict(X_test)
    plt.subplot(2, 2, 2)
    plt.scatter(y_test, y_pred, alpha=0.6)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    plt.xlabel('å®Ÿæ¸¬å€¤')
    plt.ylabel('äºˆæ¸¬å€¤')
    plt.title(f'äºˆæ¸¬ç²¾åº¦ (RÂ²={test_score:.3f})')
    
    # æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ
    plt.subplot(2, 2, 3)
    residuals = y_test - y_pred
    plt.scatter(y_pred, residuals, alpha=0.6)
    plt.axhline(0, color='red', linestyle='--')
    plt.xlabel('äºˆæ¸¬å€¤')
    plt.ylabel('æ®‹å·®')
    plt.title('æ®‹å·®åˆ†æ')
    
    # æ™‚ç³»åˆ—ãƒ—ãƒ­ãƒƒãƒˆ
    plt.subplot(2, 2, 4)
    test_dates = df_clean.loc[y_test.index, 'æ—¥ä»˜']
    sorted_indices = test_dates.argsort()
    plt.plot(test_dates.iloc[sorted_indices], y_test.iloc[sorted_indices], 'b-', label='å®Ÿæ¸¬å€¤', alpha=0.7)
    plt.plot(test_dates.iloc[sorted_indices], y_pred[sorted_indices], 'r--', label='äºˆæ¸¬å€¤', alpha=0.7)
    plt.xlabel('æ—¥ä»˜')
    plt.ylabel('å£²ä¸Šé‡‘é¡')
    plt.title('æ™‚ç³»åˆ—äºˆæ¸¬çµæœ')
    plt.legend()
    plt.xticks(rotation=45)
    
    plt.tight_layout()
    plt.show()
    
    return rf_model, feature_importance, test_score

# çµ±åˆãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¡Œ
integrated_model, feature_imp, model_score = integrated_forecast_model(df)
```

### ğŸ“Š Step 2: ç•°å¸¸å€¤æ¤œå‡ºæ©Ÿèƒ½ä»˜ãäºˆæ¸¬

```python
def anomaly_detection_forecast(df):
    """
    ç•°å¸¸å€¤æ¤œå‡ºæ©Ÿèƒ½ä»˜ãã®å£²ä¸Šäºˆæ¸¬
    """
    from sklearn.ensemble import IsolationForest
    from scipy import stats
    
    # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
    df_ts = df.sort_values('æ—¥ä»˜').copy()
    df_ts['å£²ä¸Š_7æ—¥ç§»å‹•å¹³å‡'] = df_ts['å£²ä¸Šé‡‘é¡'].rolling(window=7).mean()
    df_ts['å£²ä¸Š_30æ—¥ç§»å‹•å¹³å‡'] = df_ts['å£²ä¸Šé‡‘é¡'].rolling(window=30).mean()
    df_ts['å£²ä¸Š_æ¨™æº–åå·®'] = df_ts['å£²ä¸Šé‡‘é¡'].rolling(window=30).std()
    
    # 1. çµ±è¨ˆçš„ç•°å¸¸å€¤æ¤œå‡ºï¼ˆZ-scoreï¼‰
    df_ts['z_score'] = np.abs(stats.zscore(df_ts['å£²ä¸Šé‡‘é¡'].fillna(df_ts['å£²ä¸Šé‡‘é¡'].mean())))
    df_ts['çµ±è¨ˆçš„ç•°å¸¸å€¤'] = df_ts['z_score'] > 3
    
    # 2. æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹ç•°å¸¸å€¤æ¤œå‡º
    features_for_anomaly = ['å£²ä¸Šé‡‘é¡', 'æ°—æ¸©', 'é™æ°´é‡', 'åºƒå‘Šè²»']
    isolation_forest = IsolationForest(contamination=0.05, random_state=42)
    df_ts['ç•°å¸¸å€¤ãƒ•ãƒ©ã‚°'] = isolation_forest.fit_predict(df_ts[features_for_anomaly].fillna(0))
    df_ts['MLç•°å¸¸å€¤'] = df_ts['ç•°å¸¸å€¤ãƒ•ãƒ©ã‚°'] == -1
    
    # 3. ãƒ“ã‚¸ãƒã‚¹ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ç•°å¸¸å€¤æ¤œå‡º
    # å‰æ—¥æ¯”50%ä»¥ä¸Šã®å¤‰å‹•ã‚’ç•°å¸¸ã¨ã™ã‚‹
    df_ts['å‰æ—¥æ¯”å¤‰åŒ–ç‡'] = df_ts['å£²ä¸Šé‡‘é¡'].pct_change()
    df_ts['ãƒ“ã‚¸ãƒã‚¹ç•°å¸¸å€¤'] = np.abs(df_ts['å‰æ—¥æ¯”å¤‰åŒ–ç‡']) > 0.5
    
    # ç•°å¸¸å€¤ã®çµ±åˆåˆ¤å®š
    df_ts['ç·åˆç•°å¸¸å€¤'] = (df_ts['çµ±è¨ˆçš„ç•°å¸¸å€¤'] | df_ts['MLç•°å¸¸å€¤'] | df_ts['ãƒ“ã‚¸ãƒã‚¹ç•°å¸¸å€¤'])
    
    # ç•°å¸¸å€¤ã‚’é™¤ã„ãŸäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«
    df_normal = df_ts[~df_ts['ç·åˆç•°å¸¸å€¤']].copy()
    
    # çµæœè¡¨ç¤º
    print("=== ç•°å¸¸å€¤æ¤œå‡ºçµæœ ===")
    print(f"ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(df_ts)}")
    print(f"çµ±è¨ˆçš„ç•°å¸¸å€¤: {df_ts['çµ±è¨ˆçš„ç•°å¸¸å€¤'].sum()}ä»¶")
    print(f"MLç•°å¸¸å€¤: {df_ts['MLç•°å¸¸å€¤'].sum()}ä»¶")
    print(f"ãƒ“ã‚¸ãƒã‚¹ç•°å¸¸å€¤: {df_ts['ãƒ“ã‚¸ãƒã‚¹ç•°å¸¸å€¤'].sum()}ä»¶")
    print(f"ç·åˆç•°å¸¸å€¤: {df_ts['ç·åˆç•°å¸¸å€¤'].sum()}ä»¶ ({df_ts['ç·åˆç•°å¸¸å€¤'].mean()*100:.1f}%)")
    
    # å¯è¦–åŒ–
    plt.figure(figsize=(15, 12))
    
    # å£²ä¸Šæ¨ç§»ã¨ç•°å¸¸å€¤
    plt.subplot(3, 1, 1)
    plt.plot(df_ts['æ—¥ä»˜'], df_ts['å£²ä¸Šé‡‘é¡'], 'b-', label='å£²ä¸Šé‡‘é¡', alpha=0.7)
    plt.plot(df_ts['æ—¥ä»˜'], df_ts['å£²ä¸Š_30æ—¥ç§»å‹•å¹³å‡'], 'g-', label='30æ—¥ç§»å‹•å¹³å‡', linewidth=2)
    
    # ç•°å¸¸å€¤ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ
    anomaly_data = df_ts[df_ts['ç·åˆç•°å¸¸å€¤']]
    plt.scatter(anomaly_data['æ—¥ä»˜'], anomaly_data['å£²ä¸Šé‡‘é¡'], 
               color='red', s=50, label='ç•°å¸¸å€¤', zorder=5)
    
    plt.title('å£²ä¸Šæ¨ç§»ã¨ç•°å¸¸å€¤æ¤œå‡º')
    plt.ylabel('å£²ä¸Šé‡‘é¡')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Z-scoreãƒ—ãƒ­ãƒƒãƒˆ
    plt.subplot(3, 1, 2)
    plt.plot(df_ts['æ—¥ä»˜'], df_ts['z_score'], 'purple', alpha=0.7)
    plt.axhline(3, color='red', linestyle='--', label='ç•°å¸¸å€¤é–¾å€¤ (Z=3)')
    plt.title('Z-scoreï¼ˆçµ±è¨ˆçš„ç•°å¸¸åº¦ï¼‰')
    plt.ylabel('Z-score')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # å‰æ—¥æ¯”å¤‰åŒ–ç‡
    plt.subplot(3, 1, 3)
    plt.plot(df_ts['æ—¥ä»˜'], df_ts['å‰æ—¥æ¯”å¤‰åŒ–ç‡'] * 100, 'orange', alpha=0.7)
    plt.axhline(50, color='red', linestyle='--', label='ç•°å¸¸å€¤é–¾å€¤ (+50%)')
    plt.axhline(-50, color='red', linestyle='--', label='ç•°å¸¸å€¤é–¾å€¤ (-50%)')
    plt.title('å‰æ—¥æ¯”å¤‰åŒ–ç‡')
    plt.ylabel('å¤‰åŒ–ç‡ (%)')
    plt.xlabel('æ—¥ä»˜')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    return df_ts, df_normal

# ç•°å¸¸å€¤æ¤œå‡ºã®å®Ÿè¡Œ
df_with_anomaly, df_clean_data = anomaly_detection_forecast(df)
```

### ğŸ¯ Step 3: Tableauã§ã®é«˜åº¦ãªåˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

**è¨ˆç®—ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼šçµ±åˆäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«**
```
SCRIPT_REAL("
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor

# ç‰¹å¾´é‡ã®å—ã‘å–ã‚Š
temp = _arg1
rain = _arg2
ad_cost = _arg3
month = _arg4
dayofweek = _arg5
lag1 = _arg6
lag7 = _arg7
ma7 = _arg8

# ç‰¹å¾´é‡ã®çµåˆ
X = np.column_stack([temp, rain, ad_cost, month, dayofweek, lag1, lag7, ma7])

# äº‹å‰å­¦ç¿’æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ä¿‚æ•°ã‚’ä½¿ç”¨ï¼ˆç°¡ç•¥åŒ–ï¼‰
# å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
weights = [15000, -8000, 1.2, 25000, 30000, 0.3, 0.2, 0.4]
intercept = 200000

# äºˆæ¸¬è¨ˆç®—
predictions = intercept + np.sum(X * weights, axis=1)

return predictions
",
[æ°—æ¸©], [é™æ°´é‡], [åºƒå‘Šè²»], 
DATEPART('month', [æ—¥ä»˜]),
DATEPART('weekday', [æ—¥ä»˜]),
LOOKUP(SUM([å£²ä¸Šé‡‘é¡]), -1),
WINDOW_AVG(SUM([å£²ä¸Šé‡‘é¡]), -6, 0),
WINDOW_AVG(SUM([å£²ä¸Šé‡‘é¡]), -6, 0))
```

**è¨ˆç®—ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼šç•°å¸¸å€¤ã‚¹ã‚³ã‚¢**
```
SCRIPT_REAL("
import numpy as np
from scipy import stats

# å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã®å—ã‘å–ã‚Š
sales_data = _arg1

# Z-scoreã«ã‚ˆã‚‹ç•°å¸¸åº¦è¨ˆç®—
z_scores = np.abs(stats.zscore(sales_data))

# å‰æ—¥æ¯”å¤‰åŒ–ç‡ã®è¨ˆç®—
pct_change = np.abs(np.diff(sales_data) / sales_data[:-1])
pct_change = np.concatenate([[0], pct_change])  # æœ€åˆã®å€¤ã¯0

# ç·åˆç•°å¸¸åº¦ã‚¹ã‚³ã‚¢
anomaly_score = z_scores + pct_change * 10

return anomaly_score
", SUM([å£²ä¸Šé‡‘é¡]))
```

**ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  ğŸ“Š å£²ä¸Šåˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ›ï¸ ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒ‘ãƒãƒ«                                       â”‚
â”‚ [æœŸé–“é¸æŠ] [åº—èˆ—é¸æŠ] [äºˆæ¸¬æœŸé–“] [ä¿¡é ¼åŒºé–“]                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ“ˆ ãƒ¡ã‚¤ãƒ³æ™‚ç³»åˆ—ãƒãƒ£ãƒ¼ãƒˆ                                     â”‚
â”‚ ãƒ»å£²ä¸Šå®Ÿç¸¾ï¼ˆé’ç·šï¼‰                                          â”‚
â”‚ ãƒ»å›å¸°äºˆæ¸¬ï¼ˆã‚ªãƒ¬ãƒ³ã‚¸ç·šï¼‰                                    â”‚
â”‚ ãƒ»æ™‚ç³»åˆ—äºˆæ¸¬ï¼ˆç·‘ç·šï¼‰                                        â”‚
â”‚ ãƒ»çµ±åˆäºˆæ¸¬ï¼ˆèµ¤ç·šï¼‰                                          â”‚
â”‚ ãƒ»ç•°å¸¸å€¤ãƒãƒ¼ã‚«ãƒ¼ï¼ˆèµ¤ç‚¹ï¼‰                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ“Š æˆåˆ†åˆ†è§£      â”‚ ğŸ” å›å¸°åˆ†æçµæœ  â”‚ ğŸš¨ ç•°å¸¸å€¤ã‚¢ãƒ©ãƒ¼ãƒˆ        â”‚
â”‚ ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰      â”‚ ãƒ»ä¿‚æ•°è¡¨ç¤º      â”‚ ãƒ»ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–      â”‚
â”‚ ãƒ»å­£ç¯€æ€§        â”‚ ãƒ»æœ‰æ„æ€§æ¤œå®š    â”‚ ãƒ»é–¾å€¤è¨­å®š              â”‚
â”‚ ãƒ»æ®‹å·®          â”‚ ãƒ»RÂ²å€¤          â”‚ ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆå±¥æ­´          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ“ˆ äºˆæ¸¬ç²¾åº¦æŒ‡æ¨™  â”‚ ğŸ¯ ãƒ“ã‚¸ãƒã‚¹æŒ‡æ¨™  â”‚ ğŸ“‹ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³     â”‚
â”‚ ãƒ»MAPE         â”‚ ãƒ»å£²ä¸Šç›®æ¨™é”æˆç‡ â”‚ ãƒ»æ¨å¥¨æ–½ç­–              â”‚
â”‚ ãƒ»RMSE         â”‚ ãƒ»æˆé•·ç‡        â”‚ ãƒ»ãƒªã‚¹ã‚¯è¦å›             â”‚
â”‚ ãƒ»RÂ²           â”‚ ãƒ»å­£ç¯€èª¿æ•´å£²ä¸Š  â”‚ ãƒ»æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 6. å®Ÿå‹™æ´»ç”¨äº‹ä¾‹ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### ğŸ’¼ äº‹ä¾‹1ï¼šå°å£²æ¥­ã®éœ€è¦äºˆæ¸¬

**èª²é¡Œ**
- å­£ç¯€å•†å“ã®ä»•å…¥ã‚Œé‡æœ€é©åŒ–
- å»ƒæ£„ãƒ­ã‚¹å‰Šæ¸›ã¨ãƒãƒ£ãƒ³ã‚¹ãƒ­ã‚¹é˜²æ­¢

**åˆ†æã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**
```python
def retail_demand_forecast():
    """
    å°å£²æ¥­å‘ã‘éœ€è¦äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«
    """
    # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
    features = {
        'base_demand': 'åŸºæœ¬éœ€è¦ï¼ˆéå»å¹³å‡ï¼‰',
        'temperature': 'æ°—æ¸©å½±éŸ¿',
        'weather': 'å¤©å€™å½±éŸ¿', 
        'holiday': 'ç¥æ—¥å½±éŸ¿',
        'promotion': 'ã‚»ãƒ¼ãƒ«ãƒ»ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³',
        'competitor': 'ç«¶åˆçŠ¶æ³',
        'inventory': 'åœ¨åº«çŠ¶æ³',
        'price': 'ä¾¡æ ¼å¤‰å‹•'
    }
    
    # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ä¾‹
    model_config = {
        'short_term': 'ARIMA + å¤–éƒ¨è¦å› å›å¸°',
        'medium_term': 'Random Forest + å­£ç¯€åˆ†è§£',
        'long_term': 'Linear Trend + çµŒæ¸ˆæŒ‡æ¨™'
    }
    
    return features, model_config

# å®Ÿè£…ä¾‹
retail_features, retail_models = retail_demand_forecast()
```

**æˆæœæŒ‡æ¨™**
- äºˆæ¸¬ç²¾åº¦å‘ä¸Šï¼š65% â†’ 89%
- å»ƒæ£„ãƒ­ã‚¹å‰Šæ¸›ï¼š15%
- æ¬ å“ç‡å‰Šæ¸›ï¼š8%
- åœ¨åº«å›è»¢ç‡å‘ä¸Šï¼š1.2å€

### ğŸ’¼ äº‹ä¾‹2ï¼šè£½é€ æ¥­ã®ç”Ÿç”£è¨ˆç”»

**èª²é¡Œ**
- éœ€è¦å¤‰å‹•ã«å¯¾å¿œã—ãŸç”Ÿç”£è¨ˆç”»
- åŸææ–™èª¿é”ã®æœ€é©åŒ–

**åˆ†ææ‰‹æ³•**
```python
def manufacturing_production_plan():
    """
    è£½é€ æ¥­å‘ã‘ç”Ÿç”£è¨ˆç”»ãƒ¢ãƒ‡ãƒ«
    """
    # éœ€è¦äºˆæ¸¬
    demand_factors = [
        'éå»å—æ³¨å®Ÿç¸¾',
        'å–¶æ¥­ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³',
        'å¸‚å ´ãƒˆãƒ¬ãƒ³ãƒ‰',
        'çµŒæ¸ˆæŒ‡æ¨™',
        'ç«¶åˆå‹•å‘'
    ]
    
    # ç”Ÿç”£èƒ½åŠ›åˆ¶ç´„
    capacity_constraints = [
        'è¨­å‚™ç¨¼åƒç‡',
        'äººå“¡é…ç½®',
        'åŸææ–™åœ¨åº«',
        'ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹äºˆå®š',
        'å“è³ªè¦æ±‚æ°´æº–'
    ]
    
    return demand_factors, capacity_constraints
```

### ğŸ† ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

**1. ãƒ‡ãƒ¼ã‚¿å“è³ªã®ç¢ºä¿**
```python
def data_quality_check(df):
    """
    ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯é–¢æ•°
    """
    quality_report = {
        'æ¬ æå€¤ç‡': df.isnull().sum() / len(df),
        'é‡è¤‡ãƒ‡ãƒ¼ã‚¿': df.duplicated().sum(),
        'ç•°å¸¸å€¤': 'å„åˆ—ã®Z-score > 3ã®ä»¶æ•°',
        'ãƒ‡ãƒ¼ã‚¿å‹': df.dtypes,
        'å€¤ã®ç¯„å›²': df.describe()
    }
    return quality_report
```

**2. ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ç›£è¦–**
```python
def model_monitoring():
    """
    ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®ç¶™ç¶šç›£è¦–
    """
    monitoring_metrics = {
        'äºˆæ¸¬ç²¾åº¦': 'MAPE, RMSE, RÂ²ã®æ¨ç§»',
        'ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º': 'äºˆæ¸¬åˆ†å¸ƒã®å¤‰åŒ–',
        'ç‰¹å¾´é‡é‡è¦åº¦': 'èª¬æ˜å¤‰æ•°ã®å½±éŸ¿åº¦å¤‰åŒ–',
        'ãƒ“ã‚¸ãƒã‚¹å½±éŸ¿': 'å®Ÿéš›ã®æ„æ€æ±ºå®šã¸ã®è²¢çŒ®'
    }
    return monitoring_metrics
```

**3. è§£é‡ˆå¯èƒ½æ€§ã®ç¢ºä¿**
```python
def model_explainability():
    """
    ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆå¯èƒ½æ€§ç¢ºä¿
    """
    explanation_methods = {
        'SHAPå€¤': 'å„ç‰¹å¾´é‡ã®äºˆæ¸¬ã¸ã®è²¢çŒ®åº¦',
        'LIME': 'ãƒ­ãƒ¼ã‚«ãƒ«ãªèª¬æ˜',
        'ç‰¹å¾´é‡é‡è¦åº¦': 'ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªé‡è¦åº¦',
        'éƒ¨åˆ†ä¾å­˜ãƒ—ãƒ­ãƒƒãƒˆ': 'å„å¤‰æ•°ã®å½±éŸ¿é–¢æ•°'
    }
    return explanation_methods
```

---

## 7. ã‚ˆãã‚ã‚‹èª²é¡Œã¨è§£æ±ºç­–

### âŒ èª²é¡Œ1ï¼šäºˆæ¸¬ç²¾åº¦ãŒä½ã„

**ç—‡çŠ¶**
- RÂ²ãŒ0.5ä»¥ä¸‹
- å®Ÿæ¸¬å€¤ã¨äºˆæ¸¬å€¤ã®ä¹–é›¢ãŒå¤§ãã„
- ãƒ“ã‚¸ãƒã‚¹ã§ä½¿ãˆãªã„ãƒ¬ãƒ™ãƒ«

**åŸå› åˆ†æã¨å¯¾ç­–**
```python
def improve_prediction_accuracy():
    """
    äºˆæ¸¬ç²¾åº¦æ”¹å–„ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
    """
    improvement_steps = {
        '1. ãƒ‡ãƒ¼ã‚¿å“è³ªå‘ä¸Š': [
            'æ¬ æå€¤ã®é©åˆ‡ãªè£œå®Œ',
            'ç•°å¸¸å€¤ã®æ¤œå‡ºãƒ»å‡¦ç†',
            'ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿åé›†'
        ],
        '2. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°': [
            'ãƒ©ã‚°å¤‰æ•°ã®è¿½åŠ ',
            'ç§»å‹•å¹³å‡ãƒ»å·®åˆ†ã®æ´»ç”¨',
            'äº¤äº’ä½œç”¨é …ã®æ¤œè¨',
            'å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ'
        ],
        '3. ãƒ¢ãƒ‡ãƒ«æ”¹è‰¯': [
            'è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«',
            'ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´',
            'éç·šå½¢ãƒ¢ãƒ‡ãƒ«ã®æ¤œè¨',
            'æ™‚ç³»åˆ—å°‚ç”¨ãƒ¢ãƒ‡ãƒ«ã®æ´»ç”¨'
        ],
        '4. è©•ä¾¡æ–¹æ³•è¦‹ç›´ã—': [
            'äº¤å·®æ¤œè¨¼ã®å®Ÿæ–½',
            'æ™‚ç³»åˆ—åˆ†å‰²ã®é©ç”¨',
            'è¤‡æ•°ã®è©•ä¾¡æŒ‡æ¨™ä½¿ç”¨'
        ]
    }
    return improvement_steps
```

### âŒ èª²é¡Œ2ï¼šTableauã§ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚¨ãƒ©ãƒ¼

**ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã¨è§£æ±ºæ³•**
```python
def tableau_script_troubleshooting():
    """
    Tableauã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚¨ãƒ©ãƒ¼ã®å¯¾å‡¦æ³•
    """
    common_errors = {
        'Import Error': {
            'åŸå› ': 'ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„',
            'è§£æ±ºæ³•': 'pip install ã§å¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«'
        },
        'Type Error': {
            'åŸå› ': 'ãƒ‡ãƒ¼ã‚¿å‹ã®ä¸ä¸€è‡´',
            'è§£æ±ºæ³•': 'Tableauå´ã§ãƒ‡ãƒ¼ã‚¿å‹ã‚’ç¢ºèªãƒ»å¤‰æ›'
        },
        'Timeout Error': {
            'åŸå› ': 'ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè¡Œæ™‚é–“ãŒé•·ã™ãã‚‹',
            'è§£æ±ºæ³•': 'ãƒ‡ãƒ¼ã‚¿é‡å‰Šæ¸›ãƒ»å‡¦ç†ã®æœ€é©åŒ–'
        },
        'Memory Error': {
            'åŸå› ': 'ãƒ¡ãƒ¢ãƒªä¸è¶³',
            'è§£æ±ºæ³•': 'ãƒãƒƒãƒå‡¦ç†ãƒ»ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã®æ´»ç”¨'
        }
    }
    return common_errors
```

### âŒ èª²é¡Œ3ï¼šãƒ“ã‚¸ãƒã‚¹è¦ä»¶ã¨ã®ä¹–é›¢

**å•é¡Œ**
- çµ±è¨ˆçš„ã«ã¯æ­£ã—ã„ãŒå®Ÿå‹™ã§ä½¿ãˆãªã„
- ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼ã®ç†è§£ãŒå¾—ã‚‰ã‚Œãªã„

**è§£æ±ºã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**
```python
def business_alignment():
    """
    ãƒ“ã‚¸ãƒã‚¹è¦ä»¶ã¨ã®æ•´åˆæ€§ç¢ºä¿
    """
    alignment_strategies = {
        'è¦ä»¶å®šç¾©ã®æ˜ç¢ºåŒ–': [
            'äºˆæ¸¬ç²¾åº¦ã®ç›®æ¨™è¨­å®š',
            'ä½¿ç”¨ç›®çš„ã®å…·ä½“åŒ–',
            'æ„æ€æ±ºå®šãƒ—ãƒ­ã‚»ã‚¹ã®ç†è§£'
        ],
        'è§£é‡ˆã—ã‚„ã™ã„æŒ‡æ¨™': [
            'æ¥­ç•Œæ¨™æº–æŒ‡æ¨™ã®ä½¿ç”¨',
            'ç›´æ„Ÿçš„ãªå¯è¦–åŒ–',
            'åˆ†ã‹ã‚Šã‚„ã™ã„èª¬æ˜æ–‡'
        ],
        'æ®µéšçš„å°å…¥': [
            'ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆé‹ç”¨',
            'ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›†',
            'ç¶™ç¶šçš„æ”¹å–„'
        ]
    }
    return alignment_strategies
```

---

## 8. ã‚¹ã‚­ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

### ğŸ“š åˆç´šãƒ¬ãƒ™ãƒ«ï¼ˆ1-3ãƒ¶æœˆï¼‰

**Week 1-4: åŸºç¤ç†è§£**
- çµ±è¨ˆã®åŸºæœ¬æ¦‚å¿µç†è§£
- Pythonã®åŸºæœ¬æ“ä½œç¿’å¾—
- Tableauã§ã®åŸºæœ¬å¯è¦–åŒ–

**Month 2-3: å®Ÿè·µç·´ç¿’**
- ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ã®å›å¸°åˆ†æ
- æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–
- äºˆæ¸¬çµæœã®è§£é‡ˆç·´ç¿’

**ç¿’å¾—ç›®æ¨™**
- å˜å›å¸°åˆ†æãŒã§ãã‚‹
- æ™‚ç³»åˆ—ã®åŸºæœ¬æ¦‚å¿µã‚’ç†è§£
- Tableauã§Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒä½¿ãˆã‚‹

### ğŸ“ˆ ä¸­ç´šãƒ¬ãƒ™ãƒ«ï¼ˆ3-8ãƒ¶æœˆï¼‰

**Month 4-6: é«˜åº¦ãªåˆ†ææ‰‹æ³•**
- é‡å›å¸°åˆ†æã¨ãƒ¢ãƒ‡ãƒ«é¸æŠ
- ARIMAãƒ»å­£ç¯€åˆ†è§£ã®ç†è§£
- ç•°å¸¸å€¤æ¤œå‡ºæ‰‹æ³•ã®ç¿’å¾—

**Month 7-8: å®Ÿå‹™å¿œç”¨**
- å®Ÿéš›ã®ãƒ“ã‚¸ãƒã‚¹ãƒ‡ãƒ¼ã‚¿ã§åˆ†æ
- çµ±åˆäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
- ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®é«˜åº¦åŒ–

**ç¿’å¾—ç›®æ¨™**
- è¤‡é›‘ãªãƒ“ã‚¸ãƒã‚¹èª²é¡Œã‚’åˆ†æã§ãã‚‹
- ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦è©•ä¾¡ãƒ»æ”¹å–„ãŒã§ãã‚‹
- ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼ã«çµæœã‚’èª¬æ˜ã§ãã‚‹

### ğŸš€ ä¸Šç´šãƒ¬ãƒ™ãƒ«ï¼ˆ8ãƒ¶æœˆä»¥ä¸Šï¼‰

**é«˜åº¦ãªæ©Ÿæ¢°å­¦ç¿’**
- ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã®æ™‚ç³»åˆ—äºˆæ¸¬
- å¼·åŒ–å­¦ç¿’ã«ã‚ˆã‚‹æœ€é©åŒ–
- å› æœæ¨è«–æ‰‹æ³•ã®æ´»ç”¨

**ã‚·ã‚¹ãƒ†ãƒ åŒ–ãƒ»è‡ªå‹•åŒ–**
- MLOpsãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ 
- A/Bãƒ†ã‚¹ãƒˆè¨­è¨ˆãƒ»åˆ†æ

**çµ„ç¹”å±•é–‹**
- ãƒ‡ãƒ¼ã‚¿åˆ†ææ–‡åŒ–ã®é†¸æˆ
- ãƒãƒ¼ãƒ æ•™è‚²ãƒ»æŒ‡å°
- æˆ¦ç•¥ç«‹æ¡ˆã¸ã®è²¢çŒ®

### ğŸ“– æ¨å¥¨å­¦ç¿’ãƒªã‚½ãƒ¼ã‚¹

**æ›¸ç±**
- ã€çµ±è¨ˆå­¦ãŒæœ€å¼·ã®å­¦å•ã§ã‚ã‚‹ã€
- ã€æ™‚ç³»åˆ—åˆ†æã¨äºˆæ¸¬ã€
- ã€æ©Ÿæ¢°å­¦ç¿’ã®ãŸã‚ã®ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã€

**ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚³ãƒ¼ã‚¹**
- Coursera: Machine Learning
- edX: Statistical Analysis
- Udemy: Time Series Analysis

**å®Ÿè·µãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ **
- Kaggle competitions
- Tableau Public
- GitHub projects

---

## ã¾ã¨ã‚

### ğŸ¯ ã“ã®è¨˜äº‹ã§ç¿’å¾—ã§ããŸã‚¹ã‚­ãƒ«

âœ… **å›å¸°åˆ†æã®å®Ÿè·µæ´»ç”¨**
- å˜å›å¸°ã‹ã‚‰é‡å›å¸°ã¾ã§å®Œå…¨ç†è§£
- ãƒ“ã‚¸ãƒã‚¹è¦å› ã®å®šé‡åŒ–æ‰‹æ³•
- äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ãƒ»è©•ä¾¡

âœ… **æ™‚ç³»åˆ—åˆ†æã®å°‚é–€çŸ¥è­˜**
- ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»å­£ç¯€æ€§ã®åˆ†è§£
- ARIMAãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹äºˆæ¸¬
- ç•°å¸¸å€¤æ¤œå‡ºãƒ»ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 

âœ… **TableauÃ—Pythoné€£æºæŠ€è¡“**
- é«˜åº¦ãªè¨ˆç®—ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ä½œæˆ
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
- çµ±åˆåˆ†æãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æ§‹ç¯‰

âœ… **å®Ÿå‹™ã§ã®å•é¡Œè§£æ±ºèƒ½åŠ›**
- ãƒ“ã‚¸ãƒã‚¹èª²é¡Œã®æ•°å€¤åŒ–
- ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ–ãƒ³ãªæ„æ€æ±ºå®šæ”¯æ´
- ROIæ¸¬å®šå¯èƒ½ãªåˆ†æã‚·ã‚¹ãƒ†ãƒ 

### ğŸš€ ä»Šå¾Œã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³

**ä»Šã™ãå§‹ã‚ã‚‹ï¼ˆä»Šæ—¥ä¸­ï¼‰**
1. ç’°å¢ƒæ§‹ç¯‰ã®å®Œäº†ç¢ºèª
2. ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ã®å‹•ä½œãƒ†ã‚¹ãƒˆ
3. åŸºæœ¬çš„ãªåˆ†æã®å®Ÿè¡Œ

**1é€±é–“ä»¥å†…**
1. å®Ÿéš›ã®ãƒ“ã‚¸ãƒã‚¹ãƒ‡ãƒ¼ã‚¿ã§ç·´ç¿’
2. ãƒãƒ¼ãƒ ãƒ¡ãƒ³ãƒãƒ¼ã¨ã®çµæœå…±æœ‰
3. æ”¹å–„ç‚¹ã®æ´—ã„å‡ºã—

**1ãƒ¶æœˆä»¥å†…**
1. æœ¬æ ¼çš„ãªäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰
2. ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®å®šæœŸæ›´æ–°è‡ªå‹•åŒ–
3. é–¢ä¿‚éƒ¨ç½²ã¸ã®æˆæœå ±å‘Š

### ğŸ’¡ æˆåŠŸã®ãŸã‚ã®å¿ƒæ§‹ãˆ

**åˆ†æã¯æ‰‹æ®µã€ä¾¡å€¤å‰µå‡ºãŒç›®çš„**
- é«˜ç²¾åº¦ãªäºˆæ¸¬ã‚ˆã‚Šã‚‚ã€Œä½¿ãˆã‚‹äºˆæ¸¬ã€ã‚’é‡è¦–
- ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼ã¨ã®å¯¾è©±ã‚’å¤§åˆ‡ã«
- å°ã•ãªæˆåŠŸã‹ã‚‰ç©ã¿é‡ã­ã‚‹

**ç¶™ç¶šçš„å­¦ç¿’ã®é‡è¦æ€§**
- æ–°ã—ã„æ‰‹æ³•ã«å¸¸ã«ã‚¢ãƒ³ãƒ†ãƒŠã‚’å¼µã‚‹
- å¤±æ•—ã‚’æã‚Œãšè©¦è¡ŒéŒ¯èª¤ã™ã‚‹
- ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã§ã®çŸ¥è­˜å…±æœ‰

**ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤ã®æœ€å¤§åŒ–**
- æ•°å­—ã ã‘ã§ãªãã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’èªã‚‹
- æ„æ€æ±ºå®šã¸ã®ç›´æ¥çš„è²¢çŒ®ã‚’æ„è­˜
- ROIã‚’å¸¸ã«æ¸¬å®šãƒ»æ”¹å–„

---

## 9. ä»˜éŒ²ï¼šã‚³ãƒ¼ãƒ‰é›†ã¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

### ğŸ”§ ä¾¿åˆ©ãªé–¢æ•°ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

```python
# çµ±åˆåˆ†æãƒ©ã‚¤ãƒ–ãƒ©ãƒª
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

class SalesAnalysisToolkit:
    """
    å£²ä¸Šåˆ†æç”¨çµ±åˆãƒ„ãƒ¼ãƒ«ã‚­ãƒƒãƒˆ
    """
    
    def __init__(self):
        self.models = {}
        self.results = {}
    
    def load_data(self, file_path, sheet_name='Sheet1'):
        """
        Excelãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†
        """
        try:
            df = pd.read_excel(file_path, sheet_name=sheet_name)
            
            # åŸºæœ¬çš„ãªå‰å‡¦ç†
            df.columns = df.columns.str.strip()  # åˆ—åã®ç©ºç™½é™¤å»
            
            # æ—¥ä»˜åˆ—ã®è‡ªå‹•æ¤œå‡ºã¨å¤‰æ›
            date_columns = df.select_dtypes(include=['object']).columns
            for col in date_columns:
                try:
                    df[col] = pd.to_datetime(df[col])
                    print(f"âœ… {col}ã‚’æ—¥ä»˜å‹ã«å¤‰æ›ã—ã¾ã—ãŸ")
                    break
                except:
                    continue
            
            print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {df.shape[0]}è¡Œ Ã— {df.shape[1]}åˆ—")
            return df
            
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def quick_eda(self, df, target_col):
        """
        ã‚¯ã‚¤ãƒƒã‚¯æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æ
        """
        print("=== ã‚¯ã‚¤ãƒƒã‚¯EDAçµæœ ===")
        
        # åŸºæœ¬çµ±è¨ˆ
        print(f"\nğŸ“ˆ {target_col}ã®åŸºæœ¬çµ±è¨ˆ:")
        print(df[target_col].describe())
        
        # æ¬ æå€¤ãƒã‚§ãƒƒã‚¯
        missing_pct = (df.isnull().sum() / len(df) * 100).round(2)
        missing_info = missing_pct[missing_pct > 0]
        if len(missing_info) > 0:
            print(f"\nâš ï¸ æ¬ æå€¤:")
            for col, pct in missing_info.items():
                print(f"  {col}: {pct}%")
        else:
            print("\nâœ… æ¬ æå€¤ãªã—")
        
        # ç›¸é–¢åˆ†æ
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 1:
            correlation_with_target = df[numeric_cols].corr()[target_col].abs().sort_values(ascending=False)
            print(f"\nğŸ”— {target_col}ã¨ã®ç›¸é–¢ä¿‚æ•° (ä¸Šä½5ä½):")
            for col, corr in correlation_with_target.head(6).items():  # 6ä½ã¾ã§(è‡ªå·±ç›¸é–¢é™¤ã)
                if col != target_col:
                    print(f"  {col}: {corr:.3f}")
    
    def regression_analysis(self, df, target_col, feature_cols=None, test_size=0.2):
        """
        åŒ…æ‹¬çš„å›å¸°åˆ†æ
        """
        from sklearn.linear_model import LinearRegression
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.preprocessing import StandardScaler
        
        # ç‰¹å¾´é‡ã®è‡ªå‹•é¸æŠ
        if feature_cols is None:
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            feature_cols = [col for col in numeric_cols if col != target_col]
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        X = df[feature_cols].fillna(df[feature_cols].mean())
        y = df[target_col].fillna(df[target_col].mean())
        
        # è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)
        
        # è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã§ã®åˆ†æ
        models = {
            'Linear Regression': LinearRegression(),
            'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42)
        }
        
        results = {}
        
        for name, model in models.items():
            # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
            model.fit(X_train, y_train)
            
            # äºˆæ¸¬
            y_pred_train = model.predict(X_train)
            y_pred_test = model.predict(X_test)
            
            # è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
            results[name] = {
                'train_r2': r2_score(y_train, y_pred_train),
                'test_r2': r2_score(y_test, y_pred_test),
                'train_rmse': np.sqrt(mean_squared_error(y_train, y_pred_train)),
                'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test)),
                'test_mae': mean_absolute_error(y_test, y_pred_test)
            }
            
            # Random Forestã®å ´åˆã€ç‰¹å¾´é‡é‡è¦åº¦ã‚‚ä¿å­˜
            if hasattr(model, 'feature_importances_'):
                results[name]['feature_importance'] = dict(zip(feature_cols, model.feature_importances_))
        
        # çµæœè¡¨ç¤º
        print("=== å›å¸°åˆ†æçµæœ ===")
        for name, result in results.items():
            print(f"\nğŸ¤– {name}:")
            print(f"  è¨“ç·´RÂ²: {result['train_r2']:.4f}")
            print(f"  ãƒ†ã‚¹ãƒˆRÂ²: {result['test_r2']:.4f}")
            print(f"  ãƒ†ã‚¹ãƒˆRMSE: {result['test_rmse']:,.0f}")
            print(f"  ãƒ†ã‚¹ãƒˆMAE: {result['test_mae']:,.0f}")
            
            if 'feature_importance' in result:
                print("  ç‰¹å¾´é‡é‡è¦åº¦ (ä¸Šä½5ä½):")
                sorted_importance = sorted(result['feature_importance'].items(), 
                                         key=lambda x: x[1], reverse=True)
                for feature, importance in sorted_importance[:5]:
                    print(f"    {feature}: {importance:.4f}")
        
        self.results['regression'] = results
        return results
    
    def time_series_analysis(self, df, date_col, value_col, forecast_periods=12):
        """
        æ™‚ç³»åˆ—åˆ†æã¨äºˆæ¸¬
        """
        from statsmodels.tsa.seasonal import seasonal_decompose
        from statsmodels.tsa.arima.model import ARIMA
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        ts_data = df.set_index(date_col)[value_col].sort_index()
        
        # æ™‚ç³»åˆ—åˆ†è§£
        try:
            decomposition = seasonal_decompose(ts_data, model='additive', period=12)
            
            # å¯è¦–åŒ–
            fig, axes = plt.subplots(4, 1, figsize=(15, 12))
            
            decomposition.observed.plot(ax=axes[0], title='åŸãƒ‡ãƒ¼ã‚¿')
            decomposition.trend.plot(ax=axes[1], title='ãƒˆãƒ¬ãƒ³ãƒ‰', color='red')
            decomposition.seasonal.plot(ax=axes[2], title='å­£ç¯€æ€§', color='green')
            decomposition.resid.plot(ax=axes[3], title='æ®‹å·®', color='purple')
            
            plt.tight_layout()
            plt.show()
            
        except Exception as e:
            print(f"âš ï¸ æ™‚ç³»åˆ—åˆ†è§£ã‚¨ãƒ©ãƒ¼: {e}")
            decomposition = None
        
        # ARIMAäºˆæ¸¬
        try:
            # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç°¡æ˜“æ¢ç´¢
            best_aic = np.inf
            best_model = None
            
            for p in range(3):
                for d in range(2):
                    for q in range(3):
                        try:
                            model = ARIMA(ts_data, order=(p,d,q))
                            fitted = model.fit()
                            if fitted.aic < best_aic:
                                best_aic = fitted.aic
                                best_model = fitted
                        except:
                            continue
            
            if best_model:
                # äºˆæ¸¬å®Ÿè¡Œ
                forecast = best_model.forecast(steps=forecast_periods)
                forecast_ci = best_model.get_forecast(steps=forecast_periods).conf_int()
                
                print(f"âœ… ARIMAäºˆæ¸¬å®Œäº† (AIC: {best_aic:.2f})")
                print("äºˆæ¸¬å€¤ (æ¬¡ã®3æœŸé–“):")
                for i in range(min(3, len(forecast))):
                    print(f"  æœŸé–“{i+1}: {forecast.iloc[i]:,.0f}")
                
                self.results['forecast'] = {
                    'values': forecast,
                    'confidence_interval': forecast_ci,
                    'model': best_model
                }
                
                return forecast, forecast_ci
            else:
                print("âŒ ARIMA ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return None, None
                
        except Exception as e:
            print(f"âŒ ARIMAäºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
            return None, None
    
    def generate_tableau_script(self, analysis_type='regression'):
        """
        Tableauç”¨Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ç”Ÿæˆ
        """
        if analysis_type == 'regression':
            script = '''
SCRIPT_REAL("
import numpy as np
from sklearn.linear_model import LinearRegression

# ç‰¹å¾´é‡ã®å—ã‘å–ã‚Š
X1 = _arg1  # ç¬¬1ç‰¹å¾´é‡
X2 = _arg2  # ç¬¬2ç‰¹å¾´é‡  
X3 = _arg3  # ç¬¬3ç‰¹å¾´é‡

# ãƒ‡ãƒ¼ã‚¿ã®çµåˆ
X = np.column_stack([X1, X2, X3])

# äº‹å‰å­¦ç¿’æ¸ˆã¿ä¿‚æ•°ï¼ˆå®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼‰
coefficients = [ä¿‚æ•°1, ä¿‚æ•°2, ä¿‚æ•°3]
intercept = åˆ‡ç‰‡å€¤

# äºˆæ¸¬è¨ˆç®—
predictions = intercept + np.sum(X * coefficients, axis=1)

return predictions
", [ç‰¹å¾´é‡1], [ç‰¹å¾´é‡2], [ç‰¹å¾´é‡3])
'''
        
        elif analysis_type == 'anomaly':
            script = '''
SCRIPT_BOOL("
import numpy as np
from scipy import stats

# ãƒ‡ãƒ¼ã‚¿ã®å—ã‘å–ã‚Š
values = _arg1

# Z-score ã«ã‚ˆã‚‹ç•°å¸¸å€¤æ¤œå‡º
z_scores = np.abs(stats.zscore(values))
threshold = 3

# ç•°å¸¸å€¤ãƒ•ãƒ©ã‚°
is_anomaly = z_scores > threshold

return is_anomaly
", SUM([å£²ä¸Šé‡‘é¡]))
'''
        
        else:
            script = "# æŒ‡å®šã•ã‚ŒãŸåˆ†æã‚¿ã‚¤ãƒ—ãŒä¸æ­£ã§ã™"
        
        print("=== Tableauç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆ ===")
        print(script)
        return script

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # ãƒ„ãƒ¼ãƒ«ã‚­ãƒƒãƒˆã®åˆæœŸåŒ–
    toolkit = SalesAnalysisToolkit()
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = toolkit.load_data('regression_timeseries_data.xlsx', 'daily_data')
    
    if df is not None:
        # ã‚¯ã‚¤ãƒƒã‚¯EDA
        toolkit.quick_eda(df, 'å£²ä¸Šé‡‘é¡')
        
        # å›å¸°åˆ†æ
        regression_results = toolkit.regression_analysis(df, 'å£²ä¸Šé‡‘é¡')
        
        # æ™‚ç³»åˆ—åˆ†æï¼ˆæœˆæ¬¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆï¼‰
        try:
            monthly_df = toolkit.load_data('regression_timeseries_data.xlsx', 'monthly_data')
            if monthly_df is not None:
                forecast, ci = toolkit.time_series_analysis(monthly_df, 'å¹´æœˆ', 'å£²ä¸Šé‡‘é¡')
        except:
            print("æœˆæ¬¡ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        # Tableauã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆ
        tableau_script = toolkit.generate_tableau_script('regression')
```

### ğŸ“Š Tableauãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

**KPIè¡¨ç¤ºç”¨è¨ˆç®—ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰é›†**

```sql
-- å£²ä¸Šæˆé•·ç‡ï¼ˆå‰å¹´åŒæœˆæ¯”ï¼‰
Growth_Rate_YoY:
(SUM([å£²ä¸Šé‡‘é¡]) - LOOKUP(SUM([å£²ä¸Šé‡‘é¡]), -12)) / LOOKUP(SUM([å£²ä¸Šé‡‘é¡]), -12)

-- ç§»å‹•å¹³å‡ï¼ˆ3ãƒ¶æœˆï¼‰
Moving_Average_3M:
WINDOW_AVG(SUM([å£²ä¸Šé‡‘é¡]), -2, 0)

-- å­£ç¯€èª¿æ•´å£²ä¸Š
Seasonally_Adjusted_Sales:
SUM([å£²ä¸Šé‡‘é¡]) / [å­£ç¯€æ€§æˆåˆ†]

-- äºˆæ¸¬ç²¾åº¦ï¼ˆMAPEï¼‰
MAPE:
WINDOW_AVG(ABS([å£²ä¸Šé‡‘é¡] - [äºˆæ¸¬å£²ä¸Š]) / [å£²ä¸Šé‡‘é¡], -11, 0)

-- ãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘
Trend_Direction:
IF [Moving_Average_3M] > LOOKUP([Moving_Average_3M], -1) THEN "ä¸Šæ˜‡"
ELSEIF [Moving_Average_3M] < LOOKUP([Moving_Average_3M], -1) THEN "ä¸‹é™"
ELSE "æ¨ªã°ã„"
END

-- ç•°å¸¸å€¤ã‚¢ãƒ©ãƒ¼ãƒˆ
Anomaly_Alert:
IF [ç•°å¸¸å€¤ã‚¹ã‚³ã‚¢] > 3 THEN "è¦æ³¨æ„"
ELSEIF [ç•°å¸¸å€¤ã‚¹ã‚³ã‚¢] > 2 THEN "ç›£è¦–"
ELSE "æ­£å¸¸"
END
```

### ğŸš¨ ã‚¢ãƒ©ãƒ¼ãƒˆæ©Ÿèƒ½ã®å®Ÿè£…

```python
def setup_alert_system():
    """
    ç•°å¸¸å€¤ãƒ»äºˆæ¸¬ç²¾åº¦ã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ 
    """
    alert_config = {
        'anomaly_threshold': 3.0,  # Z-scoreé–¾å€¤
        'accuracy_threshold': 0.15,  # MAPEé–¾å€¤
        'alert_recipients': ['manager@company.com', 'analyst@company.com'],
        'alert_frequency': 'daily',
        'slack_webhook': 'https://hooks.slack.com/services/...'
    }
    
    alert_script = f'''
    import requests
    import json
    
    def send_alert(message, channel="#sales-analytics"):
        webhook_url = "{alert_config['slack_webhook']}"
        payload = {{
            "channel": channel,
            "username": "Tableau Analytics Bot",
            "text": message,
            "icon_emoji": ":warning:"
        }}
        
        response = requests.post(webhook_url, json=payload)
        return response.status_code == 200
    
    # Tableauã‹ã‚‰ã®ç•°å¸¸å€¤æ¤œå‡ºæ™‚
    if anomaly_score > {alert_config['anomaly_threshold']}:
        message = f"ğŸš¨ å£²ä¸Šç•°å¸¸å€¤æ¤œå‡º: {{date}}ã®å£²ä¸ŠãŒ{{value:,}}å†† (é€šå¸¸ã®{{deviation:.1f}}Ïƒ)"
        send_alert(message)
    
    # äºˆæ¸¬ç²¾åº¦ä½ä¸‹æ™‚
    if prediction_accuracy < {alert_config['accuracy_threshold']}:
        message = f"ğŸ“‰ äºˆæ¸¬ç²¾åº¦ä½ä¸‹: MAPE {{accuracy:.1%}} (ç›®æ¨™: {{threshold:.1%}}ä»¥ä¸‹)"
        send_alert(message)
    '''
    
    return alert_script

# ã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
alert_system = setup_alert_system()
print("ã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ ãŒè¨­å®šã•ã‚Œã¾ã—ãŸ")
```

---

## 10. æœ€çµ‚ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### âœ… æŠ€è¡“ç¿’å¾—ç¢ºèª

**åŸºç¤ãƒ¬ãƒ™ãƒ«**
- [ ] Excelãƒ‡ãƒ¼ã‚¿ã®é©åˆ‡ãªæº–å‚™ãŒã§ãã‚‹
- [ ] Tableauã§åŸºæœ¬çš„ãªæ•£å¸ƒå›³ãƒ»æ™‚ç³»åˆ—ã‚°ãƒ©ãƒ•ãŒä½œæˆã§ãã‚‹
- [ ] Pythonã§CSV/Excelãƒ•ã‚¡ã‚¤ãƒ«ãŒèª­ã¿è¾¼ã‚ã‚‹
- [ ] å˜å›å¸°åˆ†æã®çµæœã‚’è§£é‡ˆã§ãã‚‹
- [ ] TabPyã§ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒå®Ÿè¡Œã§ãã‚‹

**ä¸­ç´šãƒ¬ãƒ™ãƒ«**
- [ ] é‡å›å¸°åˆ†æã§è¤‡æ•°è¦å› ã®å½±éŸ¿ã‚’åˆ†æã§ãã‚‹
- [ ] æ™‚ç³»åˆ—åˆ†è§£ã§ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»å­£ç¯€æ€§ã‚’ç†è§£ã§ãã‚‹
- [ ] ARIMAãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬ãŒã§ãã‚‹
- [ ] ç•°å¸¸å€¤æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã§ãã‚‹
- [ ] Tableauã§é«˜åº¦ãªè¨ˆç®—ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒä½œæˆã§ãã‚‹

**ä¸Šç´šãƒ¬ãƒ™ãƒ«**
- [ ] è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ãŒã§ãã‚‹
- [ ] è‡ªå‹•åŒ–ã•ã‚ŒãŸãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ›´æ–°ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã§ãã‚‹
- [ ] ãƒ“ã‚¸ãƒã‚¹è¦ä»¶ã«å¿œã˜ãŸã‚«ã‚¹ã‚¿ãƒ åˆ†æãŒã§ãã‚‹
- [ ] ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼ã¸ã®åŠ¹æœçš„ãªçµæœå ±å‘ŠãŒã§ãã‚‹
- [ ] ç¶™ç¶šçš„ãªãƒ¢ãƒ‡ãƒ«æ”¹å–„ãƒ—ãƒ­ã‚»ã‚¹ã‚’é‹ç”¨ã§ãã‚‹

### ğŸ¯ ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤ç¢ºèª

**å®šé‡çš„æˆæœ**
- [ ] äºˆæ¸¬ç²¾åº¦ã®å…·ä½“çš„æ”¹å–„å€¤ã‚’æ¸¬å®šæ¸ˆã¿
- [ ] æ„æ€æ±ºå®šã®é«˜é€ŸåŒ–ã‚’æ™‚é–“ã§å®šé‡åŒ–æ¸ˆã¿
- [ ] ã‚³ã‚¹ãƒˆå‰Šæ¸›ãƒ»å£²ä¸Šå‘ä¸Šã®é‡‘é¡ã‚’ç®—å‡ºæ¸ˆã¿
- [ ] è‡ªå‹•åŒ–ã«ã‚ˆã‚‹å·¥æ•°å‰Šæ¸›ã‚’ç¢ºèªæ¸ˆã¿

**å®šæ€§çš„æˆæœ**
- [ ] ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ–ãƒ³ãªè­°è«–ãŒå¢—ãˆãŸ
- [ ] å‹˜ã«é ¼ã‚‰ãªã„æ„æ€æ±ºå®šãŒã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸ
- [ ] å°†æ¥ã¸ã®æº–å‚™ãŒæ—©ãã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸ
- [ ] ãƒãƒ¼ãƒ å…¨ä½“ã®åˆ†æã‚¹ã‚­ãƒ«ãŒå‘ä¸Šã—ãŸ

### ğŸ“ˆ ç¶™ç¶šæ”¹å–„è¨ˆç”»

**çŸ­æœŸç›®æ¨™ï¼ˆ1-3ãƒ¶æœˆï¼‰**
- [ ] äºˆæ¸¬ç²¾åº¦ã‚’ã•ã‚‰ã«5%å‘ä¸Š
- [ ] æ–°ã—ã„å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®çµ±åˆ
- [ ] ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®ãƒ¦ãƒ¼ã‚¶ãƒ“ãƒªãƒ†ã‚£æ”¹å–„
- [ ] ã‚¢ãƒ©ãƒ¼ãƒˆæ©Ÿèƒ½ã®ç²¾åº¦å‘ä¸Š

**ä¸­æœŸç›®æ¨™ï¼ˆ3-12ãƒ¶æœˆï¼‰**
- [ ] ä»–éƒ¨ç½²ãƒ»ä»–åœ°åŸŸã¸ã®æ¨ªå±•é–‹
- [ ] ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰
- [ ] æ©Ÿæ¢°å­¦ç¿’ã®é«˜åº¦åŒ–ï¼ˆæ·±å±¤å­¦ç¿’ç­‰ï¼‰
- [ ] å› æœæ¨è«–ã«ã‚ˆã‚‹æ–½ç­–åŠ¹æœæ¸¬å®š

**é•·æœŸç›®æ¨™ï¼ˆ1å¹´ä»¥ä¸Šï¼‰**
- [ ] çµ„ç¹”å…¨ä½“ã®ãƒ‡ãƒ¼ã‚¿åˆ†ææ–‡åŒ–é†¸æˆ
- [ ] æ–°è¦ãƒ“ã‚¸ãƒã‚¹å‰µå‡ºã¸ã®åˆ†ææ´»ç”¨
- [ ] æ¥­ç•Œãƒˆãƒƒãƒ—ã‚¯ãƒ©ã‚¹ã®äºˆæ¸¬ç²¾åº¦é”æˆ
- [ ] åˆ†æçµæœã®å¤–éƒ¨ä¾¡å€¤åŒ–ï¼ˆã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°ç­‰ï¼‰

---

## ãŠã‚ã‚Šã«

### ğŸŒŸ ã‚ãªãŸãŒæ‰‹ã«å…¥ã‚ŒãŸã‚‚ã®

ã“ã®è¨˜äº‹ã‚’é€šã˜ã¦ã€ã‚ãªãŸã¯å˜ãªã‚‹ã€Œã‚°ãƒ©ãƒ•ã‚’ä½œã‚‹äººã€ã‹ã‚‰ã€Œãƒ“ã‚¸ãƒã‚¹ã®æœªæ¥ã‚’äºˆæ¸¬ã™ã‚‹äººã€ã«å¤‰ã‚ã‚Šã¾ã—ãŸã€‚

**æŠ€è¡“çš„æˆé•·**
- Tableau Ã— Excel Ã— Python ã®å®Œå…¨é€£æºæŠ€è¡“
- çµ±è¨ˆå­¦ã¨ãƒ“ã‚¸ãƒã‚¹ã‚’çµã¶å®Ÿè·µçš„åˆ†æåŠ›
- æ•°åƒé€šã‚Šã®çµ„ã¿åˆã‚ã›ã‚’ä½¿ã„ã“ãªã™å¿œç”¨åŠ›

**ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤å‰µå‡ºåŠ›**
- ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰éš ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç™ºè¦‹ã™ã‚‹æ´å¯ŸåŠ›
- è¤‡é›‘ãªç¾è±¡ã‚’ã‚·ãƒ³ãƒ—ãƒ«ã«èª¬æ˜ã™ã‚‹è¡¨ç¾åŠ›
- ä¸ç¢ºå®Ÿãªæœªæ¥ã«å‚™ãˆã‚‹æˆ¦ç•¥ç«‹æ¡ˆåŠ›

**ç¶™ç¶šçš„å­¦ç¿’åŸºç›¤**
- æ–°ã—ã„æ‰‹æ³•ã‚’ç´ æ—©ãç¿’å¾—ã™ã‚‹å­¦ç¿’åŠ›
- å•é¡Œã«ç›´é¢ã—ãŸæ™‚ã®è§£æ±ºã‚¢ãƒ—ãƒ­ãƒ¼ãƒåŠ›
- å°‚é–€å®¶ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¨ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åŠ›

### ğŸ’ª ã“ã‚Œã‹ã‚‰ã®ã‚ãªãŸã¸

**è‡ªä¿¡ã‚’æŒã£ã¦ãã ã•ã„**
- çµ±è¨ˆå­¦ã®åŸºç¤ã‹ã‚‰é«˜åº¦ãªäºˆæ¸¬ã¾ã§ã€å¹…åºƒã„ã‚¹ã‚­ãƒ«ã‚’èº«ã«ã¤ã‘ã¾ã—ãŸ
- å®Ÿéš›ã®ãƒ“ã‚¸ãƒã‚¹èª²é¡Œã‚’è§£æ±ºã§ãã‚‹å®ŸåŠ›ãŒã‚ã‚Šã¾ã™
- çµ„ç¹”ã®ä¸­ã§ã€Œãƒ‡ãƒ¼ã‚¿åˆ†æã®å°‚é–€å®¶ã€ã¨ã—ã¦è²¢çŒ®ã§ãã¾ã™

**æŒ‘æˆ¦ã‚’ç¶šã‘ã¦ãã ã•ã„**
- æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã€æ–°ã—ã„èª²é¡Œã«ç©æ¥µçš„ã«å–ã‚Šçµ„ã‚€
- å¤±æ•—ã‚’æã‚Œãšã€è©¦è¡ŒéŒ¯èª¤ã‹ã‚‰å­¦ã³ç¶šã‘ã‚‹
- ä»–ã®åˆ†æè€…ã¨ã®äº¤æµã§çŸ¥è­˜ã‚’æ·±ã‚ã‚‹

**ä¾¡å€¤ã‚’å‰µé€ ã—ã¦ãã ã•ã„**
- ã‚ãªãŸã®åˆ†æãŒèª°ã‹ã®æ„æ€æ±ºå®šã‚’æ”¹å–„ã™ã‚‹
- æ•°å­—ã®å‘ã“ã†ã«ã‚ã‚‹äººã€…ã®ç”Ÿæ´»ã‚’ã‚ˆã‚Šè‰¯ãã™ã‚‹
- ãƒ‡ãƒ¼ã‚¿ã®åŠ›ã§ç¤¾ä¼šã«è²¢çŒ®ã™ã‚‹

### ğŸš€ æœ€å¾Œã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

ãƒ‡ãƒ¼ã‚¿åˆ†æã¯æŠ€è¡“ã§ã™ãŒã€ãã®å…ˆã«ã‚ã‚‹ã®ã¯**äººã€…ã®å¹¸ã›**ã§ã™ã€‚

ã‚ãªãŸãŒä½œã£ãŸäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ãŒåœ¨åº«ã®æœ€é©åŒ–ã‚’å®Ÿç¾ã—ã€ç„¡é§„ãªå»ƒæ£„ã‚’æ¸›ã‚‰ã™ã€‚ã‚ãªãŸãŒç™ºè¦‹ã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ãŒæ–°ã—ã„ã‚µãƒ¼ãƒ“ã‚¹ã‚’ç”Ÿã¿å‡ºã—ã€é¡§å®¢ã®æº€è¶³åº¦ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚ã‚ãªãŸãŒæ§‹ç¯‰ã—ãŸç•°å¸¸æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ ãŒå•é¡Œã‚’æ—©æœŸç™ºè¦‹ã—ã€å¤§ããªãƒˆãƒ©ãƒ–ãƒ«ã‚’æœªç„¶ã«é˜²ãã€‚

æŠ€è¡“ã¯äººã®ãŸã‚ã«ã€‚åˆ†æã¯ä¾¡å€¤å‰µé€ ã®ãŸã‚ã«ã€‚

ãã‚“ãªæƒ³ã„ã‚’èƒ¸ã«ã€ã“ã‚Œã‹ã‚‰ã‚‚ãƒ‡ãƒ¼ã‚¿åˆ†æã®ä¸–ç•Œã§æ´»èºã—ã¦ãã ã•ã„ã€‚

**ã‚ãªãŸã®åˆ†æãŒã€ã‚ˆã‚Šè‰¯ã„æœªæ¥ã‚’å‰µã‚‹ã€‚**

é ‘å¼µã£ã¦ï¼ ğŸ“Šâœ¨

---

### ğŸ“ ã‚µãƒãƒ¼ãƒˆãƒ»ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£

**è³ªå•ãƒ»ç›¸è«‡ã¯ã“ã¡ã‚‰**
- Tableau Community Forum
- Pythonå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ  
- Stack Overflow
- Reddit r/analytics

**ç¶™ç¶šå­¦ç¿’ãƒªã‚½ãƒ¼ã‚¹**
- Kaggle Learn (ç„¡æ–™ã‚ªãƒ³ãƒ©ã‚¤ãƒ³è¬›åº§)
- Coursera Data Science Specialization
- Tableau Public (ä½œå“å…±æœ‰ãƒ»å­¦ç¿’)
- GitHub (ã‚³ãƒ¼ãƒ‰å…±æœ‰ãƒ»ãƒ¬ãƒ“ãƒ¥ãƒ¼)

**æ—¥æœ¬èªã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£**
- Japan Tableau User Group
- Python Japan User Group
- ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆå”ä¼š
- çµ±è¨ˆæ¤œå®š (ã‚¹ã‚­ãƒ«èªå®š)

**ç·Šæ€¥ã‚µãƒãƒ¼ãƒˆ**
ã‚¨ãƒ©ãƒ¼ã§å›°ã£ãŸæ™‚ã¯ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãã®ã¾ã¾Googleæ¤œç´¢ã™ã‚Œã°ã€å¤§æŠµã®å ´åˆã¯è§£æ±ºç­–ãŒè¦‹ã¤ã‹ã‚Šã¾ã™ã€‚ãã‚Œã§ã‚‚è§£æ±ºã—ãªã„å ´åˆã¯ã€ä¸Šè¨˜ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã§è³ªå•ã—ã¦ã¿ã¦ãã ã•ã„ã€‚å¿…ãšè¦ªåˆ‡ãªæ–¹ãŒåŠ©ã‘ã¦ãã‚Œã¾ã™ã€‚

**æˆæœã®å…±æœ‰ã‚’å¿˜ã‚Œãšã«ï¼**
ä½œæˆã—ãŸãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚„åˆ†æçµæœã¯ã€ãœã²Tableau Publicã§å…¬é–‹ã—ã¦ãã ã•ã„ã€‚ã‚ãªãŸã®æˆé•·ã‚’å¤šãã®äººãŒå¿œæ´ã—ã¦ãã‚Œã‚‹ã¯ãšã§ã™ã€‚

ã¿ãªã•ã‚“ã®æˆåŠŸã‚’å¿ƒã‹ã‚‰å¿œæ´ã—ã¦ã„ã¾ã™ï¼ ğŸ‰
