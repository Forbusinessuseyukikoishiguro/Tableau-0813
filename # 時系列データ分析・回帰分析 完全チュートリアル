# ================================================
# æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ»å›å¸°åˆ†æ å®Œå…¨ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«
# Googleã‚³ãƒ©ãƒœå¯¾å¿œç‰ˆ - ã‚³ãƒ”ãƒšã§å³å®Ÿè¡Œå¯èƒ½
# ================================================

# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨è¨­å®š
import warnings
warnings.filterwarnings('ignore')

try:
    import japanize_matplotlib
    print("âœ… æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆåˆ©ç”¨å¯èƒ½")
except ImportError:
    import subprocess
    import sys
    subprocess.check_call([sys.executable, "-m", "pip", "install", "--quiet", "japanize-matplotlib"])
    import japanize_matplotlib
    print("âœ… æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã—ãŸ")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from scipy import stats
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# è¨­å®š
plt.style.use('seaborn-v0_8')
plt.rcParams['figure.figsize'] = (15, 8)
sns.set_palette("husl")

print("ğŸš€ æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ»å›å¸°åˆ†æãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«é–‹å§‹")
print("=" * 60)

# ================================================
# 1. æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆï¼ˆå®Ÿéš›ã®ãƒ“ã‚¸ãƒã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¨¡æ“¬ï¼‰
# ================================================

print("ğŸ“Š STEP 1: æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ")

# è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
np.random.seed(42)
n_periods = 365 * 3  # 3å¹´åˆ†ã®ãƒ‡ãƒ¼ã‚¿
start_date = '2021-01-01'

# æ—¥ä»˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
dates = pd.date_range(start=start_date, periods=n_periods, freq='D')
df = pd.DataFrame({'date': dates})

# åŸºæœ¬ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆé•·æœŸçš„ãªæˆé•·ï¼‰
base_trend = 1000 + np.arange(n_periods) * 0.5  # å¹´é–“ç´„180ã®æˆé•·

# å­£ç¯€æ€§ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå¹´æ¬¡ãƒ»é€±æ¬¡ï¼‰
yearly_seasonality = 200 * np.sin(2 * np.pi * np.arange(n_periods) / 365.25)
weekly_seasonality = 50 * np.sin(2 * np.pi * np.arange(n_periods) / 7)

# æœˆæ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆãƒ“ã‚¸ãƒã‚¹ç‰¹æœ‰ï¼‰
monthly_pattern = np.array([0.9, 0.8, 1.2, 1.0, 0.9, 1.1, 
                           0.8, 0.7, 1.1, 1.2, 1.0, 1.3] * (n_periods // 12 + 1))[:n_periods]
monthly_effect = 100 * (monthly_pattern - 1)

# å¤–çš„è¦å› ï¼ˆçµŒæ¸ˆæŒ‡æ¨™ã®å½±éŸ¿ï¼‰
economic_factor = np.cumsum(np.random.normal(0, 10, n_periods))

# ãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¤ã‚º
noise = np.random.normal(0, 50, n_periods)

# æœ€çµ‚çš„ãªç›®çš„å¤‰æ•°ï¼ˆå£²ä¸Šãªã©ï¼‰
df['target'] = (base_trend + yearly_seasonality + weekly_seasonality + 
                monthly_effect + economic_factor * 0.3 + noise)

# å¤–éƒ¨å¤‰æ•°ï¼ˆèª¬æ˜å¤‰æ•°ï¼‰ã®ç”Ÿæˆ
df['marketing_spend'] = 50 + np.random.exponential(30, n_periods)  # ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°æ”¯å‡º
df['competitor_activity'] = np.random.uniform(0.5, 1.5, n_periods)  # ç«¶åˆæ´»å‹•
df['weather_index'] = np.random.normal(0, 1, n_periods)  # å¤©å€™æŒ‡æ•°
df['economic_indicator'] = economic_factor  # çµŒæ¸ˆæŒ‡æ¨™

print(f"âœ… ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†")
print(f"   æœŸé–“: {df['date'].min().date()} ï½ {df['date'].max().date()}")
print(f"   ãƒ‡ãƒ¼ã‚¿æ•°: {len(df):,} æ—¥åˆ†")
print(f"   ç›®çš„å¤‰æ•°ç¯„å›²: {df['target'].min():.0f} ï½ {df['target'].max():.0f}")

# ================================================
# 2. æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆEDAï¼‰
# ================================================

print("\nğŸ“ˆ STEP 2: æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æ")

# åŸºæœ¬çµ±è¨ˆé‡
print("\nğŸ“Š åŸºæœ¬çµ±è¨ˆé‡:")
print(df[['target', 'marketing_spend', 'competitor_activity', 'economic_indicator']].describe().round(2))

# æ™‚ç³»åˆ—ãƒ—ãƒ­ãƒƒãƒˆ
fig, axes = plt.subplots(2, 2, figsize=(18, 12))
fig.suptitle('æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®æ¢ç´¢çš„åˆ†æ', fontsize=16, fontweight='bold')

# 1. ç›®çš„å¤‰æ•°ã®æ™‚ç³»åˆ—
axes[0, 0].plot(df['date'], df['target'], alpha=0.7, linewidth=1)
axes[0, 0].set_title('ç›®çš„å¤‰æ•°ã®æ™‚ç³»åˆ—æ¨ç§»')
axes[0, 0].set_ylabel('ç›®çš„å¤‰æ•°å€¤')
axes[0, 0].grid(True, alpha=0.3)

# 2. å­£ç¯€æ€§åˆ†æï¼ˆæœˆåˆ¥å¹³å‡ï¼‰
monthly_avg = df.groupby(df['date'].dt.month)['target'].mean()
months = ['1æœˆ', '2æœˆ', '3æœˆ', '4æœˆ', '5æœˆ', '6æœˆ', '7æœˆ', '8æœˆ', '9æœˆ', '10æœˆ', '11æœˆ', '12æœˆ']
axes[0, 1].bar(range(1, 13), monthly_avg.values, alpha=0.8)
axes[0, 1].set_title('æœˆåˆ¥å­£ç¯€æ€§ãƒ‘ã‚¿ãƒ¼ãƒ³')
axes[0, 1].set_ylabel('å¹³å‡å€¤')
axes[0, 1].set_xticks(range(1, 13))
axes[0, 1].set_xticklabels(months, rotation=45)
axes[0, 1].grid(True, alpha=0.3)

# 3. é€±æ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³
weekly_avg = df.groupby(df['date'].dt.dayofweek)['target'].mean()
weekdays = ['æœˆ', 'ç«', 'æ°´', 'æœ¨', 'é‡‘', 'åœŸ', 'æ—¥']
axes[1, 0].bar(range(7), weekly_avg.values, alpha=0.8, color='orange')
axes[1, 0].set_title('æ›œæ—¥åˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³')
axes[1, 0].set_ylabel('å¹³å‡å€¤')
axes[1, 0].set_xticks(range(7))
axes[1, 0].set_xticklabels(weekdays)
axes[1, 0].grid(True, alpha=0.3)

# 4. ç›¸é–¢åˆ†æ
corr_data = df[['target', 'marketing_spend', 'competitor_activity', 'economic_indicator']].corr()
sns.heatmap(corr_data, annot=True, cmap='coolwarm', center=0, ax=axes[1, 1])
axes[1, 1].set_title('å¤‰æ•°é–“ç›¸é–¢')

plt.tight_layout()
plt.show()

# ================================================
# 3. æ™‚ç³»åˆ—ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
# ================================================

print("\nğŸ”§ STEP 3: æ™‚ç³»åˆ—ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°")

def create_time_features(df):
    """æ™‚ç³»åˆ—ç‰¹å¾´é‡ã‚’ä½œæˆ"""
    df = df.copy()
    
    # åŸºæœ¬çš„ãªæ™‚é–“ç‰¹å¾´é‡
    df['year'] = df['date'].dt.year
    df['month'] = df['date'].dt.month
    df['day'] = df['date'].dt.day
    df['dayofweek'] = df['date'].dt.dayofweek
    df['dayofyear'] = df['date'].dt.dayofyear
    df['quarter'] = df['date'].dt.quarter
    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)
    
    # å‘¨æœŸæ€§ã®è¡¨ç¾ï¼ˆä¸‰è§’é–¢æ•°ï¼‰
    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
    df['day_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)
    df['day_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)
    
    return df

def create_lag_features(df, target_col, lags):
    """ãƒ©ã‚°ç‰¹å¾´é‡ã‚’ä½œæˆ"""
    df = df.copy()
    for lag in lags:
        df[f'{target_col}_lag_{lag}'] = df[target_col].shift(lag)
    return df

def create_rolling_features(df, target_col, windows):
    """ç§»å‹•çµ±è¨ˆé‡ã‚’ä½œæˆ"""
    df = df.copy()
    for window in windows:
        df[f'{target_col}_rolling_mean_{window}'] = df[target_col].rolling(window=window).mean()
        df[f'{target_col}_rolling_std_{window}'] = df[target_col].rolling(window=window).std()
        df[f'{target_col}_rolling_min_{window}'] = df[target_col].rolling(window=window).min()
        df[f'{target_col}_rolling_max_{window}'] = df[target_col].rolling(window=window).max()
    return df

def create_diff_features(df, target_col):
    """å·®åˆ†ç‰¹å¾´é‡ã‚’ä½œæˆ"""
    df = df.copy()
    df[f'{target_col}_diff_1'] = df[target_col].diff(1)  # 1æ—¥å‰ã¨ã®å·®åˆ†
    df[f'{target_col}_diff_7'] = df[target_col].diff(7)  # 1é€±é–“å‰ã¨ã®å·®åˆ†
    df[f'{target_col}_pct_change_1'] = df[target_col].pct_change(1)  # 1æ—¥å‰ã‹ã‚‰ã®å¤‰åŒ–ç‡
    df[f'{target_col}_pct_change_7'] = df[target_col].pct_change(7)  # 1é€±é–“å‰ã‹ã‚‰ã®å¤‰åŒ–ç‡
    return df

# ç‰¹å¾´é‡ç”Ÿæˆ
print("ç‰¹å¾´é‡ç”Ÿæˆä¸­...")
df = create_time_features(df)
df = create_lag_features(df, 'target', [1, 2, 3, 7, 14, 30])
df = create_rolling_features(df, 'target', [3, 7, 14, 30])
df = create_diff_features(df, 'target')

# å¤–éƒ¨å¤‰æ•°ã®ãƒ©ã‚°ç‰¹å¾´é‡ã‚‚ä½œæˆ
for col in ['marketing_spend', 'economic_indicator']:
    df = create_lag_features(df, col, [1, 7])

# æ¬ æå€¤ã‚’é™¤å»
df_clean = df.dropna().reset_index(drop=True)

print(f"âœ… ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†")
print(f"   å…ƒã®ç‰¹å¾´é‡æ•°: {len([col for col in df.columns if col != 'date'])}å€‹")
print(f"   ç”Ÿæˆå¾Œãƒ‡ãƒ¼ã‚¿æ•°: {len(df_clean):,}è¡Œ")

# ç‰¹å¾´é‡ä¸€è¦§è¡¨ç¤º
feature_cols = [col for col in df_clean.columns if col not in ['date', 'target']]
print(f"\nğŸ“‹ ç”Ÿæˆã•ã‚ŒãŸç‰¹å¾´é‡ï¼ˆ{len(feature_cols)}å€‹ï¼‰:")
for i, col in enumerate(feature_cols[:15], 1):  # æœ€åˆã®15å€‹ã‚’è¡¨ç¤º
    print(f"  {i:2d}. {col}")
if len(feature_cols) > 15:
    print(f"  ... ä»–{len(feature_cols)-15}å€‹")

# ================================================
# 4. è¤‡æ•°ã®å›å¸°ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…ã¨æ¯”è¼ƒ
# ================================================

print(f"\nğŸ¤– STEP 4: è¤‡æ•°å›å¸°ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒ")

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
X = df_clean[feature_cols]
y = df_clean['target']

# æ™‚ç³»åˆ—åˆ†å‰²ï¼ˆæœªæ¥ã®ãƒ‡ãƒ¼ã‚¿ã¯ä½¿ã‚ãªã„ï¼‰
def time_series_split(X, y, test_size=0.2):
    split_point = int(len(X) * (1 - test_size))
    X_train, X_test = X.iloc[:split_point], X.iloc[split_point:]
    y_train, y_test = y.iloc[:split_point], y.iloc[split_point:]
    return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = time_series_split(X, y)

print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(X_train)}è¡Œ, ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(X_test)}è¡Œ")

# ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ãƒ¢ãƒ‡ãƒ«å®šç¾©
models = {
    'ç·šå½¢å›å¸°': LinearRegression(),
    'Ridgeå›å¸°': Ridge(alpha=1.0),
    'Lassoå›å¸°': Lasso(alpha=1.0),
    'ElasticNet': ElasticNet(alpha=1.0, l1_ratio=0.5),
    'ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ': RandomForestRegressor(n_estimators=100, random_state=42),
    'ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°': GradientBoostingRegressor(n_estimators=100, random_state=42)
}

# å¤šé …å¼ç‰¹å¾´é‡ã®è¿½åŠ 
poly_features = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)
X_train_poly = poly_features.fit_transform(X_train_scaled)
X_test_poly = poly_features.transform(X_test_scaled)

models['å¤šé …å¼å›å¸°'] = LinearRegression()

# ãƒ¢ãƒ‡ãƒ«è©•ä¾¡çµæœã‚’æ ¼ç´
results = []

print("\nğŸ† ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãƒ»è©•ä¾¡å®Ÿè¡Œä¸­...")

for name, model in models.items():
    print(f"  {name} è¨“ç·´ä¸­...", end="")
    
    # å¤šé …å¼å›å¸°ã®å ´åˆã¯ç‰¹åˆ¥å‡¦ç†
    if name == 'å¤šé …å¼å›å¸°':
        model.fit(X_train_poly, y_train)
        y_pred = model.predict(X_test_poly)
    # ç·šå½¢ç³»ãƒ¢ãƒ‡ãƒ«ã¯ã‚¹ã‚±ãƒ¼ãƒ«æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨
    elif name in ['ç·šå½¢å›å¸°', 'Ridgeå›å¸°', 'Lassoå›å¸°', 'ElasticNet']:
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
    # ãƒ„ãƒªãƒ¼ç³»ãƒ¢ãƒ‡ãƒ«ã¯å…ƒãƒ‡ãƒ¼ã‚¿ä½¿ç”¨
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
    
    # è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)
    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
    
    results.append({
        'Model': name,
        'MAE': mae,
        'RMSE': rmse,
        'RÂ²': r2,
        'MAPE': mape,
        'Predictions': y_pred
    })
    
    print(f" å®Œäº† (RÂ²: {r2:.3f})")

# çµæœã‚’DataFrameã«å¤‰æ›
results_df = pd.DataFrame(results)
results_df = results_df.sort_values('RÂ²', ascending=False).reset_index(drop=True)

print(f"\nğŸ“Š ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒ:")
print("=" * 80)
print(f"{'é †ä½':<4} {'ãƒ¢ãƒ‡ãƒ«å':<20} {'MAE':<8} {'RMSE':<8} {'RÂ²':<6} {'MAPE':<7}")
print("=" * 80)
for i, row in results_df.iterrows():
    print(f"{i+1:<4} {row['Model']:<20} {row['MAE']:<8.1f} {row['RMSE']:<8.1f} {row['RÂ²']:<6.3f} {row['MAPE']:<7.1f}%")

# ================================================
# 5. äºˆæ¸¬çµæœã®å¯è¦–åŒ–
# ================================================

print(f"\nğŸ“ˆ STEP 5: äºˆæ¸¬çµæœå¯è¦–åŒ–")

# æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã®é¸æŠ
best_model_name = results_df.iloc[0]['Model']
best_predictions = results_df.iloc[0]['Predictions']

print(f"ğŸ† æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«: {best_model_name} (RÂ² = {results_df.iloc[0]['RÂ²']:.3f})")

# å¯è¦–åŒ–
fig, axes = plt.subplots(3, 2, figsize=(20, 15))
fig.suptitle('æ™‚ç³»åˆ—å›å¸°åˆ†æçµæœ', fontsize=16, fontweight='bold')

# 1. æ™‚ç³»åˆ—äºˆæ¸¬çµæœ
test_dates = df_clean['date'].iloc[len(X_train):len(X_train)+len(X_test)]
axes[0, 0].plot(test_dates, y_test.values, label='å®Ÿæ¸¬å€¤', linewidth=2, alpha=0.8)
axes[0, 0].plot(test_dates, best_predictions, label=f'äºˆæ¸¬å€¤({best_model_name})', linewidth=2, alpha=0.8)
axes[0, 0].set_title('æ™‚ç³»åˆ—äºˆæ¸¬çµæœ')
axes[0, 0].set_ylabel('ç›®çš„å¤‰æ•°å€¤')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# 2. äºˆæ¸¬ç²¾åº¦æ•£å¸ƒå›³
axes[0, 1].scatter(y_test, best_predictions, alpha=0.6)
min_val, max_val = min(y_test.min(), best_predictions.min()), max(y_test.max(), best_predictions.max())
axes[0, 1].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)
axes[0, 1].set_xlabel('å®Ÿæ¸¬å€¤')
axes[0, 1].set_ylabel('äºˆæ¸¬å€¤')
axes[0, 1].set_title(f'äºˆæ¸¬ç²¾åº¦ (RÂ² = {results_df.iloc[0]["RÂ²"]:.3f})')
axes[0, 1].grid(True, alpha=0.3)

# 3. æ®‹å·®åˆ†æ
residuals = y_test.values - best_predictions
axes[1, 0].scatter(best_predictions, residuals, alpha=0.6)
axes[1, 0].axhline(y=0, color='r', linestyle='--')
axes[1, 0].set_xlabel('äºˆæ¸¬å€¤')
axes[1, 0].set_ylabel('æ®‹å·®')
axes[1, 0].set_title('æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ')
axes[1, 0].grid(True, alpha=0.3)

# 4. æ®‹å·®ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
axes[1, 1].hist(residuals, bins=50, alpha=0.7, edgecolor='black')
axes[1, 1].set_xlabel('æ®‹å·®')
axes[1, 1].set_ylabel('é »åº¦')
axes[1, 1].set_title('æ®‹å·®åˆ†å¸ƒ')
axes[1, 1].grid(True, alpha=0.3)

# 5. ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒï¼ˆRÂ²ï¼‰
model_names = results_df['Model'].values
r2_scores = results_df['RÂ²'].values
bars = axes[2, 0].bar(range(len(model_names)), r2_scores, alpha=0.8)
axes[2, 0].set_xlabel('ãƒ¢ãƒ‡ãƒ«')
axes[2, 0].set_ylabel('RÂ² ã‚¹ã‚³ã‚¢')
axes[2, 0].set_title('ãƒ¢ãƒ‡ãƒ«åˆ¥æ€§èƒ½æ¯”è¼ƒ (RÂ²)')
axes[2, 0].set_xticks(range(len(model_names)))
axes[2, 0].set_xticklabels(model_names, rotation=45, ha='right')
axes[2, 0].grid(True, alpha=0.3)

# æœ€é«˜æ€§èƒ½ã‚’å¼·èª¿
best_idx = np.argmax(r2_scores)
bars[best_idx].set_color('red')
bars[best_idx].set_alpha(1.0)

# 6. èª¤å·®æ¯”è¼ƒï¼ˆMAPEï¼‰
mape_scores = results_df['MAPE'].values
axes[2, 1].bar(range(len(model_names)), mape_scores, alpha=0.8, color='orange')
axes[2, 1].set_xlabel('ãƒ¢ãƒ‡ãƒ«')
axes[2, 1].set_ylabel('MAPE (%)')
axes[2, 1].set_title('ãƒ¢ãƒ‡ãƒ«åˆ¥èª¤å·®æ¯”è¼ƒ (MAPE)')
axes[2, 1].set_xticks(range(len(model_names)))
axes[2, 1].set_xticklabels(model_names, rotation=45, ha='right')
axes[2, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ================================================
# 6. ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ
# ================================================

print(f"\nğŸ” STEP 6: ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ")

# ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã§ç‰¹å¾´é‡é‡è¦åº¦ã‚’åˆ†æ
rf_model = RandomForestRegressor(n_estimators=200, random_state=42)
rf_model.fit(X_train, y_train)

# é‡è¦åº¦å–å¾—
feature_importance = pd.DataFrame({
    'Feature': feature_cols,
    'Importance': rf_model.feature_importances_
}).sort_values('Importance', ascending=False)

# ä¸Šä½15å€‹ã®ç‰¹å¾´é‡
top_features = feature_importance.head(15)

print(f"ğŸ† ç‰¹å¾´é‡é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆä¸Šä½15å€‹ï¼‰:")
print("=" * 50)
for i, (_, row) in enumerate(top_features.iterrows(), 1):
    print(f"{i:2d}. {row['Feature']:<25}: {row['Importance']:.4f}")

# é‡è¦åº¦å¯è¦–åŒ–
plt.figure(figsize=(12, 8))
plt.barh(range(len(top_features)), top_features['Importance'].values)
plt.yticks(range(len(top_features)), top_features['Feature'].values)
plt.xlabel('é‡è¦åº¦')
plt.title('ç‰¹å¾´é‡é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆä¸Šä½15å€‹ï¼‰')
plt.gca().invert_yaxis()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# ================================================
# 7. æ™‚ç³»åˆ—äº¤å·®æ¤œè¨¼
# ================================================

print(f"\nâ° STEP 7: æ™‚ç³»åˆ—äº¤å·®æ¤œè¨¼")

# æ™‚ç³»åˆ—åˆ†å‰²ã§äº¤å·®æ¤œè¨¼
tscv = TimeSeriesSplit(n_splits=5)

cv_results = {}
for name, model in models.items():
    if name == 'å¤šé …å¼å›å¸°':
        continue  # è¨ˆç®—æ™‚é–“ã®é–¢ä¿‚ã§ã‚¹ã‚­ãƒƒãƒ—
    
    if name in ['ç·šå½¢å›å¸°', 'Ridgeå›å¸°', 'Lassoå›å¸°', 'ElasticNet']:
        scores = cross_val_score(model, X_train_scaled, y_train, cv=tscv, scoring='r2')
    else:
        scores = cross_val_score(model, X_train, y_train, cv=tscv, scoring='r2')
    
    cv_results[name] = {
        'mean': scores.mean(),
        'std': scores.std(),
        'scores': scores
    }

print(f"ğŸ“Š æ™‚ç³»åˆ—äº¤å·®æ¤œè¨¼çµæœ (RÂ² ã‚¹ã‚³ã‚¢):")
print("=" * 60)
print(f"{'ãƒ¢ãƒ‡ãƒ«å':<20} {'å¹³å‡':<8} {'æ¨™æº–åå·®':<8} {'å®‰å®šæ€§'}")
print("=" * 60)

for name, result in sorted(cv_results.items(), key=lambda x: x[1]['mean'], reverse=True):
    stability = "é«˜" if result['std'] < 0.02 else "ä¸­" if result['std'] < 0.05 else "ä½"
    print(f"{name:<20} {result['mean']:<8.3f} {result['std']:<8.3f} {stability}")

# ================================================
# 8. å°†æ¥äºˆæ¸¬
# ================================================

print(f"\nğŸ”® STEP 8: å°†æ¥äºˆæ¸¬")

# æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã§å°†æ¥äºˆæ¸¬
best_model = None
for name, model in models.items():
    if name == best_model_name:
        best_model = model
        break

# å°†æ¥30æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
future_dates = pd.date_range(start=df_clean['date'].iloc[-1] + timedelta(days=1), periods=30, freq='D')
future_df = pd.DataFrame({'date': future_dates})

# å¤–éƒ¨å¤‰æ•°ã®å°†æ¥å€¤ã‚’æ¨å®šï¼ˆã“ã“ã§ã¯ç°¡å˜ã«æœ€è¿‘ã®å¹³å‡å€¤ã‚’ä½¿ç”¨ï¼‰
recent_data = df_clean.tail(30)
future_df['marketing_spend'] = recent_data['marketing_spend'].mean() + np.random.normal(0, 10, 30)
future_df['competitor_activity'] = recent_data['competitor_activity'].mean() + np.random.normal(0, 0.1, 30)
future_df['weather_index'] = np.random.normal(0, 1, 30)
future_df['economic_indicator'] = recent_data['economic_indicator'].iloc[-1] + np.cumsum(np.random.normal(0, 5, 30))

# ç‰¹å¾´é‡ç”Ÿæˆï¼ˆãŸã ã—ã€ãƒ©ã‚°ç‰¹å¾´é‡ã¯éå»ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å–å¾—ï¼‰
future_df = create_time_features(future_df)

# ç°¡ç•¥åŒ–ã—ãŸç‰¹å¾´é‡ã‚»ãƒƒãƒˆï¼ˆãƒ©ã‚°ç‰¹å¾´é‡ãªã—ï¼‰
basic_features = ['month', 'day', 'dayofweek', 'quarter', 'is_weekend',
                 'month_sin', 'month_cos', 'day_sin', 'day_cos',
                 'marketing_spend', 'competitor_activity', 'weather_index', 'economic_indicator']

# å°†æ¥äºˆæ¸¬å®Ÿè¡Œ
if best_model_name in ['ç·šå½¢å›å¸°', 'Ridgeå›å¸°', 'Lassoå›å¸°', 'ElasticNet']:
    future_X = scaler.transform(future_df[basic_features])
else:
    future_X = future_df[basic_features]

future_predictions = best_model.predict(future_X)

print(f"ğŸ¯ å°†æ¥30æ—¥é–“ã®äºˆæ¸¬ ({best_model_name}ä½¿ç”¨):")
print("=" * 40)
for i, (date, pred) in enumerate(zip(future_dates, future_predictions), 1):
    if i <= 7:  # æœ€åˆã®7æ—¥é–“ã‚’è¡¨ç¤º
        print(f"{date.strftime('%Y-%m-%d (%a)')}: {pred:.1f}")
    elif i == 8:
        print("...")
    elif i > 23:  # æœ€å¾Œã®7æ—¥é–“ã‚’è¡¨ç¤º
        print(f"{date.strftime('%Y-%m-%d (%a)')}: {pred:.1f}")

# å°†æ¥äºˆæ¸¬ã®å¯è¦–åŒ–
plt.figure(figsize=(15, 6))

# éå»ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆæœ€å¾Œã®60æ—¥ï¼‰
recent_df = df_clean.tail(60)
plt.plot(recent_df['date'], recent_df['target'], label='éå»ã®å®Ÿç¸¾', linewidth=2, alpha=0.8)

# å°†æ¥äºˆæ¸¬
plt.plot(future_dates, future_predictions, label='å°†æ¥äºˆæ¸¬', linewidth=2, alpha=0.8, linestyle='--')

plt.axvline(x=df_clean['date'].iloc[-1], color='red', linestyle=':', alpha=0.7, label='äºˆæ¸¬é–‹å§‹ç‚¹')
plt.xlabel('æ—¥ä»˜')
plt.ylabel('ç›®çš„å¤‰æ•°å€¤')
plt.title('å°†æ¥30æ—¥é–“ã®äºˆæ¸¬çµæœ')
plt.legend()
plt.grid(True, alpha=0.3)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# ================================================
# 9. å®Ÿç”¨ã‚¬ã‚¤ãƒ‰ã¨ã¾ã¨ã‚
# ================================================

print(f"\nğŸ“š STEP 9: å®Ÿç”¨ã‚¬ã‚¤ãƒ‰")
print("=" * 60)

print(f"ğŸ¯ ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§å­¦ã‚“ã ã“ã¨:")
print(f"")
print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿æº–å‚™ãƒ»å‰å‡¦ç†:")
print(f"  âœ“ æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆã¨åŸºæœ¬çš„EDA")
print(f"  âœ“ å­£ç¯€æ€§ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»ãƒã‚¤ã‚ºã®åˆ†è§£")
print(f"  âœ“ ç›¸é–¢åˆ†æã¨å¤–ã‚Œå€¤æ¤œå‡º")
print(f"")
print(f"ğŸ”§ ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°:")
print(f"  âœ“ æ™‚é–“ç‰¹å¾´é‡ï¼ˆå¹´æœˆæ—¥ã€æ›œæ—¥ã€å­£ç¯€æ€§ï¼‰")
print(f"  âœ“ ãƒ©ã‚°ç‰¹å¾´é‡ï¼ˆéå»ã®å€¤ã®å½±éŸ¿ï¼‰") 
print(f"  âœ“ ç§»å‹•çµ±è¨ˆé‡ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰æ•æ‰ï¼‰")
print(f"  âœ“ å·®åˆ†ãƒ»å¤‰åŒ–ç‡ï¼ˆå¤‰å‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰")
print(f"  âœ“ å‘¨æœŸæ€§è¡¨ç¾ï¼ˆä¸‰è§’é–¢æ•°ï¼‰")
print(f"")
print(f"ğŸ¤– å›å¸°ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ:")
print(f"  âœ“ ç·šå½¢å›å¸°ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰")
print(f"  âœ“ æ­£å‰‡åŒ–å›å¸°ï¼ˆRidge, Lasso, ElasticNetï¼‰")
print(f"  âœ“ å¤šé …å¼å›å¸°ï¼ˆéç·šå½¢é–¢ä¿‚ï¼‰")
print(f"  âœ“ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ï¼ˆRF, GBMï¼‰")
print(f"")
print(f"ğŸ“ˆ è©•ä¾¡ãƒ»æ¤œè¨¼:")
print(f"  âœ“ æ™‚ç³»åˆ—åˆ†å‰²ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚±ãƒ¼ã‚¸å›é¿ï¼‰")
print(f"  âœ“ è¤‡æ•°è©•ä¾¡æŒ‡æ¨™ï¼ˆMAE, RMSE, RÂ², MAPEï¼‰")
print(f"  âœ“ æ™‚ç³»åˆ—äº¤å·®æ¤œè¨¼ï¼ˆå®‰å®šæ€§è©•ä¾¡ï¼‰")
print(f"  âœ“ æ®‹å·®åˆ†æï¼ˆãƒ¢ãƒ‡ãƒ«è¨ºæ–­ï¼‰")
print(f"")
print(f"ğŸ”® å°†æ¥äºˆæ¸¬:")
print(f"  âœ“ å¤–éƒ¨å¤‰æ•°ã®å°†æ¥å€¤æ¨å®š")
print(f"  âœ“ äºˆæ¸¬åŒºé–“ã®è€ƒæ…®")
print(f"  âœ“ äºˆæ¸¬çµæœã®å¯è¦–åŒ–")

print(f"\nğŸ’¡ å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã®æ´»ç”¨ãƒã‚¤ãƒ³ãƒˆ:")
print(f"")
print(f"ğŸ¨ ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºæ–¹æ³•:")
print(f"  1. ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆéƒ¨åˆ†ã‚’å®Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã«å¤‰æ›´")
print(f"     â†’ df = pd.read_csv('your_data.csv')")
print(f"  2. ç‰¹å¾´é‡ã‚’æ¥­ç•Œãƒ»ãƒ“ã‚¸ãƒã‚¹ç‰¹æœ‰ã®ã‚‚ã®ã«èª¿æ•´")
print(f"  3. å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ï¼ˆå¤©å€™ã€çµŒæ¸ˆæŒ‡æ¨™ç­‰ï¼‰ã®çµ±åˆ")
print(f"  4. ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã«åŸºã¥ãç‰¹å¾´é‡è¨­è¨ˆ")
print(f"")
print(f"ğŸ“Š ç¶™ç¶šçš„æ”¹å–„:")
print(f"  1. æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã§ã®å®šæœŸçš„ãªå†è¨“ç·´")
print(f"  2. äºˆæ¸¬ç²¾åº¦ã®ç›£è¦–ã¨ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š")  
print(f"  3. å¤–éƒ¨è¦å› ã®å¤‰åŒ–ã¸ã®å¯¾å¿œ")
print(f"  4. ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆæ€§å‘ä¸Š")
print(f"")
print(f"âš ï¸ æ³¨æ„äº‹é …:")
print(f"  â€¢ æ™‚ç³»åˆ—ã§ã¯æœªæ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¨“ç·´ã«ä½¿ã‚ãªã„")
print(f"  â€¢ å­£ç¯€æ€§ã®å¤‰åŒ–ã«æ³¨æ„ï¼ˆã‚³ãƒ­ãƒŠç¦ç­‰ï¼‰")
print(f"  â€¢ å¤–æŒ¿äºˆæ¸¬ã®ä¸ç¢ºå®Ÿæ€§ã‚’è€ƒæ…®")
print(f"  â€¢ ãƒ“ã‚¸ãƒã‚¹åˆ¶ç´„ã®åæ˜ ï¼ˆéè² åˆ¶ç´„ç­‰ï¼‰")

print(f"\nâœ… ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«å®Œäº†ï¼")
print(f"ğŸš€ ã“ã®ã‚³ãƒ¼ãƒ‰ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€è‡ªåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã§è©¦ã—ã¦ã¿ã¦ãã ã•ã„ï¼")
print("=" * 60)
