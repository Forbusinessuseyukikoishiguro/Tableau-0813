# ========================================
# ã‚¢ã‚¤ãƒªã‚¹åˆ†é¡ - Google Colab 1è¡Œãšã¤è©³ç´°è§£èª¬
# æ–°äººã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å‘ã‘ï¼šå„è¡Œã®æ„å‘³ã¨ç›®çš„ã‚’å®Œå…¨è§£èª¬
# ========================================

# === ã‚»ãƒ«1: ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆå¿…è¦ãªé“å…·ã‚’æº–å‚™ï¼‰ ===

import numpy as np               # æ•°å€¤è¨ˆç®—ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆé…åˆ—æ“ä½œã€æ•°å­¦è¨ˆç®—ã‚’é«˜é€Ÿå®Ÿè¡Œï¼‰
import pandas as pd              # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ æ“ä½œãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆExcelã®ã‚ˆã†ãªè¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿ã‚’æ‰±ã†ï¼‰
import matplotlib.pyplot as plt  # ã‚°ãƒ©ãƒ•æç”»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆæ£’ã‚°ãƒ©ãƒ•ã€æ•£å¸ƒå›³ã€ç·šã‚°ãƒ©ãƒ•ãªã©ã‚’ä½œæˆï¼‰
import seaborn as sns           # çµ±è¨ˆã‚°ãƒ©ãƒ•ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆmatplotlibã‚ˆã‚Šç¾ã—ãç°¡å˜ã«ã‚°ãƒ©ãƒ•ä½œæˆï¼‰

# scikit-learnï¼ˆæ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼‰ã‹ã‚‰å¿…è¦ãªæ©Ÿèƒ½ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from sklearn import datasets          # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆã‚¢ã‚¤ãƒªã‚¹ã€ãƒ¯ã‚¤ãƒ³ã€ãƒœã‚¹ãƒˆãƒ³ä½å®…ä¾¡æ ¼ãªã©ï¼‰
from sklearn.model_selection import train_test_split  # ãƒ‡ãƒ¼ã‚¿ã‚’è¨“ç·´ç”¨ã¨ãƒ†ã‚¹ãƒˆç”¨ã«åˆ†å‰²ã™ã‚‹æ©Ÿèƒ½
from sklearn.preprocessing import StandardScaler      # ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–ï¼ˆã‚¹ã‚±ãƒ¼ãƒ«ã‚’æƒãˆã‚‹ï¼‰æ©Ÿèƒ½
from sklearn.linear_model import LogisticRegression   # ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ï¼ˆç·šå½¢åˆ†é¡å™¨ï¼‰
from sklearn.tree import DecisionTreeClassifier       # æ±ºå®šæœ¨ï¼ˆãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹åˆ†é¡å™¨ï¼‰
from sklearn.ensemble import RandomForestClassifier   # ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆï¼ˆæ±ºå®šæœ¨ã®é›†åˆä½“ï¼‰
from sklearn.svm import SVC                           # ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ãƒã‚·ãƒ³ï¼ˆéç·šå½¢åˆ†é¡å™¨ï¼‰
from sklearn.neighbors import KNeighborsClassifier    # kè¿‘å‚æ³•ï¼ˆä¼¼ã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰äºˆæ¸¬ï¼‰
from sklearn.metrics import accuracy_score            # ç²¾åº¦è¨ˆç®—ï¼ˆæ­£è§£ç‡ã‚’ç®—å‡ºï¼‰
from sklearn.metrics import classification_report    # è©³ç´°ãªåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
from sklearn.metrics import confusion_matrix          # æ··åŒè¡Œåˆ—ï¼ˆäºˆæ¸¬ãƒŸã‚¹ã®è©³ç´°åˆ†æï¼‰

print("ğŸŒ¸ å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†ï¼")  # å‡¦ç†å®Œäº†ã®ç¢ºèªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

# === ã‚»ãƒ«2: ã‚¢ã‚¤ãƒªã‚¹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿ã¨åŸºæœ¬ç¢ºèª ===

# ã‚¢ã‚¤ãƒªã‚¹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ï¼ˆscikit-learnã«çµ„ã¿è¾¼ã¾ã‚Œã¦ã„ã‚‹æœ‰åãªã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼‰
iris = datasets.load_iris()
# load_iris()ï¼š150å€‹ã®ã‚¢ã‚¤ãƒªã‚¹èŠ±ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆ4ã¤ã®æ¸¬å®šå€¤ã¨3ã¤ã®å“ç¨®ãƒ©ãƒ™ãƒ«ï¼‰

print("ğŸŒ¸ ã‚¢ã‚¤ãƒªã‚¹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†æé–‹å§‹ï¼")  # åˆ†æé–‹å§‹ã®å®£è¨€

# ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬æƒ…å ±ã‚’ç¢ºèª
print(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {iris.data.shape}")  # ãƒ‡ãƒ¼ã‚¿ã®å½¢çŠ¶ï¼ˆè¡Œæ•°, åˆ—æ•°ï¼‰ã‚’è¡¨ç¤º
# iris.data.shapeï¼š(150, 4) â†’ 150ã‚µãƒ³ãƒ—ãƒ«ã€4ç‰¹å¾´é‡

print(f"ç‰¹å¾´é‡å: {iris.feature_names}")   # 4ã¤ã®ç‰¹å¾´é‡ã®åå‰ã‚’è¡¨ç¤º
# ['sepal length', 'sepal width', 'petal length', 'petal width']
# sepal=ãŒãç‰‡ã€petal=èŠ±ã³ã‚‰ã€length=é•·ã•ã€width=å¹…

print(f"ã‚¯ãƒ©ã‚¹å: {iris.target_names}")     # 3ã¤ã®å“ç¨®åã‚’è¡¨ç¤º
# ['setosa', 'versicolor', 'virginica']ï¼šã‚¢ã‚¤ãƒªã‚¹ã®3å“ç¨®

print(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®å½¢çŠ¶: {iris.target.shape}")  # æ­£è§£ãƒ©ãƒ™ãƒ«ã®å½¢çŠ¶
# (150,)ï¼š150å€‹ã®æ­£è§£ãƒ©ãƒ™ãƒ«ï¼ˆ0, 1, 2ã®æ•°å€¤ï¼‰

# å„ã‚¯ãƒ©ã‚¹ã®åˆ†å¸ƒã‚’ç¢ºèª
unique, counts = np.unique(iris.target, return_counts=True)
# np.unique()ï¼šãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå€¤ã¨ãã®å‡ºç¾å›æ•°ã‚’å–å¾—
# uniqueï¼š[0, 1, 2]ã€countsï¼šå„ã‚¯ãƒ©ã‚¹ã®å€‹æ•°

print("ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ:")
for i, (cls, count) in enumerate(zip(iris.target_names, counts)):
    # zip()ï¼š2ã¤ã®ãƒªã‚¹ãƒˆã‚’åŒæ™‚ã«å–å¾—
    print(f"  {cls}: {count}å€‹")  # å„å“ç¨®ã®ãƒ‡ãƒ¼ã‚¿æ•°ã‚’è¡¨ç¤º

# === ã‚»ãƒ«3: ãƒ‡ãƒ¼ã‚¿ã‚’Pandasã®DataFrameã«å¤‰æ›ï¼ˆè¦‹ã‚„ã™ãã™ã‚‹ãŸã‚ï¼‰ ===

# NumPyé…åˆ—ã‚’Pandasã®DataFrameã«å¤‰æ›ï¼ˆè¡¨å½¢å¼ã§è¦‹ã‚„ã™ãã™ã‚‹ï¼‰
df = pd.DataFrame(iris.data, columns=iris.feature_names)
# pd.DataFrame()ï¼šè¡¨å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’ä½œæˆ
# iris.dataï¼šç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ï¼ˆ150Ã—4ã®é…åˆ—ï¼‰
# columnsï¼šåˆ—åã‚’æŒ‡å®š

# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆå“ç¨®åï¼‰ã‚’æ–‡å­—åˆ—ã§è¿½åŠ 
df['species'] = iris.target_names[iris.target]
# iris.targetï¼š[0, 1, 2, 1, 0, ...]ã®æ•°å€¤é…åˆ—
# iris.target_names[iris.target]ï¼šæ•°å€¤ã‚’å“ç¨®åã«å¤‰æ›

print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã®æœ€åˆã®5è¡Œ:")
print(df.head())  # head()ï¼šãƒ‡ãƒ¼ã‚¿ã®æœ€åˆã®5è¡Œã‚’è¡¨ç¤º
# ãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ ã¨å†…å®¹ã‚’ç¢ºèªã™ã‚‹ãŸã‚

print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ã®æœ€å¾Œã®5è¡Œ:")
print(df.tail())  # tail()ï¼šãƒ‡ãƒ¼ã‚¿ã®æœ€å¾Œã®5è¡Œã‚’è¡¨ç¤º

print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬çµ±è¨ˆé‡:")
print(df.describe())  # describe()ï¼šå¹³å‡ã€æ¨™æº–åå·®ã€æœ€å°å€¤ã€æœ€å¤§å€¤ãªã©ã‚’è¨ˆç®—
# æ•°å€¤åˆ—ã®ã¿ãŒå¯¾è±¡ï¼ˆspeciesåˆ—ã¯é™¤å¤–ã•ã‚Œã‚‹ï¼‰

print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿å‹ã®ç¢ºèª:")
print(df.dtypes)     # å„åˆ—ã®ãƒ‡ãƒ¼ã‚¿å‹ã‚’ç¢ºèª
# float64ï¼šæµ®å‹•å°æ•°ç‚¹æ•°ã€objectï¼šæ–‡å­—åˆ—

print("\nğŸ“Š æ¬ æå€¤ã®ç¢ºèª:")
print(df.isnull().sum())  # å„åˆ—ã®æ¬ æå€¤ï¼ˆNaNï¼‰ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
# isnull()ï¼šæ¬ æå€¤ã‚’True/Falseã§åˆ¤å®š
# sum()ï¼šTrueã®æ•°ï¼ˆæ¬ æå€¤ã®æ•°ï¼‰ã‚’é›†è¨ˆ

# === ã‚»ãƒ«4: ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–ã«ã‚ˆã‚‹æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æ ===

# ã‚°ãƒ©ãƒ•ã®ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š
plt.style.use('default')  # matplotlib ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¹ã‚¿ã‚¤ãƒ«ã‚’ä½¿ç”¨
sns.set_palette("husl")   # seaborn ã®ã‚«ãƒ©ãƒ¼ãƒ‘ãƒ¬ãƒƒãƒˆã‚’è¨­å®šï¼ˆè‰²ã¨ã‚Šã©ã‚Šã®è‰²åˆã„ï¼‰

# 2è¡Œ2åˆ—ã®ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆï¼ˆ4ã¤ã®ã‚°ãƒ©ãƒ•ã‚’1ã¤ã®å›³ã«é…ç½®ï¼‰
fig, axes = plt.subplots(2, 2, figsize=(15, 12))
# subplots(2, 2)ï¼š2Ã—2ã®æ ¼å­çŠ¶ã«ã‚°ãƒ©ãƒ•é…ç½®
# figsize=(15, 12)ï¼šå›³å…¨ä½“ã®ã‚µã‚¤ã‚ºï¼ˆå¹…15ã‚¤ãƒ³ãƒã€é«˜ã•12ã‚¤ãƒ³ãƒï¼‰

# ã‚°ãƒ©ãƒ•1: èŠ±ã³ã‚‰ã®é•·ã• vs å¹…ã®æ•£å¸ƒå›³
sns.scatterplot(data=df,                    # ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
                x='petal length (cm)',      # Xè»¸ï¼šèŠ±ã³ã‚‰ã®é•·ã•
                y='petal width (cm)',       # Yè»¸ï¼šèŠ±ã³ã‚‰ã®å¹…
                hue='species',              # è‰²åˆ†ã‘ï¼šå“ç¨®åˆ¥ã«è‰²ã‚’å¤‰ãˆã‚‹
                ax=axes[0,0])               # æç”»ä½ç½®ï¼š1è¡Œ1åˆ—ç›®
# scatterplot()ï¼šæ•£å¸ƒå›³ã‚’ä½œæˆã€hueï¼šã‚«ãƒ†ã‚´ãƒªåˆ¥ã«è‰²åˆ†ã‘
axes[0,0].set_title('èŠ±ã³ã‚‰ï¼šé•·ã• vs å¹…', fontsize=14, fontweight='bold')
# set_title()ï¼šã‚°ãƒ©ãƒ•ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¨­å®š

# ã‚°ãƒ©ãƒ•2: ãŒãç‰‡ã®é•·ã• vs å¹…ã®æ•£å¸ƒå›³
sns.scatterplot(data=df,
                x='sepal length (cm)',      # Xè»¸ï¼šãŒãç‰‡ã®é•·ã•
                y='sepal width (cm)',       # Yè»¸ï¼šãŒãç‰‡ã®å¹…
                hue='species',
                ax=axes[0,1])               # æç”»ä½ç½®ï¼š1è¡Œ2åˆ—ç›®
axes[0,1].set_title('ãŒãç‰‡ï¼šé•·ã• vs å¹…', fontsize=14, fontweight='bold')

# ã‚°ãƒ©ãƒ•3: èŠ±ã³ã‚‰ã®é•·ã•ã®åˆ†å¸ƒï¼ˆç®±ã²ã’å›³ï¼‰
sns.boxplot(data=df,
           x='species',                     # Xè»¸ï¼šå“ç¨®
           y='petal length (cm)',          # Yè»¸ï¼šèŠ±ã³ã‚‰ã®é•·ã•
           ax=axes[1,0])                   # æç”»ä½ç½®ï¼š2è¡Œ1åˆ—ç›®
# boxplot()ï¼šå››åˆ†ä½æ•°ã‚’è¡¨ç¤ºã™ã‚‹ç®±ã²ã’å›³
axes[1,0].set_title('å“ç¨®åˆ¥ èŠ±ã³ã‚‰ã®é•·ã•åˆ†å¸ƒ', fontsize=14, fontweight='bold')
axes[1,0].tick_params(axis='x', rotation=45)  # Xè»¸ãƒ©ãƒ™ãƒ«ã‚’45åº¦å›è»¢

# ã‚°ãƒ©ãƒ•4: ç‰¹å¾´é‡é–“ã®ç›¸é–¢é–¢ä¿‚ï¼ˆãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼‰
correlation_matrix = df.select_dtypes(include=[np.number]).corr()
# select_dtypes()ï¼šæ•°å€¤å‹ã®åˆ—ã®ã¿é¸æŠ
# corr()ï¼šç›¸é–¢ä¿‚æ•°è¡Œåˆ—ã‚’è¨ˆç®—ï¼ˆ-1ã‹ã‚‰1ã®å€¤ã€1ã«è¿‘ã„ã»ã©æ­£ã®ç›¸é–¢ï¼‰

sns.heatmap(correlation_matrix,             # ç›¸é–¢è¡Œåˆ—ãƒ‡ãƒ¼ã‚¿
           annot=True,                      # æ•°å€¤ã‚’è¡¨ç¤º
           cmap='coolwarm',                 # ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—ï¼ˆé’èµ¤ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
           center=0,                        # 0ã‚’ä¸­å¤®å€¤ã«è¨­å®š
           square=True,                     # æ­£æ–¹å½¢ã®ã‚»ãƒ«ã«ã™ã‚‹
           fmt='.2f',                       # å°æ•°ç‚¹2æ¡ã§è¡¨ç¤º
           ax=axes[1,1])                    # æç”»ä½ç½®ï¼š2è¡Œ2åˆ—ç›®
axes[1,1].set_title('ç‰¹å¾´é‡é–“ã®ç›¸é–¢é–¢ä¿‚', fontsize=14, fontweight='bold')

# ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’è‡ªå‹•èª¿æ•´ï¼ˆã‚°ãƒ©ãƒ•åŒå£«ã®é‡ãªã‚Šã‚’é˜²ãï¼‰
plt.tight_layout()
plt.show()  # ã‚°ãƒ©ãƒ•ã‚’ç”»é¢ã«è¡¨ç¤º

# é‡è¦ãªæ´å¯Ÿã‚’ã‚³ãƒ¡ãƒ³ãƒˆã§å‡ºåŠ›
print("ğŸ’¡ é‡è¦ãªæ´å¯Ÿ:")
print("1. èŠ±ã³ã‚‰ã®ç‰¹å¾´ï¼ˆé•·ã•ãƒ»å¹…ï¼‰ã§å“ç¨®ãŒã‚ˆãåˆ†ã‹ã‚Œã¦ã„ã‚‹")
print("2. setosa ã¯ä»–ã®2å“ç¨®ã¨æ˜ç¢ºã«åŒºåˆ¥ã§ãã‚‹")
print("3. versicolor ã¨ virginica ã¯å°‘ã—é‡è¤‡ã—ã¦ã„ã‚‹")
print("4. èŠ±ã³ã‚‰ã®é•·ã•ã¨å¹…ã«ã¯å¼·ã„æ­£ã®ç›¸é–¢ãŒã‚ã‚‹")

# === ã‚»ãƒ«5: ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ï¼ˆæ©Ÿæ¢°å­¦ç¿’ã®æº–å‚™ï¼‰ ===

print("ğŸ”§ ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†é–‹å§‹...")

# ç‰¹å¾´é‡ï¼ˆXï¼‰ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆyï¼‰ã«åˆ†é›¢
X = iris.data   # ç‰¹å¾´é‡ï¼š4ã¤ã®æ¸¬å®šå€¤ï¼ˆãŒãç‰‡ãƒ»èŠ±ã³ã‚‰ã®é•·ã•å¹…ï¼‰
y = iris.target # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼šå“ç¨®ãƒ©ãƒ™ãƒ«ï¼ˆ0, 1, 2ã®æ•°å€¤ï¼‰

print(f"ç‰¹å¾´é‡Xã®å½¢çŠ¶: {X.shape}")  # (150, 4)ï¼š150ã‚µãƒ³ãƒ—ãƒ«ã€4ç‰¹å¾´é‡
print(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆyã®å½¢çŠ¶: {y.shape}")  # (150,)ï¼š150å€‹ã®ãƒ©ãƒ™ãƒ«

# ãƒ‡ãƒ¼ã‚¿ã‚’è¨“ç·´ç”¨ã¨ãƒ†ã‚¹ãƒˆç”¨ã«åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y,                    # åˆ†å‰²ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ï¼ˆç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼‰
    test_size=0.3,          # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å‰²åˆï¼ˆ30%ï¼‰
    random_state=42,        # ä¹±æ•°ã‚·ãƒ¼ãƒ‰ï¼ˆçµæœã®å†ç¾æ€§ã‚’ä¿è¨¼ï¼‰
    stratify=y              # å„ã‚¯ãƒ©ã‚¹ã®æ¯”ç‡ã‚’è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆã§åŒã˜ã«ã™ã‚‹
)
# train_test_split()ï¼šãƒ‡ãƒ¼ã‚¿ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«åˆ†å‰²
# stratify=yï¼šå„å“ç¨®ã®æ¯”ç‡ã‚’ä¿æŒã—ã¦åˆ†å‰²

print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {X_train.shape[0]}ã‚µãƒ³ãƒ—ãƒ«")  # è¨“ç·´ç”¨ã®ã‚µãƒ³ãƒ—ãƒ«æ•°
print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {X_test.shape[0]}ã‚µãƒ³ãƒ—ãƒ«")  # ãƒ†ã‚¹ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ«æ•°

# å„ã‚»ãƒƒãƒˆã§ã®ã‚¯ãƒ©ã‚¹åˆ†å¸ƒã‚’ç¢ºèª
train_unique, train_counts = np.unique(y_train, return_counts=True)
test_unique, test_counts = np.unique(y_test, return_counts=True)

print("è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ:", dict(zip(train_unique, train_counts)))
print("ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ:", dict(zip(test_unique, test_counts)))

# ç‰¹å¾´é‡ã®æ­£è¦åŒ–ï¼ˆã‚¹ã‚±ãƒ¼ãƒ«ã‚’æƒãˆã‚‹ï¼‰
scaler = StandardScaler()  # æ¨™æº–åŒ–å™¨ã‚’ä½œæˆï¼ˆå¹³å‡0ã€æ¨™æº–åå·®1ã«å¤‰æ›ï¼‰
X_train_scaled = scaler.fit_transform(X_train)  # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’&å¤‰æ›
X_test_scaled = scaler.transform(X_test)        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›ï¼ˆå­¦ç¿’ã¯ä¸è¦ï¼‰

# fit_transform()ï¼šå¹³å‡ã¨æ¨™æº–åå·®ã‚’å­¦ç¿’ã—ã¦ã‹ã‚‰å¤‰æ›
# transform()ï¼šæ—¢ã«å­¦ç¿’æ¸ˆã¿ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å¤‰æ›ã®ã¿å®Ÿè¡Œ

print("æ­£è¦åŒ–å‰ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:")
print(f"å¹³å‡: {X_train.mean(axis=0)}")      # å„ç‰¹å¾´é‡ã®å¹³å‡
print(f"æ¨™æº–åå·®: {X_train.std(axis=0)}")    # å„ç‰¹å¾´é‡ã®æ¨™æº–åå·®

print("æ­£è¦åŒ–å¾Œã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:")
print(f"å¹³å‡: {X_train_scaled.mean(axis=0)}")    # ã»ã¼0ã«ãªã‚‹
print(f"æ¨™æº–åå·®: {X_train_scaled.std(axis=0)}")  # ã»ã¼1ã«ãªã‚‹

print("âœ… ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†å®Œäº†ï¼")

# === ã‚»ãƒ«6: è¤‡æ•°ã®æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬ ===

print("ğŸ¤– è¤‡æ•°ã®æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§æ€§èƒ½æ¯”è¼ƒ")
print("-" * 60)

# è¤‡æ•°ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’è¾æ›¸ã§ç®¡ç†
models = {
    'ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°': LogisticRegression(random_state=42, max_iter=1000),
    # ç·šå½¢åˆ†é¡å™¨ã€ç¢ºç‡çš„å‡ºåŠ›ã€é«˜é€Ÿã€è§£é‡ˆã—ã‚„ã™ã„
    
    'æ±ºå®šæœ¨': DecisionTreeClassifier(random_state=42, max_depth=5),
    # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã€å¯è¦–åŒ–å¯èƒ½ã€éå­¦ç¿’ã—ã‚„ã™ã„
    
    'ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ': RandomForestClassifier(n_estimators=100, random_state=42),
    # æ±ºå®šæœ¨ã®é›†åˆã€å®‰å®šã€ç‰¹å¾´é‡é‡è¦åº¦å–å¾—å¯èƒ½
    
    'SVM': SVC(random_state=42, probability=True),
    # éç·šå½¢åˆ†é¡ã€é«˜æ€§èƒ½ã€è¨ˆç®—ã‚³ã‚¹ãƒˆé«˜
    
    'kè¿‘å‚æ³•': KNeighborsClassifier(n_neighbors=5)
    # è¿‘éš£ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰äºˆæ¸¬ã€ã‚·ãƒ³ãƒ—ãƒ«ã€ç›´æ„Ÿçš„
}

# å„ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’æ ¼ç´ã™ã‚‹è¾æ›¸
results = {}

# å„ãƒ¢ãƒ‡ãƒ«ã‚’é †ç•ªã«å­¦ç¿’ãƒ»è©•ä¾¡
for name, model in models.items():
    print(f"\nğŸ”„ {name} ã®å­¦ç¿’ã¨è©•ä¾¡ä¸­...")
    
    # ãƒ¢ãƒ‡ãƒ«ã®ç¨®é¡ã«å¿œã˜ã¦ãƒ‡ãƒ¼ã‚¿ã‚’é¸æŠ
    if name in ['ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°', 'SVM']:
        # ç·šå½¢æ‰‹æ³•ï¼šæ­£è¦åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼ˆã‚¹ã‚±ãƒ¼ãƒ«ã«æ•æ„ŸãªãŸã‚ï¼‰
        model.fit(X_train_scaled, y_train)      # å­¦ç¿’å®Ÿè¡Œ
        predictions = model.predict(X_test_scaled)  # äºˆæ¸¬å®Ÿè¡Œ
    else:
        # æœ¨ç³»æ‰‹æ³•ï¼šå…ƒãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼ˆã‚¹ã‚±ãƒ¼ãƒ«ã«éˆæ„ŸãªãŸã‚ï¼‰
        model.fit(X_train, y_train)             # å­¦ç¿’å®Ÿè¡Œ
        predictions = model.predict(X_test)      # äºˆæ¸¬å®Ÿè¡Œ
    
    # ç²¾åº¦ã‚’è¨ˆç®—
    accuracy = accuracy_score(y_test, predictions)
    # accuracy_score()ï¼šæ­£è§£ç‡ã‚’è¨ˆç®—ï¼ˆæ­£è§£æ•° Ã· å…¨äºˆæ¸¬æ•°ï¼‰
    
    # çµæœã‚’ä¿å­˜
    results[name] = {
        'accuracy': accuracy,      # ç²¾åº¦
        'predictions': predictions, # äºˆæ¸¬çµæœ
        'model': model             # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
    }
    
    # çµæœã‚’è¡¨ç¤º
    print(f"âœ… {name}: {accuracy:.4f} ({accuracy*100:.1f}%)")

# æœ€é«˜æ€§èƒ½ã®ãƒ¢ãƒ‡ãƒ«ã‚’ç‰¹å®š
best_model_name = max(results.keys(), key=lambda x: results[x]['accuracy'])
best_accuracy = results[best_model_name]['accuracy']

print(f"\nğŸ† æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«: {best_model_name}")
print(f"ğŸ“ˆ æœ€é«˜ç²¾åº¦: {best_accuracy:.4f} ({best_accuracy*100:.1f}%)")

# === ã‚»ãƒ«7: æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°åˆ†æ ===

print(f"\nğŸ“Š {best_model_name} ã®è©³ç´°åˆ†æ")
print("=" * 50)

# æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã¨ãã®äºˆæ¸¬çµæœã‚’å–å¾—
best_model = results[best_model_name]['model']
best_predictions = results[best_model_name]['predictions']

# æ··åŒè¡Œåˆ—ã‚’ä½œæˆï¼ˆäºˆæ¸¬ãƒŸã‚¹ã®è©³ç´°åˆ†æï¼‰
cm = confusion_matrix(y_test, best_predictions)
# confusion_matrix()ï¼šå®Ÿéš›ã®ã‚¯ãƒ©ã‚¹ vs äºˆæ¸¬ã‚¯ãƒ©ã‚¹ã®ã‚¯ãƒ­ã‚¹è¡¨

# æ··åŒè¡Œåˆ—ã‚’å¯è¦–åŒ–
plt.figure(figsize=(8, 6))  # å›³ã®ã‚µã‚¤ã‚ºã‚’è¨­å®š
sns.heatmap(cm,                              # æ··åŒè¡Œåˆ—ãƒ‡ãƒ¼ã‚¿
           annot=True,                       # æ•°å€¤ã‚’è¡¨ç¤º
           fmt='d',                          # æ•´æ•°å½¢å¼ã§è¡¨ç¤º
           cmap='Blues',                     # é’è‰²ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
           xticklabels=iris.target_names,    # Xè»¸ãƒ©ãƒ™ãƒ«ï¼ˆäºˆæ¸¬ã‚¯ãƒ©ã‚¹ï¼‰
           yticklabels=iris.target_names)    # Yè»¸ãƒ©ãƒ™ãƒ«ï¼ˆå®Ÿéš›ã®ã‚¯ãƒ©ã‚¹ï¼‰

plt.title(f'æ··åŒè¡Œåˆ— - {best_model_name}', fontsize=16, fontweight='bold')
plt.ylabel('å®Ÿéš›ã®ã‚¯ãƒ©ã‚¹', fontsize=12)
plt.xlabel('äºˆæ¸¬ã‚¯ãƒ©ã‚¹', fontsize=12)
plt.tight_layout()
plt.show()

# è©³ç´°ãªåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã‚’è¡¨ç¤º
print("ğŸ“‹ è©³ç´°ãªåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
report = classification_report(y_test, best_predictions, 
                             target_names=iris.target_names)
print(report)
# precisionï¼šç²¾å¯†åº¦ï¼ˆäºˆæ¸¬ã—ãŸä¸­ã§ã®æ­£è§£ç‡ï¼‰
# recallï¼šå†ç¾ç‡ï¼ˆå®Ÿéš›ã®æ­£è§£ã‚’äºˆæ¸¬ã§ããŸå‰²åˆï¼‰
# f1-scoreï¼šç²¾å¯†åº¦ã¨å†ç¾ç‡ã®èª¿å’Œå¹³å‡
# supportï¼šå„ã‚¯ãƒ©ã‚¹ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ•°

# å„ã‚¯ãƒ©ã‚¹ã”ã¨ã®ç²¾åº¦ã‚’è¨ˆç®—
print("\nğŸ“Š å„å“ç¨®ã®äºˆæ¸¬ç²¾åº¦:")
for i, species in enumerate(iris.target_names):
    # ãã®ã‚¯ãƒ©ã‚¹ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
    class_indices = (y_test == i)
    # ãã®ã‚¯ãƒ©ã‚¹ã®äºˆæ¸¬çµæœã‚’å–å¾—
    class_predictions = best_predictions[class_indices]
    # ãã®ã‚¯ãƒ©ã‚¹ã®æ­£è§£æ•°ã‚’è¨ˆç®—
    class_accuracy = (class_predictions == i).sum() / len(class_predictions)
    
    print(f"  {species}: {class_accuracy:.4f} ({class_accuracy*100:.1f}%)")

# === ã‚»ãƒ«8: ç‰¹å¾´é‡é‡è¦åº¦ã®åˆ†æï¼ˆãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®å ´åˆï¼‰ ===

# ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãŒæœ€é«˜æ€§èƒ½ã®å ´åˆã€ç‰¹å¾´é‡é‡è¦åº¦ã‚’åˆ†æ
if best_model_name == 'ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ':
    print(f"\nğŸ” {best_model_name} ã®ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ")
    print("-" * 40)
    
    # ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—
    feature_importance = best_model.feature_importances_
    # feature_importances_ï¼šå„ç‰¹å¾´é‡ãŒãƒ¢ãƒ‡ãƒ«ã«ã©ã‚Œã ã‘è²¢çŒ®ã—ãŸã‹ã®æŒ‡æ¨™
    
    # ç‰¹å¾´é‡åã¨é‡è¦åº¦ã‚’DataFrameã«æ•´ç†
    importance_df = pd.DataFrame({
        'feature': iris.feature_names,      # ç‰¹å¾´é‡å
        'importance': feature_importance    # é‡è¦åº¦ã‚¹ã‚³ã‚¢
    }).sort_values('importance', ascending=False)  # é‡è¦åº¦é™é †ã§ã‚½ãƒ¼ãƒˆ
    
    print("ç‰¹å¾´é‡é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°:")
    for i, row in importance_df.iterrows():
        print(f"{i+1}. {row['feature']}: {row['importance']:.4f}")
    
    # ç‰¹å¾´é‡é‡è¦åº¦ã‚’æ£’ã‚°ãƒ©ãƒ•ã§å¯è¦–åŒ–
    plt.figure(figsize=(10, 6))
    sns.barplot(data=importance_df,          # ãƒ‡ãƒ¼ã‚¿
               x='importance',               # Xè»¸ï¼šé‡è¦åº¦
               y='feature',                  # Yè»¸ï¼šç‰¹å¾´é‡å
               palette='viridis')            # ã‚«ãƒ©ãƒ¼ãƒ‘ãƒ¬ãƒƒãƒˆ
    
    plt.title(f'{best_model_name} ç‰¹å¾´é‡é‡è¦åº¦', fontsize=16, fontweight='bold')
    plt.xlabel('é‡è¦åº¦', fontsize=12)
    plt.ylabel('ç‰¹å¾´é‡', fontsize=12)
    plt.tight_layout()
    plt.show()

# === ã‚»ãƒ«9: æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã§ã®äºˆæ¸¬ä¾‹ ===

print("\nğŸ¯ æ–°ã—ã„ã‚¢ã‚¤ãƒªã‚¹ãƒ‡ãƒ¼ã‚¿ã§ã®äºˆæ¸¬ä¾‹")
print("-" * 40)

# æ–°ã—ã„ã‚¢ã‚¤ãƒªã‚¹ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆä¾‹ï¼šå®Ÿéš›ã®æ¸¬å®šå€¤ï¼‰
new_samples = np.array([
    [5.1, 3.5, 1.4, 0.2],  # setosa ã£ã½ã„ç‰¹å¾´
    [6.0, 2.8, 4.5, 1.5],  # versicolor ã£ã½ã„ç‰¹å¾´  
    [7.2, 3.2, 6.0, 2.0]   # virginica ã£ã½ã„ç‰¹å¾´
])

# æ–°ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã®ç‰¹å¾´é‡åã‚’è¡¨ç¤º
feature_names = iris.feature_names
print("æ–°ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã®ç‰¹å¾´é‡:")
for i, sample in enumerate(new_samples):
    print(f"\nã‚µãƒ³ãƒ—ãƒ« {i+1}:")
    for j, (feature, value) in enumerate(zip(feature_names, sample)):
        print(f"  {feature}: {value}")

# æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬
if best_model_name in ['ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°', 'SVM']:
    # ç·šå½¢æ‰‹æ³•ã®å ´åˆã¯æ­£è¦åŒ–ãŒå¿…è¦
    new_samples_scaled = scaler.transform(new_samples)
    new_predictions = best_model.predict(new_samples_scaled)
    new_probabilities = best_model.predict_proba(new_samples_scaled)
else:
    # æœ¨ç³»æ‰‹æ³•ã®å ´åˆã¯å…ƒãƒ‡ãƒ¼ã‚¿ã®ã¾ã¾
    new_predictions = best_model.predict(new_samples)
    # predict_proba()ãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
    if hasattr(best_model, 'predict_proba'):
        new_probabilities = best_model.predict_proba(new_samples)
    else:
        new_probabilities = None

# äºˆæ¸¬çµæœã‚’è¡¨ç¤º
print(f"\nğŸ”® {best_model_name} ã«ã‚ˆã‚‹äºˆæ¸¬çµæœ:")
for i, (sample, prediction) in enumerate(zip(new_samples, new_predictions)):
    predicted_species = iris.target_names[prediction]  # æ•°å€¤ã‚’å“ç¨®åã«å¤‰æ›
    print(f"\nã‚µãƒ³ãƒ—ãƒ« {i+1}: {predicted_species}")
    
    # ç¢ºç‡ãŒå–å¾—ã§ãã‚‹å ´åˆã¯è¡¨ç¤º
    if new_probabilities is not None:
        probs = new_probabilities[i]  # ãã®ã‚µãƒ³ãƒ—ãƒ«ã®å„ã‚¯ãƒ©ã‚¹ç¢ºç‡
        print("  å„å“ç¨®ã®äºˆæ¸¬ç¢ºç‡:")
        for j, (species, prob) in enumerate(zip(iris.target_names, probs)):
            print(f"    {species}: {prob:.4f} ({prob*100:.1f}%)")

# === ã‚»ãƒ«10: å­¦ç¿’çµæœã®ã¾ã¨ã‚ã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ— ===

print("\nğŸ‰ ã‚¢ã‚¤ãƒªã‚¹åˆ†é¡ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå®Œäº†ï¼")
print("=" * 50)

print("\nğŸ“Š ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆçµæœã¾ã¨ã‚:")
print(f"âœ… ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿: {len(iris.data)}ã‚µãƒ³ãƒ—ãƒ«ã€{len(iris.feature_names)}ç‰¹å¾´é‡")
print(f"âœ… è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(X_train)}ã‚µãƒ³ãƒ—ãƒ«")
print(f"âœ… ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(X_test)}ã‚µãƒ³ãƒ—ãƒ«")
print(f"âœ… è©¦è¡Œãƒ¢ãƒ‡ãƒ«æ•°: {len(models)}ç¨®é¡")
print(f"âœ… æœ€é«˜æ€§èƒ½: {best_model_name} ({best_accuracy:.4f})")

print("\nğŸ¯ ä»Šå›å­¦ã‚“ã æŠ€è¡“:")
skills = [
    "1. scikit-learn ã§ã®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿",
    "2. Pandas ã‚’ä½¿ã£ãŸãƒ‡ãƒ¼ã‚¿æ“ä½œ", 
    "3. matplotlib/seaborn ã§ã®å¯è¦–åŒ–",
    "4. ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ï¼ˆåˆ†å‰²ãƒ»æ­£è¦åŒ–ï¼‰",
    "5. è¤‡æ•°ã®æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æ¯”è¼ƒ",
    "6. ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ï¼ˆç²¾åº¦ãƒ»æ··åŒè¡Œåˆ—ãƒ»åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆï¼‰",
    "7. ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ",
    "8. æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã§ã®äºˆæ¸¬"
]

for skill in skills:
    print(f"  {skill}")

print("\nğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆæ¨å¥¨ï¼‰:")
next_steps = [
    "1. ä»–ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆXGBoostã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼‰ã‚’è©¦ã™",
    "2. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ã§æ€§èƒ½å‘ä¸Šã‚’å›³ã‚‹",
    "3. äº¤å·®æ¤œè¨¼ã§ã‚ˆã‚Šä¿¡é ¼æ€§ã®é«˜ã„è©•ä¾¡ã‚’å®Ÿæ–½",
    "4. ä»–ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆã‚¿ã‚¤ã‚¿ãƒ‹ãƒƒã‚¯ã€ä½å®…ä¾¡æ ¼ï¼‰ã«æŒ‘æˆ¦",
    "5. Kaggleç«¶æŠ€ã«å‚åŠ ã—ã¦ã‚¹ã‚­ãƒ«ã‚¢ãƒƒãƒ—"
]

for step in next_steps:
    print(f"  {step}")

print("\nğŸ’¡ é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ:")
print("- æ©Ÿæ¢°å­¦ç¿’ã¯ã€Œãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã€ãŒæœ€ã‚‚é‡è¦")
print("- è¤‡æ•°ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’è©¦ã—ã¦æ¯”è¼ƒã™ã‚‹ã“ã¨ãŒå¤§åˆ‡")
print("- ç²¾åº¦ã ã‘ã§ãªãã€ãªãœãã†ãªã‚‹ã‹ã‚’ç†è§£ã™ã‚‹")
print("- å¯è¦–åŒ–ã§ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´ã‚’æŠŠæ¡ã™ã‚‹")
print("- å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬ã—ã¦å®Ÿç”¨æ€§ã‚’ç¢ºèªã™ã‚‹")

print("\nğŸŒŸ ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼")
print("ã‚ãªãŸã¯ä»Šã€æ©Ÿæ¢°å­¦ç¿’ã®åŸºæœ¬çš„ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’")
print("ä¸€é€šã‚Šå®Ÿè£…ã§ãã‚‹ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã™ï¼")

# æœ€å¾Œã«å…¨ä½“ã®å‡¦ç†æ™‚é–“ã‚’è¡¨ç¤ºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
import time
print(f"\nâ±ï¸  å‡¦ç†å®Œäº†: {time.strftime('%Y-%m-%d %H:%M:%S')}")

# ========================================
# è£œè¶³ï¼šã•ã‚‰ãªã‚‹å­¦ç¿’ã®ãŸã‚ã®ã‚³ãƒ¼ãƒ‰ä¾‹
# ========================================

print("\nğŸ“š ç™ºå±•å­¦ç¿’ç”¨ã®ã‚³ãƒ¼ãƒ‰ä¾‹:")

# 1. äº¤å·®æ¤œè¨¼ã§ã®æ€§èƒ½è©•ä¾¡
from sklearn.model_selection import cross_val_score

print("\nğŸ”„ äº¤å·®æ¤œè¨¼ã§ã®è©•ä¾¡:")
cv_scores = cross_val_score(best_model, X, y, cv=5)  # 5åˆ†å‰²äº¤å·®æ¤œè¨¼
print(f"CVå¹³å‡ã‚¹ã‚³ã‚¢: {cv_scores.mean():.4f}")
print(f"CVæ¨™æº–åå·®: {cv_scores.std():.4f}")

# 2. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ã®ä¾‹
from sklearn.model_selection import GridSearchCV

if best_model_name == 'ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ':
    print("\nâš™ï¸ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ä¾‹:")
    param_grid = {
        'n_estimators': [50, 100, 200],      # æ±ºå®šæœ¨ã®æ•°
        'max_depth': [3, 5, 7, None]         # æœ€å¤§æ·±åº¦
    }
    
    grid_search = GridSearchCV(RandomForestClassifier(random_state=42), 
                              param_grid, cv=3, scoring='accuracy')
    grid_search.fit(X_train, y_train)
    
    print(f"æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {grid_search.best_params_}")
    print(f"æœ€é«˜ã‚¹ã‚³ã‚¢: {grid_search.best_score_:.4f}")

print("\nğŸ“ å­¦ç¿’å®Œäº†ï¼æ¬¡ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«æŒ‘æˆ¦ã—ã¾ã—ã‚‡ã†ï¼")
