# ========================================
# アイリス分類 - Google Colab 1行ずつ詳細解説
# 新人エンジニア向け：各行の意味と目的を完全解説
# ========================================

# === セル1: ライブラリのインポート（必要な道具を準備） ===

import numpy as np               # 数値計算ライブラリ（配列操作、数学計算を高速実行）
import pandas as pd              # データフレーム操作ライブラリ（Excelのような表形式データを扱う）
import matplotlib.pyplot as plt  # グラフ描画ライブラリ（棒グラフ、散布図、線グラフなどを作成）
import seaborn as sns           # 統計グラフライブラリ（matplotlibより美しく簡単にグラフ作成）

# scikit-learn（機械学習ライブラリ）から必要な機能をインポート
from sklearn import datasets          # サンプルデータセット（アイリス、ワイン、ボストン住宅価格など）
from sklearn.model_selection import train_test_split  # データを訓練用とテスト用に分割する機能
from sklearn.preprocessing import StandardScaler      # データの正規化（スケールを揃える）機能
from sklearn.linear_model import LogisticRegression   # ロジスティック回帰（線形分類器）
from sklearn.tree import DecisionTreeClassifier       # 決定木（ルールベース分類器）
from sklearn.ensemble import RandomForestClassifier   # ランダムフォレスト（決定木の集合体）
from sklearn.svm import SVC                           # サポートベクターマシン（非線形分類器）
from sklearn.neighbors import KNeighborsClassifier    # k近傍法（似ているデータから予測）
from sklearn.metrics import accuracy_score            # 精度計算（正解率を算出）
from sklearn.metrics import classification_report    # 詳細な分類レポート作成
from sklearn.metrics import confusion_matrix          # 混同行列（予測ミスの詳細分析）

print("🌸 必要なライブラリのインポート完了！")  # 処理完了の確認メッセージ

# === セル2: アイリスデータセットの読み込みと基本確認 ===

# アイリスデータセットを読み込み（scikit-learnに組み込まれている有名なサンプルデータ）
iris = datasets.load_iris()
# load_iris()：150個のアイリス花のデータ（4つの測定値と3つの品種ラベル）

print("🌸 アイリスデータセット分析開始！")  # 分析開始の宣言

# データの基本情報を確認
print(f"データサイズ: {iris.data.shape}")  # データの形状（行数, 列数）を表示
# iris.data.shape：(150, 4) → 150サンプル、4特徴量

print(f"特徴量名: {iris.feature_names}")   # 4つの特徴量の名前を表示
# ['sepal length', 'sepal width', 'petal length', 'petal width']
# sepal=がく片、petal=花びら、length=長さ、width=幅

print(f"クラス名: {iris.target_names}")     # 3つの品種名を表示
# ['setosa', 'versicolor', 'virginica']：アイリスの3品種

print(f"ターゲットの形状: {iris.target.shape}")  # 正解ラベルの形状
# (150,)：150個の正解ラベル（0, 1, 2の数値）

# 各クラスの分布を確認
unique, counts = np.unique(iris.target, return_counts=True)
# np.unique()：ユニークな値とその出現回数を取得
# unique：[0, 1, 2]、counts：各クラスの個数

print("クラス分布:")
for i, (cls, count) in enumerate(zip(iris.target_names, counts)):
    # zip()：2つのリストを同時に取得
    print(f"  {cls}: {count}個")  # 各品種のデータ数を表示

# === セル3: データをPandasのDataFrameに変換（見やすくするため） ===

# NumPy配列をPandasのDataFrameに変換（表形式で見やすくする）
df = pd.DataFrame(iris.data, columns=iris.feature_names)
# pd.DataFrame()：表形式のデータ構造を作成
# iris.data：特徴量データ（150×4の配列）
# columns：列名を指定

# ターゲット（品種名）を文字列で追加
df['species'] = iris.target_names[iris.target]
# iris.target：[0, 1, 2, 1, 0, ...]の数値配列
# iris.target_names[iris.target]：数値を品種名に変換

print("📊 データの最初の5行:")
print(df.head())  # head()：データの最初の5行を表示
# データの構造と内容を確認するため

print("\n📊 データの最後の5行:")
print(df.tail())  # tail()：データの最後の5行を表示

print("\n📊 データの基本統計量:")
print(df.describe())  # describe()：平均、標準偏差、最小値、最大値などを計算
# 数値列のみが対象（species列は除外される）

print("\n📊 データ型の確認:")
print(df.dtypes)     # 各列のデータ型を確認
# float64：浮動小数点数、object：文字列

print("\n📊 欠損値の確認:")
print(df.isnull().sum())  # 各列の欠損値（NaN）の数をカウント
# isnull()：欠損値をTrue/Falseで判定
# sum()：Trueの数（欠損値の数）を集計

# === セル4: データの可視化による探索的データ分析 ===

# グラフのスタイル設定
plt.style.use('default')  # matplotlib のデフォルトスタイルを使用
sns.set_palette("husl")   # seaborn のカラーパレットを設定（色とりどりの色合い）

# 2行2列のサブプロット（4つのグラフを1つの図に配置）
fig, axes = plt.subplots(2, 2, figsize=(15, 12))
# subplots(2, 2)：2×2の格子状にグラフ配置
# figsize=(15, 12)：図全体のサイズ（幅15インチ、高さ12インチ）

# グラフ1: 花びらの長さ vs 幅の散布図
sns.scatterplot(data=df,                    # 使用するデータフレーム
                x='petal length (cm)',      # X軸：花びらの長さ
                y='petal width (cm)',       # Y軸：花びらの幅
                hue='species',              # 色分け：品種別に色を変える
                ax=axes[0,0])               # 描画位置：1行1列目
# scatterplot()：散布図を作成、hue：カテゴリ別に色分け
axes[0,0].set_title('花びら：長さ vs 幅', fontsize=14, fontweight='bold')
# set_title()：グラフタイトルを設定

# グラフ2: がく片の長さ vs 幅の散布図
sns.scatterplot(data=df,
                x='sepal length (cm)',      # X軸：がく片の長さ
                y='sepal width (cm)',       # Y軸：がく片の幅
                hue='species',
                ax=axes[0,1])               # 描画位置：1行2列目
axes[0,1].set_title('がく片：長さ vs 幅', fontsize=14, fontweight='bold')

# グラフ3: 花びらの長さの分布（箱ひげ図）
sns.boxplot(data=df,
           x='species',                     # X軸：品種
           y='petal length (cm)',          # Y軸：花びらの長さ
           ax=axes[1,0])                   # 描画位置：2行1列目
# boxplot()：四分位数を表示する箱ひげ図
axes[1,0].set_title('品種別 花びらの長さ分布', fontsize=14, fontweight='bold')
axes[1,0].tick_params(axis='x', rotation=45)  # X軸ラベルを45度回転

# グラフ4: 特徴量間の相関関係（ヒートマップ）
correlation_matrix = df.select_dtypes(include=[np.number]).corr()
# select_dtypes()：数値型の列のみ選択
# corr()：相関係数行列を計算（-1から1の値、1に近いほど正の相関）

sns.heatmap(correlation_matrix,             # 相関行列データ
           annot=True,                      # 数値を表示
           cmap='coolwarm',                 # カラーマップ（青赤グラデーション）
           center=0,                        # 0を中央値に設定
           square=True,                     # 正方形のセルにする
           fmt='.2f',                       # 小数点2桁で表示
           ax=axes[1,1])                    # 描画位置：2行2列目
axes[1,1].set_title('特徴量間の相関関係', fontsize=14, fontweight='bold')

# レイアウトを自動調整（グラフ同士の重なりを防ぐ）
plt.tight_layout()
plt.show()  # グラフを画面に表示

# 重要な洞察をコメントで出力
print("💡 重要な洞察:")
print("1. 花びらの特徴（長さ・幅）で品種がよく分かれている")
print("2. setosa は他の2品種と明確に区別できる")
print("3. versicolor と virginica は少し重複している")
print("4. 花びらの長さと幅には強い正の相関がある")

# === セル5: データの前処理（機械学習の準備） ===

print("🔧 データ前処理開始...")

# 特徴量（X）とターゲット（y）に分離
X = iris.data   # 特徴量：4つの測定値（がく片・花びらの長さ幅）
y = iris.target # ターゲット：品種ラベル（0, 1, 2の数値）

print(f"特徴量Xの形状: {X.shape}")  # (150, 4)：150サンプル、4特徴量
print(f"ターゲットyの形状: {y.shape}")  # (150,)：150個のラベル

# データを訓練用とテスト用に分割
X_train, X_test, y_train, y_test = train_test_split(
    X, y,                    # 分割するデータ（特徴量とターゲット）
    test_size=0.3,          # テストデータの割合（30%）
    random_state=42,        # 乱数シード（結果の再現性を保証）
    stratify=y              # 各クラスの比率を訓練・テストで同じにする
)
# train_test_split()：データをランダムに分割
# stratify=y：各品種の比率を保持して分割

print(f"訓練データ: {X_train.shape[0]}サンプル")  # 訓練用のサンプル数
print(f"テストデータ: {X_test.shape[0]}サンプル")  # テスト用のサンプル数

# 各セットでのクラス分布を確認
train_unique, train_counts = np.unique(y_train, return_counts=True)
test_unique, test_counts = np.unique(y_test, return_counts=True)

print("訓練データのクラス分布:", dict(zip(train_unique, train_counts)))
print("テストデータのクラス分布:", dict(zip(test_unique, test_counts)))

# 特徴量の正規化（スケールを揃える）
scaler = StandardScaler()  # 標準化器を作成（平均0、標準偏差1に変換）
X_train_scaled = scaler.fit_transform(X_train)  # 訓練データで学習&変換
X_test_scaled = scaler.transform(X_test)        # テストデータを変換（学習は不要）

# fit_transform()：平均と標準偏差を学習してから変換
# transform()：既に学習済みのパラメータで変換のみ実行

print("正規化前の訓練データ統計:")
print(f"平均: {X_train.mean(axis=0)}")      # 各特徴量の平均
print(f"標準偏差: {X_train.std(axis=0)}")    # 各特徴量の標準偏差

print("正規化後の訓練データ統計:")
print(f"平均: {X_train_scaled.mean(axis=0)}")    # ほぼ0になる
print(f"標準偏差: {X_train_scaled.std(axis=0)}")  # ほぼ1になる

print("✅ データ前処理完了！")

# === セル6: 複数の機械学習モデルで予測 ===

print("🤖 複数の機械学習モデルで性能比較")
print("-" * 60)

# 複数のアルゴリズムを辞書で管理
models = {
    'ロジスティック回帰': LogisticRegression(random_state=42, max_iter=1000),
    # 線形分類器、確率的出力、高速、解釈しやすい
    
    '決定木': DecisionTreeClassifier(random_state=42, max_depth=5),
    # ルールベース、可視化可能、過学習しやすい
    
    'ランダムフォレスト': RandomForestClassifier(n_estimators=100, random_state=42),
    # 決定木の集合、安定、特徴量重要度取得可能
    
    'SVM': SVC(random_state=42, probability=True),
    # 非線形分類、高性能、計算コスト高
    
    'k近傍法': KNeighborsClassifier(n_neighbors=5)
    # 近隣データから予測、シンプル、直感的
}

# 各モデルの性能を格納する辞書
results = {}

# 各モデルを順番に学習・評価
for name, model in models.items():
    print(f"\n🔄 {name} の学習と評価中...")
    
    # モデルの種類に応じてデータを選択
    if name in ['ロジスティック回帰', 'SVM']:
        # 線形手法：正規化データを使用（スケールに敏感なため）
        model.fit(X_train_scaled, y_train)      # 学習実行
        predictions = model.predict(X_test_scaled)  # 予測実行
    else:
        # 木系手法：元データを使用（スケールに鈍感なため）
        model.fit(X_train, y_train)             # 学習実行
        predictions = model.predict(X_test)      # 予測実行
    
    # 精度を計算
    accuracy = accuracy_score(y_test, predictions)
    # accuracy_score()：正解率を計算（正解数 ÷ 全予測数）
    
    # 結果を保存
    results[name] = {
        'accuracy': accuracy,      # 精度
        'predictions': predictions, # 予測結果
        'model': model             # 学習済みモデル
    }
    
    # 結果を表示
    print(f"✅ {name}: {accuracy:.4f} ({accuracy*100:.1f}%)")

# 最高性能のモデルを特定
best_model_name = max(results.keys(), key=lambda x: results[x]['accuracy'])
best_accuracy = results[best_model_name]['accuracy']

print(f"\n🏆 最高性能モデル: {best_model_name}")
print(f"📈 最高精度: {best_accuracy:.4f} ({best_accuracy*100:.1f}%)")

# === セル7: 最高性能モデルの詳細分析 ===

print(f"\n📊 {best_model_name} の詳細分析")
print("=" * 50)

# 最高性能モデルとその予測結果を取得
best_model = results[best_model_name]['model']
best_predictions = results[best_model_name]['predictions']

# 混同行列を作成（予測ミスの詳細分析）
cm = confusion_matrix(y_test, best_predictions)
# confusion_matrix()：実際のクラス vs 予測クラスのクロス表

# 混同行列を可視化
plt.figure(figsize=(8, 6))  # 図のサイズを設定
sns.heatmap(cm,                              # 混同行列データ
           annot=True,                       # 数値を表示
           fmt='d',                          # 整数形式で表示
           cmap='Blues',                     # 青色グラデーション
           xticklabels=iris.target_names,    # X軸ラベル（予測クラス）
           yticklabels=iris.target_names)    # Y軸ラベル（実際のクラス）

plt.title(f'混同行列 - {best_model_name}', fontsize=16, fontweight='bold')
plt.ylabel('実際のクラス', fontsize=12)
plt.xlabel('予測クラス', fontsize=12)
plt.tight_layout()
plt.show()

# 詳細な分類レポートを表示
print("📋 詳細な分類レポート:")
report = classification_report(y_test, best_predictions, 
                             target_names=iris.target_names)
print(report)
# precision：精密度（予測した中での正解率）
# recall：再現率（実際の正解を予測できた割合）
# f1-score：精密度と再現率の調和平均
# support：各クラスのテストデータ数

# 各クラスごとの精度を計算
print("\n📊 各品種の予測精度:")
for i, species in enumerate(iris.target_names):
    # そのクラスのテストデータのインデックスを取得
    class_indices = (y_test == i)
    # そのクラスの予測結果を取得
    class_predictions = best_predictions[class_indices]
    # そのクラスの正解数を計算
    class_accuracy = (class_predictions == i).sum() / len(class_predictions)
    
    print(f"  {species}: {class_accuracy:.4f} ({class_accuracy*100:.1f}%)")

# === セル8: 特徴量重要度の分析（ランダムフォレストの場合） ===

# ランダムフォレストが最高性能の場合、特徴量重要度を分析
if best_model_name == 'ランダムフォレスト':
    print(f"\n🔝 {best_model_name} の特徴量重要度分析")
    print("-" * 40)
    
    # 特徴量重要度を取得
    feature_importance = best_model.feature_importances_
    # feature_importances_：各特徴量がモデルにどれだけ貢献したかの指標
    
    # 特徴量名と重要度をDataFrameに整理
    importance_df = pd.DataFrame({
        'feature': iris.feature_names,      # 特徴量名
        'importance': feature_importance    # 重要度スコア
    }).sort_values('importance', ascending=False)  # 重要度降順でソート
    
    print("特徴量重要度ランキング:")
    for i, row in importance_df.iterrows():
        print(f"{i+1}. {row['feature']}: {row['importance']:.4f}")
    
    # 特徴量重要度を棒グラフで可視化
    plt.figure(figsize=(10, 6))
    sns.barplot(data=importance_df,          # データ
               x='importance',               # X軸：重要度
               y='feature',                  # Y軸：特徴量名
               palette='viridis')            # カラーパレット
    
    plt.title(f'{best_model_name} 特徴量重要度', fontsize=16, fontweight='bold')
    plt.xlabel('重要度', fontsize=12)
    plt.ylabel('特徴量', fontsize=12)
    plt.tight_layout()
    plt.show()

# === セル9: 新しいデータでの予測例 ===

print("\n🎯 新しいアイリスデータでの予測例")
print("-" * 40)

# 新しいアイリスのサンプルデータを作成（例：実際の測定値）
new_samples = np.array([
    [5.1, 3.5, 1.4, 0.2],  # setosa っぽい特徴
    [6.0, 2.8, 4.5, 1.5],  # versicolor っぽい特徴  
    [7.2, 3.2, 6.0, 2.0]   # virginica っぽい特徴
])

# 新しいサンプルの特徴量名を表示
feature_names = iris.feature_names
print("新しいサンプルの特徴量:")
for i, sample in enumerate(new_samples):
    print(f"\nサンプル {i+1}:")
    for j, (feature, value) in enumerate(zip(feature_names, sample)):
        print(f"  {feature}: {value}")

# 最高性能モデルで予測
if best_model_name in ['ロジスティック回帰', 'SVM']:
    # 線形手法の場合は正規化が必要
    new_samples_scaled = scaler.transform(new_samples)
    new_predictions = best_model.predict(new_samples_scaled)
    new_probabilities = best_model.predict_proba(new_samples_scaled)
else:
    # 木系手法の場合は元データのまま
    new_predictions = best_model.predict(new_samples)
    # predict_proba()が利用可能かチェック
    if hasattr(best_model, 'predict_proba'):
        new_probabilities = best_model.predict_proba(new_samples)
    else:
        new_probabilities = None

# 予測結果を表示
print(f"\n🔮 {best_model_name} による予測結果:")
for i, (sample, prediction) in enumerate(zip(new_samples, new_predictions)):
    predicted_species = iris.target_names[prediction]  # 数値を品種名に変換
    print(f"\nサンプル {i+1}: {predicted_species}")
    
    # 確率が取得できる場合は表示
    if new_probabilities is not None:
        probs = new_probabilities[i]  # そのサンプルの各クラス確率
        print("  各品種の予測確率:")
        for j, (species, prob) in enumerate(zip(iris.target_names, probs)):
            print(f"    {species}: {prob:.4f} ({prob*100:.1f}%)")

# === セル10: 学習結果のまとめと次のステップ ===

print("\n🎉 アイリス分類プロジェクト完了！")
print("=" * 50)

print("\n📊 プロジェクト結果まとめ:")
print(f"✅ 使用データ: {len(iris.data)}サンプル、{len(iris.feature_names)}特徴量")
print(f"✅ 訓練データ: {len(X_train)}サンプル")
print(f"✅ テストデータ: {len(X_test)}サンプル")
print(f"✅ 試行モデル数: {len(models)}種類")
print(f"✅ 最高性能: {best_model_name} ({best_accuracy:.4f})")

print("\n🎯 今回学んだ技術:")
skills = [
    "1. scikit-learn でのデータ読み込み",
    "2. Pandas を使ったデータ操作", 
    "3. matplotlib/seaborn での可視化",
    "4. データの前処理（分割・正規化）",
    "5. 複数の機械学習アルゴリズム比較",
    "6. モデル評価（精度・混同行列・分類レポート）",
    "7. 特徴量重要度分析",
    "8. 新しいデータでの予測"
]

for skill in skills:
    print(f"  {skill}")

print("\n🚀 次のステップ（推奨）:")
next_steps = [
    "1. 他のアルゴリズム（XGBoost、ニューラルネットワーク）を試す",
    "2. ハイパーパラメータ調整で性能向上を図る",
    "3. 交差検証でより信頼性の高い評価を実施",
    "4. 他のデータセット（タイタニック、住宅価格）に挑戦",
    "5. Kaggle競技に参加してスキルアップ"
]

for step in next_steps:
    print(f"  {step}")

print("\n💡 重要なポイント:")
print("- 機械学習は「データの前処理」が最も重要")
print("- 複数のアルゴリズムを試して比較することが大切")
print("- 精度だけでなく、なぜそうなるかを理解する")
print("- 可視化でデータの特徴を把握する")
print("- 実際のデータで予測して実用性を確認する")

print("\n🌟 おめでとうございます！")
print("あなたは今、機械学習の基本的なパイプラインを")
print("一通り実装できるエンジニアです！")

# 最後に全体の処理時間を表示（オプション）
import time
print(f"\n⏱️  処理完了: {time.strftime('%Y-%m-%d %H:%M:%S')}")

# ========================================
# 補足：さらなる学習のためのコード例
# ========================================

print("\n📚 発展学習用のコード例:")

# 1. 交差検証での性能評価
from sklearn.model_selection import cross_val_score

print("\n🔄 交差検証での評価:")
cv_scores = cross_val_score(best_model, X, y, cv=5)  # 5分割交差検証
print(f"CV平均スコア: {cv_scores.mean():.4f}")
print(f"CV標準偏差: {cv_scores.std():.4f}")

# 2. ハイパーパラメータ調整の例
from sklearn.model_selection import GridSearchCV

if best_model_name == 'ランダムフォレスト':
    print("\n⚙️ ハイパーパラメータ調整例:")
    param_grid = {
        'n_estimators': [50, 100, 200],      # 決定木の数
        'max_depth': [3, 5, 7, None]         # 最大深度
    }
    
    grid_search = GridSearchCV(RandomForestClassifier(random_state=42), 
                              param_grid, cv=3, scoring='accuracy')
    grid_search.fit(X_train, y_train)
    
    print(f"最適パラメータ: {grid_search.best_params_}")
    print(f"最高スコア: {grid_search.best_score_:.4f}")

print("\n🎓 学習完了！次のプロジェクトに挑戦しましょう！")
