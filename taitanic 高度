# ========================================
# ã‚¿ã‚¤ã‚¿ãƒ‹ãƒƒã‚¯ç«¶æŠ€ ç²¾åº¦å‘ä¸Šã®ãŸã‚ã®é«˜åº¦ãªåˆ†ææ‰‹æ³•
# Kaggleä¸Šä½è€…ã®æ‰‹æ³•ã‚’å«ã‚€åŒ…æ‹¬çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
# ========================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# é«˜åº¦ãªæ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import xgboost as xgb
import lightgbm as lgb
from scipy import stats
import re

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
train_df = pd.read_csv('/kaggle/input/titanic/train.csv')
test_df = pd.read_csv('/kaggle/input/titanic/test.csv')

print("ğŸš¢ é«˜åº¦ãªã‚¿ã‚¤ã‚¿ãƒ‹ãƒƒã‚¯åˆ†æé–‹å§‹ï¼")
print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {train_df.shape}, ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {test_df.shape}")

# ========================================
# 1. æ·±åº¦ã®ã‚ã‚‹æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆAdvanced EDAï¼‰
# ========================================

def advanced_eda(df):
    """
    é«˜åº¦ãªæ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æ
    - çµ±è¨ˆçš„ãªé–¢ä¿‚æ€§ã®ç™ºè¦‹
    - éš ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã®æŠ½å‡º
    - å¤–ã‚Œå€¤ã®è©³ç´°åˆ†æ
    """
    print("\nğŸ” === é«˜åº¦ãªEDAé–‹å§‹ ===")
    
    # 1. ç”Ÿå­˜ç‡ã®è©³ç´°åˆ†æ
    print("\nğŸ“Š è©³ç´°ãªç”Ÿå­˜ç‡åˆ†æ:")
    
    # è¤‡æ•°æ¡ä»¶ã§ã®ç”Ÿå­˜ç‡
    survival_analysis = df.groupby(['Sex', 'Pclass'])['Survived'].agg(['count', 'mean']).round(3)
    print("\næ€§åˆ¥ Ã— ã‚¯ãƒ©ã‚¹åˆ¥ç”Ÿå­˜ç‡:")
    print(survival_analysis)
    
    # å¹´é½¢ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥ã®è©³ç´°åˆ†æ
    df['AgeGroup'] = pd.cut(df['Age'], bins=[0, 12, 18, 35, 50, 80], 
                           labels=['Child', 'Teen', 'Young_Adult', 'Middle_Age', 'Senior'])
    
    age_survival = df.groupby(['AgeGroup', 'Sex'])['Survived'].mean().unstack()
    print("\nå¹´é½¢ã‚°ãƒ«ãƒ¼ãƒ— Ã— æ€§åˆ¥ã®ç”Ÿå­˜ç‡:")
    print(age_survival.round(3))
    
    # 2. çµ±è¨ˆçš„æœ‰æ„æ€§ã®æ¤œå®š
    from scipy.stats import chi2_contingency
    
    # æ€§åˆ¥ã¨ç”Ÿå­˜ã®é–¢ä¿‚ï¼ˆã‚«ã‚¤äºŒä¹—æ¤œå®šï¼‰
    contingency_sex = pd.crosstab(df['Sex'], df['Survived'])
    chi2_sex, p_value_sex = chi2_contingency(contingency_sex)[:2]
    print(f"\næ€§åˆ¥ã¨ç”Ÿå­˜ã®é–¢ä¿‚ (på€¤: {p_value_sex:.2e})")
    
    # ã‚¯ãƒ©ã‚¹ã¨ç”Ÿå­˜ã®é–¢ä¿‚
    contingency_class = pd.crosstab(df['Pclass'], df['Survived'])
    chi2_class, p_value_class = chi2_contingency(contingency_class)[:2]
    print(f"ã‚¯ãƒ©ã‚¹ã¨ç”Ÿå­˜ã®é–¢ä¿‚ (på€¤: {p_value_class:.2e})")
    
    # 3. ç›¸é–¢é–¢ä¿‚ã®è©³ç´°åˆ†æ
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    correlation_matrix = df[numeric_cols].corr()
    
    plt.figure(figsize=(12, 8))
    sns.heatmap(correlation_matrix, annot=True, cmap='RdBu_r', center=0,
                square=True, fmt='.2f')
    plt.title('ç‰¹å¾´é‡é–“ã®ç›¸é–¢é–¢ä¿‚', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.show()
    
    # 4. å¤–ã‚Œå€¤ã®æ¤œå‡ºã¨åˆ†æ
    print("\nğŸ¯ å¤–ã‚Œå€¤åˆ†æ:")
    
    # é‹è³ƒã®å¤–ã‚Œå€¤
    Q1_fare = df['Fare'].quantile(0.25)
    Q3_fare = df['Fare'].quantile(0.75)
    IQR_fare = Q3_fare - Q1_fare
    fare_outliers = df[(df['Fare'] < Q1_fare - 1.5*IQR_fare) | 
                      (df['Fare'] > Q3_fare + 1.5*IQR_fare)]
    
    print(f"é‹è³ƒã®å¤–ã‚Œå€¤: {len(fare_outliers)}ä»¶")
    if len(fare_outliers) > 0:
        print("å¤–ã‚Œå€¤ã®ç‰¹å¾´:")
        print(fare_outliers[['Name', 'Pclass', 'Fare', 'Survived']].head())
    
    # å¹´é½¢ã®å¤–ã‚Œå€¤
    age_outliers = df[df['Age'] > df['Age'].quantile(0.95)]
    print(f"\né«˜é½¢è€…ï¼ˆ95%ileä»¥ä¸Šï¼‰ã®ç”Ÿå­˜ç‡: {age_outliers['Survived'].mean():.3f}")
    
    return df

# é«˜åº¦ãªEDAå®Ÿè¡Œ
train_df = advanced_eda(train_df)

# ========================================
# 2. é«˜åº¦ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
# ========================================

def advanced_feature_engineering(train_df, test_df):
    """
    Kaggleä¸Šä½è€…ãƒ¬ãƒ™ãƒ«ã®ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
    """
    print("\nğŸ”§ === é«˜åº¦ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° ===")
    
    # ãƒ‡ãƒ¼ã‚¿çµåˆ
    all_data = pd.concat([train_df, test_df], ignore_index=True)
    
    # === 1. åå‰ã‹ã‚‰ã®é«˜åº¦ãªæƒ…å ±æŠ½å‡º ===
    
    # æ•¬ç§°ã®è©³ç´°åˆ†é¡
    all_data['Title'] = all_data['Name'].str.extract(' ([A-Za-z]+)\.', expand=False)
    
    # ã‚ˆã‚Šè©³ç´°ãªæ•¬ç§°ãƒãƒƒãƒ”ãƒ³ã‚°
    title_mapping = {
        'Mr': 'Mr',
        'Miss': 'Miss', 
        'Mrs': 'Mrs',
        'Master': 'Master',
        'Dr': 'Officer',
        'Rev': 'Officer',
        'Col': 'Officer',
        'Major': 'Officer',
        'Mlle': 'Miss',
        'Countess': 'Royalty',
        'Ms': 'Miss',
        'Lady': 'Royalty',
        'Jonkheer': 'Royalty',
        'Don': 'Royalty',
        'Dona': 'Royalty',
        'Mme': 'Mrs',
        'Capt': 'Officer',
        'Sir': 'Royalty'
    }
    all_data['Title'] = all_data['Title'].map(title_mapping)
    all_data['Title'].fillna('Other', inplace=True)
    
    # å§“ï¼ˆè‹—å­—ï¼‰ã®æŠ½å‡º
    all_data['Surname'] = all_data['Name'].str.split(',').str[0]
    
    # åŒã˜å§“ã‚’æŒã¤å®¶æ—ã®ã‚µã‚¤ã‚º
    surname_counts = all_data['Surname'].value_counts()
    all_data['SurnameCount'] = all_data['Surname'].map(surname_counts)
    
    # === 2. å®¶æ—æ§‹æˆã®é«˜åº¦ãªåˆ†æ ===
    
    # å®¶æ—ã‚µã‚¤ã‚º
    all_data['FamilySize'] = all_data['SibSp'] + all_data['Parch'] + 1
    
    # å®¶æ—ã‚µã‚¤ã‚ºã®ã‚«ãƒ†ã‚´ãƒªåŒ–
    all_data['FamilySizeGroup'] = pd.cut(all_data['FamilySize'], 
                                        bins=[0, 1, 4, 7, 11], 
                                        labels=['Solo', 'Small', 'Medium', 'Large'])
    
    # å­ä¾›ã‹ã©ã†ã‹ï¼ˆã‚ˆã‚Šç²¾å¯†ãªå®šç¾©ï¼‰
    all_data['IsChild'] = ((all_data['Age'] < 16) & (all_data['Parch'] > 0)).astype(int)
    
    # æ¯è¦ªã‹ã©ã†ã‹ã®æ¨å®š
    all_data['IsMother'] = ((all_data['Sex'] == 'female') & 
                           (all_data['Parch'] > 0) & 
                           (all_data['Age'] > 18) & 
                           (all_data['Title'] != 'Miss')).astype(int)
    
    # === 3. ãƒã‚±ãƒƒãƒˆæƒ…å ±ã®æ´»ç”¨ ===
    
    # ãƒã‚±ãƒƒãƒˆç•ªå·ã®æ•°å­—éƒ¨åˆ†ã‚’æŠ½å‡º
    all_data['TicketNumber'] = all_data['Ticket'].str.extract('(\d+)').astype(float)
    
    # ãƒã‚±ãƒƒãƒˆæ¥é ­è¾ã®æŠ½å‡º
    all_data['TicketPrefix'] = all_data['Ticket'].str.extract('([A-Za-z]+)')
    all_data['TicketPrefix'].fillna('None', inplace=True)
    
    # åŒã˜ãƒã‚±ãƒƒãƒˆã‚’æŒã¤äººæ•°ï¼ˆã‚°ãƒ«ãƒ¼ãƒ—ãƒã‚±ãƒƒãƒˆï¼‰
    ticket_counts = all_data['Ticket'].value_counts()
    all_data['TicketGroup'] = all_data['Ticket'].map(ticket_counts)
    
    # === 4. å®¢å®¤æƒ…å ±ã®æ´»ç”¨ ===
    
    # ãƒ‡ãƒƒã‚­ï¼ˆå®¢å®¤ã®æœ€åˆã®æ–‡å­—ï¼‰
    all_data['Deck'] = all_data['Cabin'].str[0]
    all_data['Deck'].fillna('Unknown', inplace=True)
    
    # å®¢å®¤ç•ªå·
    all_data['CabinNumber'] = all_data['Cabin'].str.extract('(\d+)').astype(float)
    
    # è¤‡æ•°å®¢å®¤ã‚’æŒã¤ã‹ã©ã†ã‹
    all_data['MultipleCabins'] = all_data['Cabin'].str.count(' ').fillna(0)
    all_data['HasMultipleCabins'] = (all_data['MultipleCabins'] > 0).astype(int)
    
    # === 5. é‹è³ƒã®é«˜åº¦ãªåˆ†æ ===
    
    # ä¸€äººå½“ãŸã‚Šã®é‹è³ƒ
    all_data['FarePerPerson'] = all_data['Fare'] / all_data['TicketGroup']
    
    # ã‚¯ãƒ©ã‚¹åˆ¥é‹è³ƒã®æ¨™æº–åŒ–
    for pclass in [1, 2, 3]:
        class_fare_median = all_data[all_data['Pclass'] == pclass]['Fare'].median()
        all_data.loc[all_data['Pclass'] == pclass, 'FareNormalized'] = (
            all_data.loc[all_data['Pclass'] == pclass, 'Fare'] / class_fare_median
        )
    
    # === 6. å¹´é½¢ã®é«˜åº¦ãªå‡¦ç† ===
    
    # å¹´é½¢ã®æ¬ æå€¤ã‚’ã€ã‚ˆã‚Šç²¾å¯†ãªæ–¹æ³•ã§è£œå®Œ
    # æ•¬ç§°ã€ã‚¯ãƒ©ã‚¹ã€å®¶æ—æ§‹æˆã‚’è€ƒæ…®ã—ãŸè£œå®Œ
    def fill_age_advanced(row):
        if pd.isna(row['Age']):
            # åŒã˜æ•¬ç§°ã€ã‚¯ãƒ©ã‚¹ã€å®¶æ—ã‚µã‚¤ã‚ºã®ã‚°ãƒ«ãƒ¼ãƒ—ã®ä¸­å¤®å€¤
            similar_passengers = all_data[
                (all_data['Title'] == row['Title']) & 
                (all_data['Pclass'] == row['Pclass']) &
                (all_data['FamilySize'] == row['FamilySize'])
            ]['Age']
            
            if len(similar_passengers.dropna()) > 0:
                return similar_passengers.median()
            else:
                # ã‚ˆã‚Šåºƒã„æ¡ä»¶ã§è£œå®Œ
                return all_data[all_data['Title'] == row['Title']]['Age'].median()
        return row['Age']
    
    all_data['Age'] = all_data.apply(fill_age_advanced, axis=1)
    
    # å¹´é½¢ã‚°ãƒ«ãƒ¼ãƒ—ã®è©³ç´°åŒ–
    all_data['AgeGroup'] = pd.cut(all_data['Age'], 
                                 bins=[0, 5, 12, 18, 25, 35, 50, 65, 100],
                                 labels=['Baby', 'Child', 'Teen', 'Young', 'Adult', 'Middle', 'Senior', 'Elderly'])
    
    # === 7. ä¹—èˆ¹æ¸¯ã®é«˜åº¦ãªæ´»ç”¨ ===
    
    # ä¹—èˆ¹æ¸¯ã®æ¬ æå€¤è£œå®Œï¼ˆã‚¯ãƒ©ã‚¹ã¨é‹è³ƒã‚’è€ƒæ…®ï¼‰
    all_data['Embarked'].fillna(all_data['Embarked'].mode()[0], inplace=True)
    
    # ä¹—èˆ¹æ¸¯åˆ¥ã®å¹³å‡é‹è³ƒ
    embarked_fare_mean = all_data.groupby('Embarked')['Fare'].mean()
    all_data['EmbarkedFareDiff'] = all_data.apply(
        lambda x: x['Fare'] - embarked_fare_mean[x['Embarked']], axis=1
    )
    
    # === 8. æ–°ã—ã„çµ„ã¿åˆã‚ã›ç‰¹å¾´é‡ ===
    
    # æ€§åˆ¥ã¨ã‚¯ãƒ©ã‚¹ã®çµ„ã¿åˆã‚ã›
    all_data['Sex_Pclass'] = all_data['Sex'].astype(str) + '_' + all_data['Pclass'].astype(str)
    
    # å¹´é½¢ã‚°ãƒ«ãƒ¼ãƒ—ã¨æ€§åˆ¥ã®çµ„ã¿åˆã‚ã›
    all_data['Age_Sex'] = all_data['AgeGroup'].astype(str) + '_' + all_data['Sex']
    
    # æ•¬ç§°ã¨ã‚¯ãƒ©ã‚¹ã®çµ„ã¿åˆã‚ã›
    all_data['Title_Pclass'] = all_data['Title'].astype(str) + '_' + all_data['Pclass'].astype(str)
    
    # === 9. çµ±è¨ˆçš„ç‰¹å¾´é‡ ===
    
    # å„ã‚°ãƒ«ãƒ¼ãƒ—å†…ã§ã®ç›¸å¯¾çš„ä½ç½®
    # åŒã˜ã‚¯ãƒ©ã‚¹å†…ã§ã®å¹´é½¢ã®ç›¸å¯¾ä½ç½®
    all_data['Age_Pclass_Rank'] = all_data.groupby('Pclass')['Age'].rank(pct=True)
    
    # åŒã˜ã‚¯ãƒ©ã‚¹å†…ã§ã®é‹è³ƒã®ç›¸å¯¾ä½ç½®
    all_data['Fare_Pclass_Rank'] = all_data.groupby('Pclass')['Fare'].rank(pct=True)
    
    print(f"âœ… ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Œäº†ï¼")
    print(f"ğŸ“Š å…ƒã®ç‰¹å¾´é‡æ•°: {train_df.shape[1]}")
    print(f"ğŸ“Š æ–°ã—ã„ç‰¹å¾´é‡æ•°: {all_data.shape[1]}")
    
    return all_data

# é«˜åº¦ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Ÿè¡Œ
all_data = advanced_feature_engineering(train_df, test_df)

# ========================================
# 3. é«˜åº¦ãªãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
# ========================================

def advanced_preprocessing(all_data, train_size):
    """
    æ©Ÿæ¢°å­¦ç¿’å‘ã‘ã®é«˜åº¦ãªå‰å‡¦ç†
    """
    print("\nâš™ï¸ === é«˜åº¦ãªå‰å‡¦ç† ===")
    
    # æ®‹ã‚Šã®æ¬ æå€¤å‡¦ç†
    all_data['Fare'].fillna(all_data['Fare'].median(), inplace=True)
    all_data['FarePerPerson'].fillna(all_data['FarePerPerson'].median(), inplace=True)
    
    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    categorical_features = ['Sex', 'Embarked', 'Title', 'FamilySizeGroup', 
                           'AgeGroup', 'Deck', 'TicketPrefix', 'Sex_Pclass', 
                           'Age_Sex', 'Title_Pclass']
    
    # Label Encodingã¨ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®ä½¿ã„åˆ†ã‘
    le_features = ['Sex']  # é †åºæ€§ãŒã‚ã‚‹ã‚‚ã®
    ohe_features = [f for f in categorical_features if f not in le_features]  # é †åºæ€§ãŒãªã„ã‚‚ã®
    
    # Label Encoding
    from sklearn.preprocessing import LabelEncoder
    for feature in le_features:
        if feature in all_data.columns:
            le = LabelEncoder()
            all_data[feature] = le.fit_transform(all_data[feature].astype(str))
    
    # One-Hot Encoding
    for feature in ohe_features:
        if feature in all_data.columns:
            # ã‚«ãƒ†ã‚´ãƒªæ•°ãŒå¤šã™ãã‚‹å ´åˆã¯ä¸Šä½ã®ã¿
            value_counts = all_data[feature].value_counts()
            if len(value_counts) > 10:  # 10ã‚«ãƒ†ã‚´ãƒªä»¥ä¸Šã®å ´åˆ
                top_categories = value_counts.head(10).index
                all_data[feature] = all_data[feature].apply(
                    lambda x: x if x in top_categories else 'Other'
                )
            
            dummies = pd.get_dummies(all_data[feature], prefix=feature)
            all_data = pd.concat([all_data, dummies], axis=1)
    
    # ä¸è¦ãªåˆ—ã‚’å‰Šé™¤
    drop_columns = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'Surname'] + ohe_features
    features_to_drop = [col for col in drop_columns if col in all_data.columns]
    all_data = all_data.drop(features_to_drop, axis=1)
    
    # è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²
    train_processed = all_data[:train_size]
    test_processed = all_data[train_size:]
    
    # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«åˆ†é›¢
    X_train = train_processed.drop('Survived', axis=1)
    y_train = train_processed['Survived']
    X_test = test_processed.drop('Survived', axis=1)
    
    print(f"ğŸ“Š æœ€çµ‚ç‰¹å¾´é‡æ•°: {X_train.shape[1]}")
    print(f"ğŸ“‹ ç‰¹å¾´é‡ä¸€è¦§:")
    feature_list = X_train.columns.tolist()
    for i, feature in enumerate(feature_list):
        if i % 5 == 0:
            print()
        print(f"{feature:20s}", end=" ")
    print("\n")
    
    return X_train, y_train, X_test

# å‰å‡¦ç†å®Ÿè¡Œ
X_train, y_train, X_test = advanced_preprocessing(all_data, len(train_df))

# ========================================
# 4. ç‰¹å¾´é‡é¸æŠ
# ========================================

def feature_selection(X_train, y_train):
    """
    çµ±è¨ˆçš„æ‰‹æ³•ã«ã‚ˆã‚‹ç‰¹å¾´é‡é¸æŠ
    """
    print("\nğŸ¯ === ç‰¹å¾´é‡é¸æŠ ===")
    
    # 1. å˜å¤‰é‡çµ±è¨ˆæ¤œå®šã«ã‚ˆã‚‹é¸æŠ
    from sklearn.feature_selection import SelectKBest, f_classif, chi2
    from sklearn.feature_selection import mutual_info_classif
    
    # Få€¤ã«ã‚ˆã‚‹é¸æŠ
    selector_f = SelectKBest(score_func=f_classif, k=30)
    X_train_f = selector_f.fit_transform(X_train, y_train)
    selected_features_f = X_train.columns[selector_f.get_support()].tolist()
    
    # ç›¸äº’æƒ…å ±é‡ã«ã‚ˆã‚‹é¸æŠ
    mi_scores = mutual_info_classif(X_train, y_train)
    feature_mi = pd.DataFrame({
        'feature': X_train.columns,
        'mutual_info': mi_scores
    }).sort_values('mutual_info', ascending=False)
    
    print("ğŸ” ç›¸äº’æƒ…å ±é‡ã«ã‚ˆã‚‹ç‰¹å¾´é‡é‡è¦åº¦ TOP 15:")
    print(feature_mi.head(15))
    
    # 2. ç›¸é–¢ãŒé«˜ã„ç‰¹å¾´é‡ã®é™¤å»
    correlation_matrix = X_train.corr().abs()
    high_corr_pairs = []
    
    for i in range(len(correlation_matrix.columns)):
        for j in range(i+1, len(correlation_matrix.columns)):
            if correlation_matrix.iloc[i, j] > 0.9:
                high_corr_pairs.append((
                    correlation_matrix.columns[i], 
                    correlation_matrix.columns[j],
                    correlation_matrix.iloc[i, j]
                ))
    
    if high_corr_pairs:
        print(f"\nâš ï¸ é«˜ç›¸é–¢ãƒšã‚¢ï¼ˆ>0.9ï¼‰: {len(high_corr_pairs)}çµ„")
        for pair in high_corr_pairs[:5]:  # ä¸Šä½5çµ„è¡¨ç¤º
            print(f"  {pair[0]} - {pair[1]}: {pair[2]:.3f}")
    
    return selected_features_f, feature_mi

selected_features, feature_importance_mi = feature_selection(X_train, y_train)

# ========================================
# 5. é«˜åº¦ãªãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
# ========================================

def advanced_model_building(X_train, y_train):
    """
    è¤‡æ•°ã®é«˜æ€§èƒ½ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ¯”è¼ƒã¨æœ€é©åŒ–
    """
    print("\nğŸ¤– === é«˜åº¦ãªãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ ===")
    
    # äº¤å·®æ¤œè¨¼ã®è¨­å®š
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    # 1. åŸºæœ¬ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©
    models = {
        'RandomForest': RandomForestClassifier(
            n_estimators=200,
            max_depth=8,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42
        ),
        'GradientBoosting': GradientBoostingClassifier(
            n_estimators=150,
            learning_rate=0.1,
            max_depth=6,
            random_state=42
        ),
        'XGBoost': xgb.XGBClassifier(
            n_estimators=200,
            max_depth=6,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42
        ),
        'LightGBM': lgb.LGBMClassifier(
            n_estimators=200,
            max_depth=6,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            verbose=-1
        ),
        'LogisticRegression': LogisticRegression(
            C=1.0,
            random_state=42,
            max_iter=1000
        ),
        'SVM': SVC(
            C=1.0,
            kernel='rbf',
            probability=True,
            random_state=42
        )
    }
    
    # 2. ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
    model_scores = {}
    print("\nğŸ“Š ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒ:")
    print("-" * 60)
    
    for name, model in models.items():
        scores = cross_val_score(model, X_train, y_train, cv=skf, scoring='accuracy')
        model_scores[name] = {
            'mean': scores.mean(),
            'std': scores.std(),
            'scores': scores
        }
        print(f"{name:20s}: {scores.mean():.4f} (+/- {scores.std()*2:.4f})")
    
    # 3. æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å®š
    best_model_name = max(model_scores.keys(), key=lambda x: model_scores[x]['mean'])
    best_model = models[best_model_name]
    
    print(f"\nğŸ† æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«: {best_model_name} ({model_scores[best_model_name]['mean']:.4f})")
    
    return models, model_scores, best_model_name, best_model

models, model_scores, best_model_name, best_model = advanced_model_building(X_train, y_train)

# ========================================
# 6. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æœ€é©åŒ–
# ========================================

def hyperparameter_optimization(best_model, best_model_name, X_train, y_train):
    """
    ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã«ã‚ˆã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
    """
    print(f"\nğŸ”§ === {best_model_name} ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ– ===")
    
    # ãƒ¢ãƒ‡ãƒ«åˆ¥ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒªãƒƒãƒ‰
    param_grids = {
        'RandomForest': {
            'n_estimators': [100, 200, 300],
            'max_depth': [6, 8, 10],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4]
        },
        'XGBoost': {
            'n_estimators': [100, 200, 300],
            'max_depth': [3, 6, 9],
            'learning_rate': [0.05, 0.1, 0.2],
            'subsample': [0.8, 0.9, 1.0]
        },
        'LightGBM': {
            'n_estimators': [100, 200, 300],
            'max_depth': [3, 6, 9],
            'learning_rate': [0.05, 0.1, 0.2],
            'subsample': [0.8, 0.9, 1.0]
        },
        'GradientBoosting': {
            'n_estimators': [100, 150, 200],
            'max_depth': [3, 6, 9],
            'learning_rate': [0.05, 0.1, 0.15]
        }
    }
    
    if best_model_name in param_grids:
        param_grid = param_grids[best_model_name]
        
        # ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒå®Ÿè¡Œ
        grid_search = GridSearchCV(
            best_model,
            param_grid,
            cv=5,
            scoring='accuracy',
            n_jobs=-1,
            verbose=1
        )
        
        grid_search.fit(X_train, y_train)
        
        print(f"\nâœ… æœ€é©åŒ–å®Œäº†!")
        print(f"ğŸ“ˆ æœ€é«˜ã‚¹ã‚³ã‚¢: {grid_search.best_score_:.4f}")
        print(f"ğŸ¯ æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
        for param, value in grid_search.best_params_.items():
            print(f"  {param}: {value}")
        
        return grid_search.best_estimator_
    else:
        print(f"âš ï¸ {best_model_name} ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒªãƒƒãƒ‰ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return best_model

# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–å®Ÿè¡Œ
optimized_model = hyperparameter_optimization(best_model, best_model_name, X_train, y_train)

# ========================================
# 7. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•
# ========================================

def create_ensemble_model(models, X_train, y_train):
    """
    è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã‚’çµ„ã¿åˆã‚ã›ãŸã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
    """
    print("\nğŸ­ === ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ ===")
    
    # ä¸Šä½3ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
    top_models = sorted(model_scores.items(), key=lambda x: x[1]['mean'], reverse=True)[:3]
    
    print("ğŸ” ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å¯¾è±¡ãƒ¢ãƒ‡ãƒ«:")
    ensemble_models = []
    for name, score_info in top_models:
        print(f"  {name}: {score_info['mean']:.4f}")
        ensemble_models.append((name, models[name]))
    
    # Voting Classifierï¼ˆå¤šæ•°æ±ºï¼‰
    voting_clf = VotingClassifier(
        estimators=ensemble_models,
        voting='soft'  # ç¢ºç‡ãƒ™ãƒ¼ã‚¹ã®æŠ•ç¥¨
    )
    
    # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®æ€§èƒ½è©•ä¾¡
    ensemble_scores = cross_val_score(voting_clf, X_train, y_train, cv=5, scoring='accuracy')
    
    print(f"\nğŸ“Š ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ€§èƒ½: {ensemble_scores.mean():.4f} (+/- {ensemble_scores.std()*2:.4f})")
    print(f"ğŸ“ˆ æœ€é«˜å˜ä½“ãƒ¢ãƒ‡ãƒ«: {top_models[0][1]['mean']:.4f}")
    print(f"ğŸ¯ æ”¹å–„åº¦: {ensemble_scores.mean() - top_models[0][1]['mean']:.4f}")
    
    return voting_clf, ensemble_scores

ensemble_model, ensemble_scores = create_ensemble_model(models, X_train, y_train)

# ========================================
# 8. ç‰¹å¾´é‡é‡è¦åº¦ã®è©³ç´°åˆ†æ
# ========================================

def analyze_feature_importance(model, X_train, feature_names):
    """
    ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´é‡é‡è¦åº¦ã‚’è©³ç´°åˆ†æ
    """
    print("\nğŸ“Š === ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ ===")
    
    if hasattr(model, 'feature_importances_'):
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print("ğŸ” é‡è¦ç‰¹å¾´é‡ TOP 20:")
        print(importance_df.head(20))
        
        # é‡è¦åº¦ã®å¯è¦–åŒ–
        plt.figure(figsize=(12, 8))
        top_features = importance_df.head(15)
        sns.barplot(data=top_features, x='importance', y='feature', palette='viridis')
        plt.title('ç‰¹å¾´é‡é‡è¦åº¦ TOP 15', fontsize=16, fontweight='bold')
        plt.xlabel('é‡è¦åº¦', fontsize=12)
        plt.tight_layout()
        plt.show()
        
        return importance_df
    else:
        print("âš ï¸ ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ç‰¹å¾´é‡é‡è¦åº¦ã‚’æä¾›ã—ã¾ã›ã‚“")
        return None

# æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ã§ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ
feature_importance_df = analyze_feature_importance(optimized_model, X_train, X_train.columns)

# ========================================
# 9. ã‚¨ãƒ©ãƒ¼åˆ†æ
# ========================================

def error_analysis(model, X_train, y_train):
    """
    äºˆæ¸¬ã‚¨ãƒ©ãƒ¼ã®è©³ç´°åˆ†æ
    """
    print("\nğŸ” === ã‚¨ãƒ©ãƒ¼åˆ†æ ===")
    
    # äº¤å·®æ¤œè¨¼ã§ã®äºˆæ¸¬
    from sklearn.model_selection import cross_val_predict
    y_pred = cross_val_predict(model, X_train, y_train, cv=5)
    
    # æ··åŒè¡Œåˆ—
    cm = confusion_matrix(y_train, y_pred)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['æ­»äº¡', 'ç”Ÿå­˜'], yticklabels=['æ­»äº¡', 'ç”Ÿå­˜'])
    plt.title('æ··åŒè¡Œåˆ—', fontsize=16, fontweight='bold')
    plt.ylabel('å®Ÿéš›ã®å€¤', fontsize=12)
    plt.xlabel('äºˆæ¸¬å€¤', fontsize=12)
    plt.tight_layout()
    plt.show()
    
    # ã‚¨ãƒ©ãƒ¼è©³ç´°
    errors = X_train[y_train != y_pred]
    error_actual = y_train[y_train != y_pred]
    error_pred = y_pred[y_train != y_pred]
    
    print(f"ğŸ“Š ã‚¨ãƒ©ãƒ¼åˆ†æçµæœ:")
    print(f"  ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(y_train)}")
    print(f"  ã‚¨ãƒ©ãƒ¼æ•°: {len(errors)} ({len(errors)/len(y_train)*100:.1f}%)")
    print(f"  å½é™½æ€§ï¼ˆç”Ÿå­˜äºˆæ¸¬â†’å®Ÿéš›ã¯æ­»äº¡ï¼‰: {sum((y_pred == 1) & (y_train == 0))}")
    print(f"  å½é™°æ€§ï¼ˆæ­»äº¡äºˆæ¸¬â†’å®Ÿéš›ã¯ç”Ÿå­˜ï¼‰: {sum((y_pred == 0) & (y_train == 1))}")
    
    # åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ
    print("\nğŸ“‹ åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
    print(classification_report(y_train, y_pred, target_names=['æ­»äº¡', 'ç”Ÿå­˜']))
    
    return errors, error_actual, error_pred

errors, error_actual, error_pred = error_analysis(optimized_model, X_train, y_train)

# ========================================
# 10. æœ€çµ‚äºˆæ¸¬ã¨æå‡º
# ========================================

def final_prediction_and_submission(model, X_test, test_df):
    """
    æœ€çµ‚äºˆæ¸¬ã®å®Ÿè¡Œã¨æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    """
    print("\nğŸ¯ === æœ€çµ‚äºˆæ¸¬ãƒ»æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ ===")
    
    # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã§å­¦ç¿’
    model.fit(X_train, y_train)
    
    # äºˆæ¸¬å®Ÿè¡Œ
    predictions = model.predict(X_test)
    prediction_proba = model.predict_proba(X_test)[:, 1]
    
    # äºˆæ¸¬çµæœã®åˆ†æ
    print(f"ğŸ“Š äºˆæ¸¬çµæœ:")
    print(f"  ç”Ÿå­˜äºˆæ¸¬: {sum(predictions)} äºº ({sum(predictions)/len(predictions)*100:.1f}%)")
    print(f"  æ­»äº¡äºˆæ¸¬: {len(predictions) - sum(predictions)} äºº ({(len(predictions) - sum(predictions))/len(predictions)*100:.1f}%)")
    print(f"  å¹³å‡ç¢ºä¿¡åº¦: {prediction_proba.mean():.3f}")
    print(f"  ç¢ºä¿¡åº¦ç¯„å›²: {prediction_proba.min():.3f} - {prediction_proba.max():.3f}")
    
    # ç¢ºä¿¡åº¦ã®åˆ†å¸ƒ
    plt.figure(figsize=(10, 6))
    plt.hist(prediction_proba, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
    plt.axvline(prediction_proba.mean(), color='red', linestyle='--', 
                label=f'å¹³å‡: {prediction_proba.mean():.3f}')
    plt.title('äºˆæ¸¬ç¢ºä¿¡åº¦ã®åˆ†å¸ƒ', fontsize=16, fontweight='bold')
    plt.xlabel('ç”Ÿå­˜ç¢ºç‡', fontsize=12)
    plt.ylabel('é »åº¦', fontsize=12)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
    
    # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    submission = pd.DataFrame({
        'PassengerId': test_df['PassengerId'],
        'Survived': predictions
    })
    
    # æ¤œè¨¼
    assert submission.shape == (418, 2), "æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢çŠ¶ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“"
    assert submission['Survived'].isin([0, 1]).all(), "äºˆæ¸¬å€¤ã¯0ã¾ãŸã¯1ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™"
    
    # ä¿å­˜
    submission.to_csv('advanced_titanic_submission.csv', index=False)
    
    print("\nâœ… æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†: advanced_titanic_submission.csv")
    print("\nğŸ“ˆ æœŸå¾…ã‚¹ã‚³ã‚¢ç¯„å›²: 0.80 - 0.84")
    
    return submission, predictions, prediction_proba

# ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã§æœ€çµ‚äºˆæ¸¬
submission, final_predictions, final_probabilities = final_prediction_and_submission(
    ensemble_model, X_test, test_df
)

# ========================================
# 11. æ”¹å–„ã®ãŸã‚ã®è¿½åŠ ã‚¢ã‚¤ãƒ‡ã‚¢
# ========================================

print("\nğŸš€ === ã•ã‚‰ãªã‚‹æ”¹å–„ã‚¢ã‚¤ãƒ‡ã‚¢ ===")

improvement_ideas = """
ğŸ¯ ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®è¿½åŠ ã‚¢ã‚¤ãƒ‡ã‚¢:
  â€¢ åå‰ã®é•·ã•ï¼ˆç¤¾ä¼šçš„åœ°ä½ã®æ¨å®šï¼‰
  â€¢ ãƒã‚±ãƒƒãƒˆä¾¡æ ¼ã®ä¹—èˆ¹æ¸¯åˆ¥åå·®
  â€¢ å¹´é½¢ã®æ•¬ç§°åˆ¥åå·®
  â€¢ å®¶æ—å†…ã®å¹´é½¢é †ä½

ğŸ¤– ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®æ”¹å–„:
  â€¢ CatBoost, Neural Networks ã®è¿½åŠ 
  â€¢ Stackingï¼ˆéšå±¤ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼‰
  â€¢ Pseudo Labelingï¼ˆåŠæ•™å¸«ã‚ã‚Šå­¦ç¿’ï¼‰
  â€¢ Feature Selection ã®è‡ªå‹•åŒ–

ğŸ“Š å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã®æ´»ç”¨:
  â€¢ æ­´å²çš„ãƒ‡ãƒ¼ã‚¿ï¼ˆå®Ÿéš›ã®ä¹—å®¢åç°¿ï¼‰
  â€¢ å®¢å®¤ã®è©³ç´°æƒ…å ±
  â€¢ å½“æ™‚ã®ç¤¾ä¼šæƒ…å‹¢ãƒ‡ãƒ¼ã‚¿

ğŸ”§ é«˜åº¦ãªå‰å‡¦ç†:
  â€¢ å¤–ã‚Œå€¤ã®é™¤å»ãƒ»å¤‰æ›
  â€¢ ç‰¹å¾´é‡ã®äº¤äº’ä½œç”¨
  â€¢ PCAã«ã‚ˆã‚‹æ¬¡å…ƒå‰Šæ¸›
  â€¢ Target Encoding

ğŸ“ˆ è©•ä¾¡ãƒ»æ¤œè¨¼ã®æ”¹å–„:
  â€¢ Out-of-Fold äºˆæ¸¬
  â€¢ Time Series Splitï¼ˆç–‘ä¼¼æ™‚ç³»åˆ—ï¼‰
  â€¢ Adversarial Validation
  â€¢ Bayesian Optimization
"""

print(improvement_ideas)

print("\nğŸ‰ é«˜åº¦ãªåˆ†æå®Œäº†ï¼Kaggleã§ã®ä¸Šä½å…¥è³ã‚’ç›®æŒ‡ã—ã¾ã—ã‚‡ã†ï¼")
