# Kaggle勝利のためのAI戦略ガイド

## 1. Kaggleの基本理解

### 1.1 Kaggleコンペティションの特徴
- **実世界のデータ**: ノイズやバイアスを含む
- **評価指標**: コンペ固有の評価指標
- **リーダーボード**: Public/Private split
- **時間制限**: 数週間から数ヶ月
- **チーム戦**: 最大4人まで

### 1.2 成功の鍵
```
1. データ理解 (EDA) - 30%
2. 特徴量エンジニアリング - 25%
3. モデリング - 20%
4. アンサンブル - 15%
5. バリデーション戦略 - 10%
```

## 2. 必須ライブラリとツール

### 2.1 基本的なライブラリ
```python
# データ操作・分析
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# 機械学習
from sklearn.model_selection import KFold, StratifiedKFold, train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler
from sklearn.metrics import accuracy_score, roc_auc_score, mean_squared_error
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor

# 高性能アルゴリズム
import lightgbm as lgb
import xgboost as xgb
import catboost as cb

# 深層学習
import tensorflow as tf
from tensorflow import keras
import torch
import torch.nn as nn

# 特徴量選択・生成
from sklearn.feature_selection import SelectKBest, RFE
from sklearn.decomposition import PCA
import optuna  # ハイパーパラメータ最適化

# その他
import warnings
warnings.filterwarnings('ignore')
```

### 2.2 Kaggle特化ライブラリ
```python
# Kaggle Environments
import kaggle
from kaggle.api.kaggle_api_extended import KaggleApi

# 高速化・効率化
import polars as pl  # 高速なDataFrame操作
import cupy as cp    # GPU加速
import rapids        # GPU版pandas/sklearn
```

## 3. データ理解とEDA（探索的データ分析）

### 3.1 基本的なデータ理解
```python
def basic_data_info(df):
    """基本的なデータ情報を表示"""
    print("=== データの基本情報 ===")
    print(f"Shape: {df.shape}")
    print(f"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
    
    print("\n=== データ型 ===")
    print(df.dtypes.value_counts())
    
    print("\n=== 欠損値 ===")
    missing = df.isnull().sum()
    print(missing[missing > 0].sort_values(ascending=False))
    
    print("\n=== 数値列の統計 ===")
    print(df.describe())
    
    print("\n=== カテゴリ列のユニーク数 ===")
    for col in df.select_dtypes(include=['object']):
        print(f"{col}: {df[col].nunique()} unique values")

# 使用例
basic_data_info(train_df)
```

### 3.2 高度なEDAテクニック
```python
def advanced_eda(df, target_col):
    """高度なEDA"""
    
    # 1. 相関分析
    plt.figure(figsize=(15, 12))
    correlation_matrix = df.corr()
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
    plt.title('Correlation Matrix')
    plt.show()
    
    # 2. ターゲット変数との関係
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if col != target_col:
            plt.figure(figsize=(12, 4))
            
            plt.subplot(1, 2, 1)
            plt.scatter(df[col], df[target_col], alpha=0.5)
            plt.xlabel(col)
            plt.ylabel(target_col)
            plt.title(f'{col} vs {target_col}')
            
            plt.subplot(1, 2, 2)
            df[col].hist(bins=50, alpha=0.7)
            plt.xlabel(col)
            plt.title(f'Distribution of {col}')
            
            plt.tight_layout()
            plt.show()
    
    # 3. 異常値検出
    Q1 = df[numeric_cols].quantile(0.25)
    Q3 = df[numeric_cols].quantile(0.75)
    IQR = Q3 - Q1
    outliers = ((df[numeric_cols] < (Q1 - 1.5 * IQR)) | 
                (df[numeric_cols] > (Q3 + 1.5 * IQR))).sum()
    print("Outliers per column:")
    print(outliers[outliers > 0].sort_values(ascending=False))

# 使用例
advanced_eda(train_df, 'target')
```

### 3.3 リーク検出
```python
def detect_leakage(train_df, test_df, target_col):
    """データリークの検出"""
    
    # 1. 統計的分布の比較
    numeric_cols = train_df.select_dtypes(include=[np.number]).columns
    
    for col in numeric_cols:
        if col != target_col:
            # KS検定
            from scipy.stats import ks_2samp
            statistic, p_value = ks_2samp(train_df[col].dropna(), 
                                         test_df[col].dropna())
            
            if p_value < 0.01:  # 有意水準1%
                print(f"Warning: {col} may have distribution shift (p={p_value:.6f})")
    
    # 2. 特徴量重要度による簡単な予測
    # trainとtestを区別できる特徴量 = リークの可能性
    combined_df = pd.concat([
        train_df.assign(is_train=1),
        test_df.assign(is_train=0)
    ]).reset_index(drop=True)
    
    # 簡単な分類モデルでtrain/testを予測
    features = [col for col in combined_df.columns 
                if col not in [target_col, 'is_train']]
    
    X = combined_df[features].fillna(-999)
    y = combined_df['is_train']
    
    model = lgb.LGBMClassifier(n_estimators=100, random_state=42)
    model.fit(X, y)
    
    # AUCが0.5に近いほど良い（train/testが区別できない）
    predictions = model.predict_proba(X)[:, 1]
    auc = roc_auc_score(y, predictions)
    print(f"Train/Test AUC: {auc:.4f}")
    
    if auc > 0.8:
        print("Warning: High train/test distinguishability - possible leakage!")
        
        # 重要な特徴量を表示
        importance_df = pd.DataFrame({
            'feature': features,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print("Top features distinguishing train/test:")
        print(importance_df.head(10))

# 使用例
detect_leakage(train_df, test_df, 'target')
```

## 4. 特徴量エンジニアリング

### 4.1 基本的な特徴量生成
```python
class FeatureEngineering:
    def __init__(self):
        self.scalers = {}
        self.encoders = {}
    
    def numeric_features(self, df, cols):
        """数値特徴量の生成"""
        new_df = df.copy()
        
        for col in cols:
            # 基本統計量
            new_df[f'{col}_squared'] = df[col] ** 2
            new_df[f'{col}_sqrt'] = np.sqrt(np.abs(df[col]))
            new_df[f'{col}_log'] = np.log1p(np.abs(df[col]))
            
            # ビニング
            new_df[f'{col}_bin_10'] = pd.cut(df[col], bins=10, labels=False)
            new_df[f'{col}_bin_5'] = pd.cut(df[col], bins=5, labels=False)
            
            # 異常値フラグ
            Q1, Q3 = df[col].quantile([0.25, 0.75])
            IQR = Q3 - Q1
            new_df[f'{col}_is_outlier'] = ((df[col] < Q1 - 1.5*IQR) | 
                                          (df[col] > Q3 + 1.5*IQR)).astype(int)
        
        return new_df
    
    def categorical_features(self, df, cols):
        """カテゴリ特徴量の処理"""
        new_df = df.copy()
        
        for col in cols:
            # 頻度エンコーディング
            freq_map = df[col].value_counts().to_dict()
            new_df[f'{col}_freq'] = df[col].map(freq_map)
            
            # ラベルエンコーディング
            if col not in self.encoders:
                self.encoders[col] = LabelEncoder()
                new_df[f'{col}_label'] = self.encoders[col].fit_transform(df[col].fillna('Unknown'))
            else:
                new_df[f'{col}_label'] = self.encoders[col].transform(df[col].fillna('Unknown'))
            
            # 高頻度カテゴリの統合
            top_categories = df[col].value_counts().head(10).index
            new_df[f'{col}_top10'] = df[col].apply(lambda x: x if x in top_categories else 'Other')
        
        return new_df
    
    def interaction_features(self, df, cols):
        """交互作用特徴量"""
        new_df = df.copy()
        
        # 数値×数値
        numeric_cols = [col for col in cols if df[col].dtype in ['int64', 'float64']]
        for i in range(len(numeric_cols)):
            for j in range(i+1, len(numeric_cols)):
                col1, col2 = numeric_cols[i], numeric_cols[j]
                new_df[f'{col1}_{col2}_mul'] = df[col1] * df[col2]
                new_df[f'{col1}_{col2}_div'] = df[col1] / (df[col2] + 1e-8)
                new_df[f'{col1}_{col2}_add'] = df[col1] + df[col2]
                new_df[f'{col1}_{col2}_sub'] = df[col1] - df[col2]
        
        return new_df

# 使用例
fe = FeatureEngineering()
train_enhanced = fe.numeric_features(train_df, numeric_columns)
train_enhanced = fe.categorical_features(train_enhanced, categorical_columns)
train_enhanced = fe.interaction_features(train_enhanced, important_columns)
```

### 4.2 高度な特徴量生成
```python
def advanced_feature_engineering(df, target_col=None):
    """高度な特徴量エンジニアリング"""
    
    # 1. 集約特徴量（グループ統計）
    categorical_cols = df.select_dtypes(include=['object']).columns
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    
    new_features = pd.DataFrame(index=df.index)
    
    for cat_col in categorical_cols[:3]:  # 計算時間を考慮して制限
        for num_col in numeric_cols[:5]:
            if num_col != target_col:
                # グループ統計量
                group_stats = df.groupby(cat_col)[num_col].agg([
                    'mean', 'std', 'min', 'max', 'median'
                ]).add_prefix(f'{cat_col}_{num_col}_')
                
                new_features = new_features.join(
                    df[cat_col].map(group_stats['mean']).rename(f'{cat_col}_{num_col}_mean'),
                    how='left'
                )
    
    # 2. 時系列特徴量（日付列がある場合）
    date_cols = df.select_dtypes(include=['datetime64']).columns
    for date_col in date_cols:
        new_features[f'{date_col}_year'] = df[date_col].dt.year
        new_features[f'{date_col}_month'] = df[date_col].dt.month
        new_features[f'{date_col}_day'] = df[date_col].dt.day
        new_features[f'{date_col}_dayofweek'] = df[date_col].dt.dayofweek
        new_features[f'{date_col}_quarter'] = df[date_col].dt.quarter
        new_features[f'{date_col}_is_weekend'] = (df[date_col].dt.dayofweek >= 5).astype(int)
    
    # 3. 次元削減特徴量
    from sklearn.decomposition import PCA
    from sklearn.manifold import TSNE
    
    # PCA
    numeric_data = df[numeric_cols].fillna(0)
    pca = PCA(n_components=min(10, len(numeric_cols)))
    pca_features = pca.fit_transform(numeric_data)
    
    for i in range(pca_features.shape[1]):
        new_features[f'pca_{i}'] = pca_features[:, i]
    
    return pd.concat([df, new_features], axis=1)

# 使用例
train_advanced = advanced_feature_engineering(train_df, 'target')
```

## 5. Kaggleで勝つためのモデリング戦略

### 5.1 ベースライン構築
```python
def create_baseline_models(X_train, y_train, X_val, y_val, task_type='classification'):
    """複数のベースラインモデルを構築"""
    
    models = {}
    scores = {}
    
    if task_type == 'classification':
        # LightGBM
        lgb_model = lgb.LGBMClassifier(n_estimators=1000, random_state=42)
        lgb_model.fit(X_train, y_train, 
                     eval_set=[(X_val, y_val)], 
                     callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)])
        
        models['lgb'] = lgb_model
        scores['lgb'] = roc_auc_score(y_val, lgb_model.predict_proba(X_val)[:, 1])
        
        # XGBoost
        xgb_model = xgb.XGBClassifier(n_estimators=1000, random_state=42)
        xgb_model.fit(X_train, y_train,
                     eval_set=[(X_val, y_val)],
                     early_stopping_rounds=100,
                     verbose=False)
        
        models['xgb'] = xgb_model
        scores['xgb'] = roc_auc_score(y_val, xgb_model.predict_proba(X_val)[:, 1])
        
        # CatBoost
        cb_model = cb.CatBoostClassifier(iterations=1000, random_state=42, verbose=False)
        cb_model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=100)
        
        models['catboost'] = cb_model
        scores['catboost'] = roc_auc_score(y_val, cb_model.predict_proba(X_val)[:, 1])
        
    else:  # regression
        # Similar setup for regression models
        pass
    
    # 結果表示
    for model_name, score in scores.items():
        print(f"{model_name}: {score:.4f}")
    
    return models, scores

# 使用例
models, scores = create_baseline_models(X_train, y_train, X_val, y_val)
```

### 5.2 ハイパーパラメータ最適化
```python
def optimize_lgb_params(X_train, y_train, X_val, y_val, task_type='classification'):
    """OptunaでLightGBMのハイパーパラメータ最適化"""
    
    def objective(trial):
        params = {
            'objective': 'binary' if task_type == 'classification' else 'regression',
            'metric': 'auc' if task_type == 'classification' else 'rmse',
            'boosting_type': 'gbdt',
            'num_leaves': trial.suggest_int('num_leaves', 10, 300),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
            'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),
            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),
            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),
            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
            'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),
            'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),
            'verbosity': -1,
            'random_state': 42
        }
        
        model = lgb.LGBMClassifier(**params, n_estimators=1000)
        model.fit(X_train, y_train, 
                 eval_set=[(X_val, y_val)],
                 callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)])
        
        if task_type == 'classification':
            preds = model.predict_proba(X_val)[:, 1]
            score = roc_auc_score(y_val, preds)
        else:
            preds = model.predict(X_val)
            score = -mean_squared_error(y_val, preds)  # 最大化のため負の値
        
        return score
    
    study = optuna.create_study(direction='maximize')
    study.optimize(objective, n_trials=100)
    
    print(f"Best score: {study.best_value:.4f}")
    print(f"Best params: {study.best_params}")
    
    return study.best_params

# 使用例
best_params = optimize_lgb_params(X_train, y_train, X_val, y_val)
```

### 5.3 アンサンブル手法
```python
class KaggleEnsemble:
    def __init__(self):
        self.models = {}
        self.weights = {}
    
    def add_model(self, name, model, weight=1.0):
        """モデルをアンサンブルに追加"""
        self.models[name] = model
        self.weights[name] = weight
    
    def simple_average(self, X):
        """単純平均アンサンブル"""
        predictions = []
        for name, model in self.models.items():
            if hasattr(model, 'predict_proba'):
                pred = model.predict_proba(X)[:, 1]
            else:
                pred = model.predict(X)
            predictions.append(pred)
        
        return np.mean(predictions, axis=0)
    
    def weighted_average(self, X):
        """重み付き平均アンサンブル"""
        predictions = []
        weights = []
        
        for name, model in self.models.items():
            if hasattr(model, 'predict_proba'):
                pred = model.predict_proba(X)[:, 1]
            else:
                pred = model.predict(X)
            predictions.append(pred)
            weights.append(self.weights[name])
        
        weights = np.array(weights) / np.sum(weights)
        return np.average(predictions, axis=0, weights=weights)
    
    def stacking_ensemble(self, X_train, y_train, X_val, y_val, X_test):
        """スタッキングアンサンブル"""
        # レベル1の予測値を生成
        train_preds = np.zeros((X_train.shape[0], len(self.models)))
        val_preds = np.zeros((X_val.shape[0], len(self.models)))
        test_preds = np.zeros((X_test.shape[0], len(self.models)))
        
        for i, (name, model) in enumerate(self.models.items()):
            # 訓練データの予測（CV使用推奨）
            if hasattr(model, 'predict_proba'):
                train_preds[:, i] = model.predict_proba(X_train)[:, 1]
                val_preds[:, i] = model.predict_proba(X_val)[:, 1]
                test_preds[:, i] = model.predict_proba(X_test)[:, 1]
            else:
                train_preds[:, i] = model.predict(X_train)
                val_preds[:, i] = model.predict(X_val)
                test_preds[:, i] = model.predict(X_test)
        
        # レベル2モデル（メタモデル）
        meta_model = lgb.LGBMClassifier(n_estimators=100, random_state=42)
        meta_model.fit(train_preds, y_train)
        
        # 最終予測
        final_preds = meta_model.predict_proba(test_preds)[:, 1]
        
        return final_preds

# 使用例
ensemble = KaggleEnsemble()
ensemble.add_model('lgb', lgb_model, weight=0.4)
ensemble.add_model('xgb', xgb_model, weight=0.3)
ensemble.add_model('catboost', cb_model, weight=0.3)

# 重み付きアンサンブル
ensemble_pred = ensemble.weighted_average(X_test)
```

## 6. バリデーション戦略

### 6.1 クロスバリデーション設計
```python
def create_cv_strategy(df, target_col, task_type='classification', time_col=None):
    """適切なCV戦略を設計"""
    
    if time_col is not None:
        # 時系列データの場合：TimeSeriesSplit
        from sklearn.model_selection import TimeSeriesSplit
        cv = TimeSeriesSplit(n_splits=5)
        print("Using TimeSeriesSplit for time series data")
        
    elif task_type == 'classification':
        # 分類の場合：StratifiedKFold
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        print("Using StratifiedKFold for classification")
        
    else:
        # 回帰の場合：KFold
        cv = KFold(n_splits=5, shuffle=True, random_state=42)
        print("Using KFold for regression")
    
    return cv

def cross_validate_model(model, X, y, cv, scoring='roc_auc'):
    """クロスバリデーション実行"""
    from sklearn.model_selection import cross_val_score
    
    scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)
    
    print(f"CV Scores: {scores}")
    print(f"Mean: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})")
    
    return scores

# 使用例
cv = create_cv_strategy(train_df, 'target', 'classification')
cv_scores = cross_validate_model(lgb_model, X, y, cv)
```

### 6.2 バリデーション戦略の検証
```python
def validate_cv_strategy(train_df, test_df, cv, target_col):
    """CV戦略が適切かどうかを検証"""
    
    # Public LBとLocal CVの相関を確認
    # （実際のコンペでは過去のsubmissionデータを使用）
    
    # 1. Train/Validationの分布比較
    scores = []
    for train_idx, val_idx in cv.split(train_df, train_df[target_col]):
        train_fold = train_df.iloc[train_idx]
        val_fold = train_df.iloc[val_idx]
        
        # 特徴量分布の比較
        from scipy.stats import ks_2samp
        
        numeric_cols = train_df.select_dtypes(include=[np.number]).columns
        p_values = []
        
        for col in numeric_cols:
            if col != target_col:
                _, p_value = ks_2samp(train_fold[col].dropna(), 
                                     val_fold[col].dropna())
                p_values.append(p_value)
        
        scores.append(np.mean(p_values))
    
    print(f"Average p-value across folds: {np.mean(scores):.4f}")
    
    # 2. ターゲット分布の確認
    print("\nTarget distribution per fold:")
    for i, (train_idx, val_idx) in enumerate(cv.split(train_df, train_df[target_col])):
        val_target = train_df.iloc[val_idx][target_col]
        print(f"Fold {i+1}: {val_target.value_counts().to_dict()}")

# 使用例
validate_cv_strategy(train_df, test_df, cv, 'target')
```

## 7. 深層学習アプローチ

### 7.1 表形式データ用ニューラルネット
```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

class TabularNN:
    def __init__(self, input_dim, output_dim, task_type='classification'):
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.task_type = task_type
        self.model = None
    
    def build_model(self):
        """深層学習モデル構築"""
        inputs = keras.Input(shape=(self.input_dim,))
        
        # エンベディング層（カテゴリ特徴量用）
        x = layers.Dense(512, activation='relu')(inputs)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.3)(x)
        
        x = layers.Dense(256, activation='relu')(x)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.3)(x)
        
        x = layers.Dense(128, activation='relu')(x)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.2)(x)
        
        x = layers.Dense(64, activation='relu')(x)
        x = layers.Dropout(0.2)(x)
        
        # 出力層
        if self.task_type == 'classification':
            if self.output_dim == 1:
                outputs = layers.Dense(1, activation='sigmoid')(x)
            else:
                outputs = layers.Dense(self.output_dim, activation='softmax')(x)
        else:
            outputs = layers.Dense(self.output_dim)(x)
        
        self.model = keras.Model(inputs, outputs)
        
        # コンパイル
        if self.task_type == 'classification':
            loss = 'binary_crossentropy' if self.output_dim == 1 else 'categorical_crossentropy'
            metrics = ['accuracy', 'auc'] if self.output_dim == 1 else ['accuracy']
        else:
            loss = 'mse'
            metrics = ['mae']
        
        self.model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.001),
            loss=loss,
            metrics=metrics
        )
        
        return self.model
    
    def train(self, X_train, y_train, X_val, y_val, epochs=100):
        """モデル訓練"""
        callbacks = [
            keras.callbacks.EarlyStopping(
                monitor='val_loss', 
                patience=20, 
                restore_best_weights=True
            ),
            keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=10,
                min_lr=1e-7
            )
        ]
        
        history = self.model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=epochs,
            batch_size=256,
            callbacks=callbacks,
            verbose=1
        )
        
        return history

# 使用例
nn_model = TabularNN(X_train.shape[1], 1, 'classification')
model = nn_model.build_model()
history = nn_model.train(X_train, y_train, X_val, y_val)
```

### 7.2 Entity Embeddings
```python
def create_entity_embeddings(df, categorical_cols, embedding_dims=None):
    """カテゴリ変数のEntity Embeddings"""
    
    if embedding_dims is None:
        embedding_dims = {}
        for col in categorical_cols:
            unique_count = df[col].nunique()
            embedding_dims[col] = min(50, (unique_count + 1) // 2)
    
    inputs = {}
    embeddings = {}
    
    for col in categorical_cols:
        vocab_size = df[col].nunique() + 1  # +1 for unknown
        embed_dim = embedding_dims[col]
        
        input_layer = keras.Input(shape=(1,), name=f'{col}_input')
        embedding = layers.Embedding(
            vocab_size, 
            embed_dim, 
            name=f'{col}_embedding'
        )(input_layer)
        embedding = layers.Flatten()(embedding)
        
        inputs[col] = input_layer
        embeddings[col] = embedding
    
    # 数値特徴量の入力
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    numeric_input = keras.Input(shape=(len(numeric_cols),), name='numeric_input')
    
    # 全特徴量の結合
    all_features = [numeric_input] + list(embeddings.values())
    combined = layers.concatenate(all_features)
    
    return inputs, combined, numeric_input

# 使用例
inputs, combined_features, numeric_input = create_entity_embeddings(
    train_df, categorical_columns
)
```

## 8. 時系列データ専用テクニック

### 8.1 時系列特徴量生成
```python
def create_time_series_features(df, date_col, value_col, id_col=None):
    """時系列特徴量の生成"""
    
    df = df.copy()
    df[date_col] = pd.to_datetime(df[date_col])
    df = df.sort_values([id_col, date_col] if id_col else [date_col])
    
    # 基本的な時間特徴量
    df['year'] = df[date_col].dt.year
    df['month'] = df[date_col].dt.month
    df['day'] = df[date_col].dt.day
    df['dayofweek'] = df[date_col].dt.dayofweek
    df['quarter'] = df[date_col].dt.quarter
    df['is_weekend'] = (df[date_col].dt.dayofweek >= 5).astype(int)
    
    # ラグ特徴量
    for lag in [1, 2, 3, 7, 14, 30]:
        if id_col:
            df[f'{value_col}_lag_{lag}'] = df.groupby(id_col)[value_col].shift(lag)
        else:
            df[f'{value_col}_lag_{lag}'] = df[value_col].shift(lag)
    
    # 移動平均
    for window in [3, 7, 14, 30]:
        if id_col:
            df[f'{value_col}_ma_{window}'] = (
                df.groupby(id_col)[value_col]
                .rolling(window, min_periods=1)
                .mean()
                .reset_index(0, drop=True)
            )
        else:
            df[f'{value_col}_ma_{window}'] = (
                df[value_col].rolling(window, min_periods=1).mean()
            )
    
    # 差分特徴量
    if id_col:
        df[f'{value_col}_diff'] = df.groupby(id_col)[value_col].diff()
    else:
        df[f'{value_col}_diff'] = df[value_col].diff()
    
    # 変化率
    for lag in [1, 7, 30]:
        lag_col = f'{value_col}_lag_{lag}'
        if lag_col in df.columns:
            df[f'{value_col}_pct_change_{lag}'] = (
                (df[value_col] - df[lag_col]) / (df[lag_col] + 1e-8)
            )
    
    return df

# 使用例
train_ts = create_time_series_features(
    train_df, 'date', 'target_value', 'id'
)
```

### 8.2 時系列バリデーション
```python
class TimeSeriesCV:
    def __init__(self, n_splits=5, gap=0):
        self.n_splits = n_splits
        self.gap = gap
    
    def split(self, X, y=None, groups=None):
        """時系列クロスバリデーション"""
        n_samples = X.shape[0]
        n_splits = self.n_splits
        
        # 各分割のサイズ
        fold_size = n_samples // (n_splits + 1)
        
        for i in range(n_splits):
            # 訓練期間の終了点
            train_end = fold_size * (i + 1)
            
            # 検証期間の開始点（ギャップを考慮）
            val_start = train_end + self.gap
            val_end = min(val_start + fold_size, n_samples)
            
            if val_end <= val_start:
                break
            
            train_indices = np.arange(0, train_end)
            val_indices = np.arange(val_start, val_end)
            
            yield train_indices, val_indices

# 使用例
ts_cv = TimeSeriesCV(n_splits=5, gap=7)  # 7日のギャップ
for train_idx, val_idx in ts_cv.split(X):
    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]
    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]
    # モデル訓練・評価
```

## 9. 効率化のためのTips

### 9.1 メモリ使用量最適化
```python
def reduce_memory_usage(df):
    """メモリ使用量を削減"""
    start_memory = df.memory_usage(deep=True).sum() / 1024**2
    
    for col in df.columns:
        col_type = df[col].dtype
        
        if col_type != 'object':
            c_min = df[col].min()
            c_max = df[col].max()
            
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)
            else:
                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)
        else:
            # カテゴリ型への変換
            if df[col].nunique() < df.shape[0] * 0.5:
                df[col] = df[col].astype('category')
    
    end_memory = df.memory_usage(deep=True).sum() / 1024**2
    print(f'Memory usage decreased from {start_memory:.2f} MB to {end_memory:.2f} MB')
    print(f'Reduction: {(start_memory - end_memory) / start_memory * 100:.1f}%')
    
    return df

# 使用例
train_df = reduce_memory_usage(train_df)
```

### 9.2 並列処理
```python
from multiprocessing import Pool
import functools

def parallel_feature_engineering(df, func, n_cores=4):
    """特徴量エンジニアリングの並列実行"""
    
    # データを分割
    chunk_size = len(df) // n_cores
    chunks = [df[i:i + chunk_size] for i in range(0, len(df), chunk_size)]
    
    # 並列実行
    with Pool(n_cores) as pool:
        results = pool.map(func, chunks)
    
    # 結果を結合
    return pd.concat(results, ignore_index=True)

def feature_creation_chunk(chunk):
    """チャンクごとの特徴量作成"""
    # 重い計算をここで実行
    chunk['new_feature'] = chunk['col1'] * chunk['col2']
    return chunk

# 使用例
# train_df = parallel_feature_engineering(train_df, feature_creation_chunk)
```

## 10. 実戦的なワークフロー

### 10.1 コンペ攻略の流れ
```python
class KaggleWorkflow:
    def __init__(self, competition_name):
        self.competition_name = competition_name
        self.models = {}
        self.scores = {}
        self.submissions = []
    
    def step1_eda(self, train_df, test_df, target_col):
        """Step 1: 探索的データ分析"""
        print("=== Step 1: EDA ===")
        
        # 基本情報
        basic_data_info(train_df)
        
        # リーク検出
        detect_leakage(train_df, test_df, target_col)
        
        # 高度なEDA
        advanced_eda(train_df, target_col)
    
    def step2_feature_engineering(self, train_df, test_df):
        """Step 2: 特徴量エンジニアリング"""
        print("=== Step 2: Feature Engineering ===")
        
        # メモリ最適化
        train_df = reduce_memory_usage(train_df)
        test_df = reduce_memory_usage(test_df)
        
        # 特徴量生成
        fe = FeatureEngineering()
        
        # 数値特徴量
        numeric_cols = train_df.select_dtypes(include=[np.number]).columns
        train_df = fe.numeric_features(train_df, numeric_cols)
        test_df = fe.numeric_features(test_df, numeric_cols)
        
        # カテゴリ特徴量
        categorical_cols = train_df.select_dtypes(include=['object', 'category']).columns
        train_df = fe.categorical_features(train_df, categorical_cols)
        test_df = fe.categorical_features(test_df, categorical_cols)
        
        return train_df, test_df
    
    def step3_validation_strategy(self, train_df, target_col):
        """Step 3: バリデーション戦略設計"""
        print("=== Step 3: Validation Strategy ===")
        
        cv = create_cv_strategy(train_df, target_col)
        validate_cv_strategy(train_df, None, cv, target_col)
        
        return cv
    
    def step4_baseline_models(self, X_train, y_train, X_val, y_val):
        """Step 4: ベースラインモデル構築"""
        print("=== Step 4: Baseline Models ===")
        
        models, scores = create_baseline_models(X_train, y_train, X_val, y_val)
        self.models.update(models)
        self.scores.update(scores)
        
        return models
    
    def step5_optimization(self, X_train, y_train, X_val, y_val):
        """Step 5: ハイパーパラメータ最適化"""
        print("=== Step 5: Hyperparameter Optimization ===")
        
        best_params = optimize_lgb_params(X_train, y_train, X_val, y_val)
        
        # 最適化されたモデルを再訓練
        optimized_model = lgb.LGBMClassifier(**best_params, n_estimators=1000)
        optimized_model.fit(X_train, y_train,
                           eval_set=[(X_val, y_val)],
                           callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)])
        
        self.models['lgb_optimized'] = optimized_model
        self.scores['lgb_optimized'] = roc_auc_score(
            y_val, optimized_model.predict_proba(X_val)[:, 1]
        )
        
        return optimized_model
    
    def step6_ensemble(self, X_test):
        """Step 6: アンサンブル"""
        print("=== Step 6: Ensemble ===")
        
        ensemble = KaggleEnsemble()
        for name, model in self.models.items():
            weight = self.scores[name]  # スコアに基づく重み
            ensemble.add_model(name, model, weight)
        
        final_prediction = ensemble.weighted_average(X_test)
        
        return final_prediction
    
    def step7_submission(self, test_ids, predictions, submission_message=""):
        """Step 7: 提出"""
        print("=== Step 7: Submission ===")
        
        submission_df = pd.DataFrame({
            'id': test_ids,
            'target': predictions
        })
        
        filename = f'submission_{len(self.submissions)+1}.csv'
        submission_df.to_csv(filename, index=False)
        
        self.submissions.append({
            'filename': filename,
            'message': submission_message,
            'local_cv': np.mean(list(self.scores.values()))
        })
        
        print(f"Submission saved as {filename}")
        return submission_df

# 使用例
workflow = KaggleWorkflow("example-competition")

# Step 1: EDA
workflow.step1_eda(train_df, test_df, 'target')

# Step 2: Feature Engineering
train_processed, test_processed = workflow.step2_feature_engineering(train_df, test_df)

# Step 3: Validation Strategy
cv = workflow.step3_validation_strategy(train_processed, 'target')

# データ分割
X = train_processed.drop(['target'], axis=1)
y = train_processed['target']
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Baseline Models
models = workflow.step4_baseline_models(X_train, y_train, X_val, y_val)

# Step 5: Optimization
optimized_model = workflow.step5_optimization(X_train, y_train, X_val, y_val)

# Step 6: Ensemble
X_test = test_processed.drop(['id'], axis=1)  # idカラムがある場合
final_predictions = workflow.step6_ensemble(X_test)

# Step 7: Submission
submission = workflow.step7_submission(
    test_processed['id'], 
    final_predictions, 
    "Ensemble of LGB, XGB, CatBoost with optimized parameters"
)
```

## まとめ：Kaggle勝利の方程式

### 重要なポイント

1. **データ理解が最重要** - EDAに時間をかける
2. **リーク検出** - Public/Private LBの乖離を防ぐ
3. **適切なCV戦略** - Local CVとLBの相関を確保
4. **特徴量エンジニアリング** - ドメイン知識を活用
5. **アンサンブル** - 多様性のあるモデルを組み合わせ
6. **継続的改善** - 小さな改善を積み重ね

### チーム戦略
- **役割分担**: EDA、FE、モデリング、アンサンブル
- **知識共有**: 定期的なディスカッション
- **コード共有**: GitHubやKaggle Datasets活用
- **時間管理**: マイルストーン設定

### 学習リソース
- **Kaggle Learn**: 無料のコース
- **過去のコンペ**: 解法をStudy
- **Discussion**: 他の参加者との情報交換
- **書籍**: 『Kaggleで勝つデータ分析の技術』

Kaggleでの成功は一朝一夕には達成できませんが、体系的なアプローチと継続的な学習により、確実にスキルアップできます。
