# Kaggleで勝つ！scikit-learn & NumPy完全攻略 - 新人エンジニア向け

## 🏆 勝利への道筋
Kaggleで上位入賞するために必要なscikit-learnとNumPyのテクニックを、実戦で使えるコード付きで徹底解説します。

---

## 🎯 1. scikit-learn 勝利のパターン

### 強力なモデル選択戦略

```python
# 必須インポート（これだけは覚えておく）
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

# 複数モデルの性能比較（勝つための基本戦略）
models = {
    'RandomForest': RandomForestClassifier(random_state=42),      # アンサンブル系最強
    'GradientBoosting': GradientBoostingClassifier(random_state=42),  # ブースティング系
    'LogisticRegression': LogisticRegression(random_state=42),    # 線形モデル
    'SVM': SVC(random_state=42, probability=True)                # サポートベクター
}

# 各モデルのCV性能を比較
best_score = 0
best_model_name = ""

for name, model in models.items():
    # 5分割クロスバリデーションを実行
    scores = cross_val_score(
        model,              # 評価するモデル
        X_train,           # 訓練用特徴量
        y_train,           # 訓練用目的変数
        cv=5,              # 分割数
        scoring='accuracy'  # 評価指標（タスクに応じて変更）
    )
    mean_score = scores.mean()  # 平均スコア計算
    std_score = scores.std()    # 標準偏差計算
    
    print(f'{name}: {mean_score:.4f} (+/- {std_score * 2:.4f})')
    
    # 最高性能モデルを記録
    if mean_score > best_score:
        best_score = mean_score
        best_model_name = name

print(f'\n最優秀モデル: {best_model_name} (スコア: {best_score:.4f})')
```

### ハイパーパラメータ最適化（勝負の分かれ目）

```python
# GridSearchCVで最適パラメータを発見
param_grid = {
    'n_estimators': [100, 200, 300],        # 木の数
    'max_depth': [10, 20, 30, None],        # 木の深さ
    'min_samples_split': [2, 5, 10],        # 分割に必要な最小サンプル数
    'min_samples_leaf': [1, 2, 4]           # 葉ノードの最小サンプル数
}

# グリッドサーチを実行
grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),  # ベースモデル
    param_grid,                              # パラメータ候補
    cv=5,                                   # クロスバリデーション分割数
    scoring='accuracy',                      # 評価指標
    n_jobs=-1,                              # 並列処理（全CPU使用）
    verbose=1                               # 進捗表示
)

# 最適化実行
grid_search.fit(X_train, y_train)

# 最適パラメータと性能を表示
print(f'最適パラメータ: {grid_search.best_params_}')
print(f'最高CVスコア: {grid_search.best_score_:.4f}')

# 最適モデルを取得
best_model = grid_search.best_estimator_
```

### 特徴量重要度分析（勝利の鍵）

```python
# 特徴量重要度を取得・可視化
feature_importance = best_model.feature_importances_  # 重要度配列取得
feature_names = X_train.columns                      # 特徴量名取得

# 重要度順にソート
importance_df = pd.DataFrame({
    'feature': feature_names,      # 特徴量名
    'importance': feature_importance  # 重要度
}).sort_values('importance', ascending=False)  # 降順ソート

# 上位10特徴量を表示
print("重要特徴量トップ10:")
print(importance_df.head(10))

# 重要度が低い特徴量を除去（次元削減）
important_features = importance_df[importance_df['importance'] > 0.01]['feature']
X_train_selected = X_train[important_features]  # 重要特徴量のみ選択
X_test_selected = X_test[important_features]    # テストデータも同様に選択
```

---

## 🔢 2. NumPy 高速化テクニック

### ベクトル化で爆速処理

```python
import numpy as np

# 悪い例：Pythonループ（遅い）
def slow_calculation(arr):
    result = []
    for x in arr:           # 1つずつ処理（遅い）
        result.append(x ** 2 + np.sin(x))
    return result

# 良い例：NumPyベクトル化（高速）
def fast_calculation(arr):
    return arr ** 2 + np.sin(arr)  # 全要素を一度に処理（高速）

# 速度比較用のテストデータ
large_array = np.random.randn(1000000)  # 100万個のランダム数

# 実行時間測定
%timeit slow_calculation(large_array)   # 数秒かかる
%timeit fast_calculation(large_array)   # 数ミリ秒で完了
```

### 高度な配列操作

```python
# 条件付き配列操作（where関数活用）
arr = np.array([1, -2, 3, -4, 5])
positive_only = np.where(arr > 0, arr, 0)  # 負数を0に置換
print(positive_only)  # [1, 0, 3, 0, 5]

# 複数条件での選択
conditions = [
    arr < 0,                    # 条件1：負数
    (arr >= 0) & (arr <= 3),   # 条件2：0以上3以下
    arr > 3                     # 条件3：3より大きい
]
choices = ['negative', 'small', 'large']    # 各条件に対応するラベル
labels = np.select(conditions, choices)     # 条件に応じてラベル付け

# 配列の統計処理
percentiles = np.percentile(arr, [25, 50, 75])  # 四分位数計算
print(f'25%点: {percentiles[0]}, 中央値: {percentiles[1]}, 75%点: {percentiles[2]}')

# 外れ値検出（IQR法）
Q1 = np.percentile(arr, 25)    # 第1四分位数
Q3 = np.percentile(arr, 75)    # 第3四分位数
IQR = Q3 - Q1                  # 四分位範囲
outlier_mask = (arr < Q1 - 1.5 * IQR) | (arr > Q3 + 1.5 * IQR)  # 外れ値フラグ
```

---

## 🚀 3. Kaggle必勝の前処理パイプライン

### 完璧な前処理テンプレート

```python
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer

def kaggle_preprocessing_pipeline(train_df, test_df, target_col):
    """
    Kaggle用の完璧な前処理パイプライン
    
    Args:
        train_df: 訓練データのDataFrame
        test_df: テストデータのDataFrame  
        target_col: 目的変数の列名
    """
    
    # 目的変数を分離
    y_train = train_df[target_col].copy()           # 目的変数をコピー
    X_train = train_df.drop(target_col, axis=1)     # 特徴量のみ残す
    X_test = test_df.copy()                         # テストデータコピー
    
    # 数値列とカテゴリ列を分離
    numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()      # 数値列特定
    categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()   # カテゴリ列特定
    
    print(f'数値列: {len(numeric_cols)}個, カテゴリ列: {len(categorical_cols)}個')
    
    # 数値列の欠損値補完
    numeric_imputer = SimpleImputer(strategy='median')  # 中央値で補完
    X_train[numeric_cols] = numeric_imputer.fit_transform(X_train[numeric_cols])
    X_test[numeric_cols] = numeric_imputer.transform(X_test[numeric_cols])
    
    # カテゴリ列の欠損値補完とエンコーディング
    for col in categorical_cols:
        # 最頻値で欠損値補完
        mode_value = X_train[col].mode()[0]             # 最頻値取得
        X_train[col].fillna(mode_value, inplace=True)   # 訓練データ補完
        X_test[col].fillna(mode_value, inplace=True)    # テストデータ補完
        
        # ラベルエンコーディング実行
        le = LabelEncoder()                             # エンコーダー初期化
        X_train[col] = le.fit_transform(X_train[col])   # 訓練データエンコード
        X_test[col] = le.transform(X_test[col])         # テストデータエンコード
    
    # 数値列の標準化
    scaler = StandardScaler()                                      # 標準化器初期化
    X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])  # 訓練データ標準化
    X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])        # テストデータ標準化
    
    return X_train, X_test, y_train
```

---

## 🎪 4. アンサンブル技法（上位入賞の秘訣）

### スタッキング（最強のアンサンブル）

```python
from sklearn.model_selection import StratifiedKFold
from sklearn.base import clone

def create_stacking_features(models, X_train, y_train, X_test, n_folds=5):
    """
    スタッキング用の特徴量を作成
    
    Args:
        models: ベースモデルのリスト
        X_train: 訓練用特徴量
        y_train: 訓練用目的変数
        X_test: テスト用特徴量
        n_folds: 分割数
    """
    
    # 結果格納用配列を初期化
    train_blend = np.zeros((X_train.shape[0], len(models)))  # 訓練用ブレンド特徴量
    test_blend = np.zeros((X_test.shape[0], len(models)))    # テスト用ブレンド特徴量
    
    # 層化K分割を作成
    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)
    
    # 各ベースモデルに対して処理
    for i, model in enumerate(models):
        print(f'ベースモデル {i+1}/{len(models)} を処理中...')
        
        test_predictions = []  # テスト予測を格納
        
        # K分割クロスバリデーション
        for train_idx, val_idx in skf.split(X_train, y_train):
            # 分割データを取得
            X_fold_train = X_train.iloc[train_idx]  # 学習用データ
            y_fold_train = y_train.iloc[train_idx]  # 学習用ラベル
            X_fold_val = X_train.iloc[val_idx]      # 検証用データ
            
            # モデルをクローンして訓練
            fold_model = clone(model)               # モデルを複製
            fold_model.fit(X_fold_train, y_fold_train)  # 学習実行
            
            # 検証データで予測（out-of-fold予測）
            val_pred = fold_model.predict_proba(X_fold_val)[:, 1]  # 確率予測
            train_blend[val_idx, i] = val_pred                     # ブレンド特徴量に格納
            
            # テストデータで予測
            test_pred = fold_model.predict_proba(X_test)[:, 1]     # テスト予測
            test_predictions.append(test_pred)                     # 予測リストに追加
        
        # テスト予測の平均を取る
        test_blend[:, i] = np.mean(test_predictions, axis=0)       # 平均予測値
    
    return train_blend, test_blend

# スタッキング実行例
base_models = [
    RandomForestClassifier(n_estimators=100, random_state=42),
    GradientBoostingClassifier(random_state=42),
    SVC(probability=True, random_state=42)
]

# ブレンド特徴量を作成
train_blend, test_blend = create_stacking_features(base_models, X_train, y_train, X_test)

# メタモデル（第2層）で最終予測
meta_model = LogisticRegression(random_state=42)  # シンプルなメタモデル
meta_model.fit(train_blend, y_train)              # ブレンド特徴量で学習
final_pred = meta_model.predict_proba(test_blend)[:, 1]  # 最終予測
```

---

## 🔢 5. NumPy 高速化の奥義

### メモリ効率と計算最適化

```python
# 大容量データの効率的処理
def efficient_feature_engineering(data):
    """
    メモリ効率を考慮した特徴量エンジニアリング
    """
    
    # データ型最適化（メモリ節約）
    for col in data.select_dtypes(include=['int64']).columns:
        col_min = data[col].min()  # 列の最小値
        col_max = data[col].max()  # 列の最大値
        
        # 適切なデータ型を選択
        if col_min >= 0:  # 非負整数の場合
            if col_max < 255:
                data[col] = data[col].astype(np.uint8)      # 8bit符号なし整数
            elif col_max < 65535:
                data[col] = data[col].astype(np.uint16)     # 16bit符号なし整数
        else:  # 負数を含む場合
            if col_min >= -128 and col_max <= 127:
                data[col] = data[col].astype(np.int8)       # 8bit符号付き整数
            elif col_min >= -32768 and col_max <= 32767:
                data[col] = data[col].astype(np.int16)      # 16bit符号付き整数
    
    # float64をfloat32に変換（メモリ半分）
    for col in data.select_dtypes(include=['float64']).columns:
        data[col] = data[col].astype(np.float32)
    
    return data

# 高速統計計算
def fast_rolling_statistics(arr, window=5):
    """
    高速な移動統計量計算
    """
    # NumPyの畳み込みを使用した高速移動平均
    kernel = np.ones(window) / window           # 移動平均カーネル
    moving_avg = np.convolve(arr, kernel, mode='valid')  # 畳み込み実行
    
    # 移動標準偏差（効率的計算）
    arr_squared = arr ** 2                      # 二乗配列
    moving_var = np.convolve(arr_squared, kernel, mode='valid') - moving_avg ** 2  # 分散計算
    moving_std = np.sqrt(np.maximum(moving_var, 0))  # 標準偏差（負数回避）
    
    return moving_avg, moving_std
```

### 特徴量生成の黄金パターン

```python
def create_winning_features(df):
    """
    Kaggle上位陣が使う特徴量生成パターン
    """
    
    # 1. 統計的特徴量
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    
    # 行ごとの統計量
    df['row_mean'] = df[numeric_cols].mean(axis=1)      # 行平均
    df['row_std'] = df[numeric_cols].std(axis=1)        # 行標準偏差
    df['row_max'] = df[numeric_cols].max(axis=1)        # 行最大値
    df['row_min'] = df[numeric_cols].min(axis=1)        # 行最小値
    df['row_range'] = df['row_max'] - df['row_min']     # 行レンジ
    
    # 2. 交互作用特徴量（重要！）
    important_cols = numeric_cols[:5]  # 重要な列を選択
    for i, col1 in enumerate(important_cols):
        for col2 in important_cols[i+1:]:
            df[f'{col1}_x_{col2}'] = df[col1] * df[col2]        # 積
            df[f'{col1}_div_{col2}'] = df[col1] / (df[col2] + 1e-8)  # 商（ゼロ除算回避）
            df[f'{col1}_diff_{col2}'] = df[col1] - df[col2]     # 差
    
    # 3. 多項式特徴量
    for col in important_cols:
        df[f'{col}_squared'] = df[col] ** 2                 # 二乗
        df[f'{col}_cubed'] = df[col] ** 3                   # 三乗
        df[f'{col}_sqrt'] = np.sqrt(np.abs(df[col]))        # 平方根
        df[f'{col}_log'] = np.log1p(np.abs(df[col]))        # 対数変換
    
    return df

# 特徴量生成実行
enhanced_train = create_winning_features(X_train.copy())
enhanced_test = create_winning_features(X_test.copy())
```

---

## 🏅 6. Kaggle提出用コードテンプレート

### 完璧な提出スクリプト

```python
# 完全なKaggleソリューションテンプレート
def kaggle_solution_template():
    """
    Kaggle上位入賞のための完全テンプレート
    """
    
    # 1. データ読み込み
    train = pd.read_csv('/kaggle/input/train.csv')      # 訓練データ
    test = pd.read_csv('/kaggle/input/test.csv')        # テストデータ
    submission = pd.read_csv('/kaggle/input/sample_submission.csv')  # 提出形式
    
    # 2. 前処理実行
    X_train, X_test, y_train = kaggle_preprocessing_pipeline(train, test, 'target')
    
    # 3. 特徴量エンジニアリング
    X_train = create_winning_features(X_train)
    X_test = create_winning_features(X_test)
    
    # 4. モデル訓練（アンサンブル）
    train_blend, test_blend = create_stacking_features(base_models, X_train, y_train, X_test)
    
    # 5. 最終モデルで予測
    final_model = LogisticRegression(random_state=42)
    final_model.fit(train_blend, y_train)
    predictions = final_model.predict_proba(test_blend)[:, 1]
    
    # 6. 提出ファイル作成
    submission['target'] = predictions                   # 予測値を格納
    submission.to_csv('submission.csv', index=False)    # CSVで保存
    
    print('提出ファイルが完成しました！')
    return predictions

# 実行
predictions = kaggle_solution_template()
```

---

## ⚡ クイックリファレンス

### scikit-learn必須コマンド
```python
# モデル評価
cross_val_score(model, X, y, cv=5)          # クロスバリデーション
classification_report(y_true, y_pred)       # 分類レポート
confusion_matrix(y_true, y_pred)           # 混同行列

# ハイパーパラメータ最適化  
GridSearchCV(model, param_grid, cv=5)       # グリッドサーチ
RandomizedSearchCV(model, param_dist, cv=5) # ランダムサーチ

# 前処理
StandardScaler().fit_transform(X)           # 標準化
MinMaxScaler().fit_transform(X)            # 正規化
LabelEncoder().fit_transform(y)            # ラベルエンコード
```

### NumPy高速化コマンド
```python
# 高速統計計算
np.mean(arr, axis=0)                       # 列ごとの平均
np.std(arr, axis=1)                        # 行ごとの標準偏差
np.percentile(arr, [25, 50, 75])           # パーセンタイル

# 条件処理
np.where(condition, value_if_true, value_if_false)  # 条件分岐
np.select([cond1, cond2], [val1, val2])            # 複数条件

# 配列操作
np.concatenate([arr1, arr2])               # 配列結合
np.vstack([arr1, arr2])                    # 縦方向結合
np.hstack([arr1, arr2])                    # 横方向結合
```

---

## 🎯 勝利のための最終チェックリスト

### ✅ 提出前の必須確認項目

1. **データリーク確認**
   - 未来情報が特徴量に含まれていないか
   - テストデータの情報が訓練に漏れていないか

2. **クロスバリデーション設定**
   - 時系列データなら時間順分割
   - 層化サンプリングでクラス比率維持

3. **特徴量重要度チェック**
   - 不要な特徴量は削除
   - 重要特徴量の組み合わせを追加

4. **アンサンブル多様性**
   - 異なるアルゴリズムの組み合わせ
   - 異なる特徴量セットでの学習

5. **予測値の後処理**
   - 確率値の範囲チェック（0-1）
   - 異常値のクリッピング

---

このガイドのテクニックを実践すれば、Kaggleで確実に上位入賞を狙えます！💪

まずは小さなコンペで練習して、徐々に高度なテクニックを身につけていきましょう。頑張ってください！🚀
