# Kaggleã§å‹ã¤ï¼scikit-learn & NumPyå®Œå…¨æ”»ç•¥ - æ–°äººã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å‘ã‘

## ğŸ† å‹åˆ©ã¸ã®é“ç­‹
Kaggleã§ä¸Šä½å…¥è³ã™ã‚‹ãŸã‚ã«å¿…è¦ãªscikit-learnã¨NumPyã®ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã‚’ã€å®Ÿæˆ¦ã§ä½¿ãˆã‚‹ã‚³ãƒ¼ãƒ‰ä»˜ãã§å¾¹åº•è§£èª¬ã—ã¾ã™ã€‚

---

## ğŸ¯ 1. scikit-learn å‹åˆ©ã®ãƒ‘ã‚¿ãƒ¼ãƒ³

### å¼·åŠ›ãªãƒ¢ãƒ‡ãƒ«é¸æŠæˆ¦ç•¥

```python
# å¿…é ˆã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã“ã‚Œã ã‘ã¯è¦šãˆã¦ãŠãï¼‰
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

# è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½æ¯”è¼ƒï¼ˆå‹ã¤ãŸã‚ã®åŸºæœ¬æˆ¦ç•¥ï¼‰
models = {
    'RandomForest': RandomForestClassifier(random_state=42),      # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç³»æœ€å¼·
    'GradientBoosting': GradientBoostingClassifier(random_state=42),  # ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ç³»
    'LogisticRegression': LogisticRegression(random_state=42),    # ç·šå½¢ãƒ¢ãƒ‡ãƒ«
    'SVM': SVC(random_state=42, probability=True)                # ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼
}

# å„ãƒ¢ãƒ‡ãƒ«ã®CVæ€§èƒ½ã‚’æ¯”è¼ƒ
best_score = 0
best_model_name = ""

for name, model in models.items():
    # 5åˆ†å‰²ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ
    scores = cross_val_score(
        model,              # è©•ä¾¡ã™ã‚‹ãƒ¢ãƒ‡ãƒ«
        X_train,           # è¨“ç·´ç”¨ç‰¹å¾´é‡
        y_train,           # è¨“ç·´ç”¨ç›®çš„å¤‰æ•°
        cv=5,              # åˆ†å‰²æ•°
        scoring='accuracy'  # è©•ä¾¡æŒ‡æ¨™ï¼ˆã‚¿ã‚¹ã‚¯ã«å¿œã˜ã¦å¤‰æ›´ï¼‰
    )
    mean_score = scores.mean()  # å¹³å‡ã‚¹ã‚³ã‚¢è¨ˆç®—
    std_score = scores.std()    # æ¨™æº–åå·®è¨ˆç®—
    
    print(f'{name}: {mean_score:.4f} (+/- {std_score * 2:.4f})')
    
    # æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã‚’è¨˜éŒ²
    if mean_score > best_score:
        best_score = mean_score
        best_model_name = name

print(f'\næœ€å„ªç§€ãƒ¢ãƒ‡ãƒ«: {best_model_name} (ã‚¹ã‚³ã‚¢: {best_score:.4f})')
```

### ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ï¼ˆå‹è² ã®åˆ†ã‹ã‚Œç›®ï¼‰

```python
# GridSearchCVã§æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç™ºè¦‹
param_grid = {
    'n_estimators': [100, 200, 300],        # æœ¨ã®æ•°
    'max_depth': [10, 20, 30, None],        # æœ¨ã®æ·±ã•
    'min_samples_split': [2, 5, 10],        # åˆ†å‰²ã«å¿…è¦ãªæœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°
    'min_samples_leaf': [1, 2, 4]           # è‘‰ãƒãƒ¼ãƒ‰ã®æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°
}

# ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã‚’å®Ÿè¡Œ
grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),  # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«
    param_grid,                              # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å€™è£œ
    cv=5,                                   # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³åˆ†å‰²æ•°
    scoring='accuracy',                      # è©•ä¾¡æŒ‡æ¨™
    n_jobs=-1,                              # ä¸¦åˆ—å‡¦ç†ï¼ˆå…¨CPUä½¿ç”¨ï¼‰
    verbose=1                               # é€²æ—è¡¨ç¤º
)

# æœ€é©åŒ–å®Ÿè¡Œ
grid_search.fit(X_train, y_train)

# æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨æ€§èƒ½ã‚’è¡¨ç¤º
print(f'æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {grid_search.best_params_}')
print(f'æœ€é«˜CVã‚¹ã‚³ã‚¢: {grid_search.best_score_:.4f}')

# æœ€é©ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
best_model = grid_search.best_estimator_
```

### ç‰¹å¾´é‡é‡è¦åº¦åˆ†æï¼ˆå‹åˆ©ã®éµï¼‰

```python
# ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—ãƒ»å¯è¦–åŒ–
feature_importance = best_model.feature_importances_  # é‡è¦åº¦é…åˆ—å–å¾—
feature_names = X_train.columns                      # ç‰¹å¾´é‡åå–å¾—

# é‡è¦åº¦é †ã«ã‚½ãƒ¼ãƒˆ
importance_df = pd.DataFrame({
    'feature': feature_names,      # ç‰¹å¾´é‡å
    'importance': feature_importance  # é‡è¦åº¦
}).sort_values('importance', ascending=False)  # é™é †ã‚½ãƒ¼ãƒˆ

# ä¸Šä½10ç‰¹å¾´é‡ã‚’è¡¨ç¤º
print("é‡è¦ç‰¹å¾´é‡ãƒˆãƒƒãƒ—10:")
print(importance_df.head(10))

# é‡è¦åº¦ãŒä½ã„ç‰¹å¾´é‡ã‚’é™¤å»ï¼ˆæ¬¡å…ƒå‰Šæ¸›ï¼‰
important_features = importance_df[importance_df['importance'] > 0.01]['feature']
X_train_selected = X_train[important_features]  # é‡è¦ç‰¹å¾´é‡ã®ã¿é¸æŠ
X_test_selected = X_test[important_features]    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚‚åŒæ§˜ã«é¸æŠ
```

---

## ğŸ”¢ 2. NumPy é«˜é€ŸåŒ–ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯

### ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã§çˆ†é€Ÿå‡¦ç†

```python
import numpy as np

# æ‚ªã„ä¾‹ï¼šPythonãƒ«ãƒ¼ãƒ—ï¼ˆé…ã„ï¼‰
def slow_calculation(arr):
    result = []
    for x in arr:           # 1ã¤ãšã¤å‡¦ç†ï¼ˆé…ã„ï¼‰
        result.append(x ** 2 + np.sin(x))
    return result

# è‰¯ã„ä¾‹ï¼šNumPyãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆé«˜é€Ÿï¼‰
def fast_calculation(arr):
    return arr ** 2 + np.sin(arr)  # å…¨è¦ç´ ã‚’ä¸€åº¦ã«å‡¦ç†ï¼ˆé«˜é€Ÿï¼‰

# é€Ÿåº¦æ¯”è¼ƒç”¨ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
large_array = np.random.randn(1000000)  # 100ä¸‡å€‹ã®ãƒ©ãƒ³ãƒ€ãƒ æ•°

# å®Ÿè¡Œæ™‚é–“æ¸¬å®š
%timeit slow_calculation(large_array)   # æ•°ç§’ã‹ã‹ã‚‹
%timeit fast_calculation(large_array)   # æ•°ãƒŸãƒªç§’ã§å®Œäº†
```

### é«˜åº¦ãªé…åˆ—æ“ä½œ

```python
# æ¡ä»¶ä»˜ãé…åˆ—æ“ä½œï¼ˆwhereé–¢æ•°æ´»ç”¨ï¼‰
arr = np.array([1, -2, 3, -4, 5])
positive_only = np.where(arr > 0, arr, 0)  # è² æ•°ã‚’0ã«ç½®æ›
print(positive_only)  # [1, 0, 3, 0, 5]

# è¤‡æ•°æ¡ä»¶ã§ã®é¸æŠ
conditions = [
    arr < 0,                    # æ¡ä»¶1ï¼šè² æ•°
    (arr >= 0) & (arr <= 3),   # æ¡ä»¶2ï¼š0ä»¥ä¸Š3ä»¥ä¸‹
    arr > 3                     # æ¡ä»¶3ï¼š3ã‚ˆã‚Šå¤§ãã„
]
choices = ['negative', 'small', 'large']    # å„æ¡ä»¶ã«å¯¾å¿œã™ã‚‹ãƒ©ãƒ™ãƒ«
labels = np.select(conditions, choices)     # æ¡ä»¶ã«å¿œã˜ã¦ãƒ©ãƒ™ãƒ«ä»˜ã‘

# é…åˆ—ã®çµ±è¨ˆå‡¦ç†
percentiles = np.percentile(arr, [25, 50, 75])  # å››åˆ†ä½æ•°è¨ˆç®—
print(f'25%ç‚¹: {percentiles[0]}, ä¸­å¤®å€¤: {percentiles[1]}, 75%ç‚¹: {percentiles[2]}')

# å¤–ã‚Œå€¤æ¤œå‡ºï¼ˆIQRæ³•ï¼‰
Q1 = np.percentile(arr, 25)    # ç¬¬1å››åˆ†ä½æ•°
Q3 = np.percentile(arr, 75)    # ç¬¬3å››åˆ†ä½æ•°
IQR = Q3 - Q1                  # å››åˆ†ä½ç¯„å›²
outlier_mask = (arr < Q1 - 1.5 * IQR) | (arr > Q3 + 1.5 * IQR)  # å¤–ã‚Œå€¤ãƒ•ãƒ©ã‚°
```

---

## ğŸš€ 3. Kaggleå¿…å‹ã®å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

### å®Œç’§ãªå‰å‡¦ç†ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

```python
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer

def kaggle_preprocessing_pipeline(train_df, test_df, target_col):
    """
    Kaggleç”¨ã®å®Œç’§ãªå‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    
    Args:
        train_df: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®DataFrame
        test_df: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®DataFrame  
        target_col: ç›®çš„å¤‰æ•°ã®åˆ—å
    """
    
    # ç›®çš„å¤‰æ•°ã‚’åˆ†é›¢
    y_train = train_df[target_col].copy()           # ç›®çš„å¤‰æ•°ã‚’ã‚³ãƒ”ãƒ¼
    X_train = train_df.drop(target_col, axis=1)     # ç‰¹å¾´é‡ã®ã¿æ®‹ã™
    X_test = test_df.copy()                         # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚³ãƒ”ãƒ¼
    
    # æ•°å€¤åˆ—ã¨ã‚«ãƒ†ã‚´ãƒªåˆ—ã‚’åˆ†é›¢
    numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()      # æ•°å€¤åˆ—ç‰¹å®š
    categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()   # ã‚«ãƒ†ã‚´ãƒªåˆ—ç‰¹å®š
    
    print(f'æ•°å€¤åˆ—: {len(numeric_cols)}å€‹, ã‚«ãƒ†ã‚´ãƒªåˆ—: {len(categorical_cols)}å€‹')
    
    # æ•°å€¤åˆ—ã®æ¬ æå€¤è£œå®Œ
    numeric_imputer = SimpleImputer(strategy='median')  # ä¸­å¤®å€¤ã§è£œå®Œ
    X_train[numeric_cols] = numeric_imputer.fit_transform(X_train[numeric_cols])
    X_test[numeric_cols] = numeric_imputer.transform(X_test[numeric_cols])
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ—ã®æ¬ æå€¤è£œå®Œã¨ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    for col in categorical_cols:
        # æœ€é »å€¤ã§æ¬ æå€¤è£œå®Œ
        mode_value = X_train[col].mode()[0]             # æœ€é »å€¤å–å¾—
        X_train[col].fillna(mode_value, inplace=True)   # è¨“ç·´ãƒ‡ãƒ¼ã‚¿è£œå®Œ
        X_test[col].fillna(mode_value, inplace=True)    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿è£œå®Œ
        
        # ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Ÿè¡Œ
        le = LabelEncoder()                             # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼åˆæœŸåŒ–
        X_train[col] = le.fit_transform(X_train[col])   # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        X_test[col] = le.transform(X_test[col])         # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    
    # æ•°å€¤åˆ—ã®æ¨™æº–åŒ–
    scaler = StandardScaler()                                      # æ¨™æº–åŒ–å™¨åˆæœŸåŒ–
    X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])  # è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–
    X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–
    
    return X_train, X_test, y_train
```

---

## ğŸª 4. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æŠ€æ³•ï¼ˆä¸Šä½å…¥è³ã®ç§˜è¨£ï¼‰

### ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ï¼ˆæœ€å¼·ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼‰

```python
from sklearn.model_selection import StratifiedKFold
from sklearn.base import clone

def create_stacking_features(models, X_train, y_train, X_test, n_folds=5):
    """
    ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ç”¨ã®ç‰¹å¾´é‡ã‚’ä½œæˆ
    
    Args:
        models: ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆ
        X_train: è¨“ç·´ç”¨ç‰¹å¾´é‡
        y_train: è¨“ç·´ç”¨ç›®çš„å¤‰æ•°
        X_test: ãƒ†ã‚¹ãƒˆç”¨ç‰¹å¾´é‡
        n_folds: åˆ†å‰²æ•°
    """
    
    # çµæœæ ¼ç´ç”¨é…åˆ—ã‚’åˆæœŸåŒ–
    train_blend = np.zeros((X_train.shape[0], len(models)))  # è¨“ç·´ç”¨ãƒ–ãƒ¬ãƒ³ãƒ‰ç‰¹å¾´é‡
    test_blend = np.zeros((X_test.shape[0], len(models)))    # ãƒ†ã‚¹ãƒˆç”¨ãƒ–ãƒ¬ãƒ³ãƒ‰ç‰¹å¾´é‡
    
    # å±¤åŒ–Kåˆ†å‰²ã‚’ä½œæˆ
    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)
    
    # å„ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦å‡¦ç†
    for i, model in enumerate(models):
        print(f'ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ« {i+1}/{len(models)} ã‚’å‡¦ç†ä¸­...')
        
        test_predictions = []  # ãƒ†ã‚¹ãƒˆäºˆæ¸¬ã‚’æ ¼ç´
        
        # Kåˆ†å‰²ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        for train_idx, val_idx in skf.split(X_train, y_train):
            # åˆ†å‰²ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            X_fold_train = X_train.iloc[train_idx]  # å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿
            y_fold_train = y_train.iloc[train_idx]  # å­¦ç¿’ç”¨ãƒ©ãƒ™ãƒ«
            X_fold_val = X_train.iloc[val_idx]      # æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿
            
            # ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ã—ã¦è¨“ç·´
            fold_model = clone(model)               # ãƒ¢ãƒ‡ãƒ«ã‚’è¤‡è£½
            fold_model.fit(X_fold_train, y_fold_train)  # å­¦ç¿’å®Ÿè¡Œ
            
            # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬ï¼ˆout-of-foldäºˆæ¸¬ï¼‰
            val_pred = fold_model.predict_proba(X_fold_val)[:, 1]  # ç¢ºç‡äºˆæ¸¬
            train_blend[val_idx, i] = val_pred                     # ãƒ–ãƒ¬ãƒ³ãƒ‰ç‰¹å¾´é‡ã«æ ¼ç´
            
            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬
            test_pred = fold_model.predict_proba(X_test)[:, 1]     # ãƒ†ã‚¹ãƒˆäºˆæ¸¬
            test_predictions.append(test_pred)                     # äºˆæ¸¬ãƒªã‚¹ãƒˆã«è¿½åŠ 
        
        # ãƒ†ã‚¹ãƒˆäºˆæ¸¬ã®å¹³å‡ã‚’å–ã‚‹
        test_blend[:, i] = np.mean(test_predictions, axis=0)       # å¹³å‡äºˆæ¸¬å€¤
    
    return train_blend, test_blend

# ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°å®Ÿè¡Œä¾‹
base_models = [
    RandomForestClassifier(n_estimators=100, random_state=42),
    GradientBoostingClassifier(random_state=42),
    SVC(probability=True, random_state=42)
]

# ãƒ–ãƒ¬ãƒ³ãƒ‰ç‰¹å¾´é‡ã‚’ä½œæˆ
train_blend, test_blend = create_stacking_features(base_models, X_train, y_train, X_test)

# ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ï¼ˆç¬¬2å±¤ï¼‰ã§æœ€çµ‚äºˆæ¸¬
meta_model = LogisticRegression(random_state=42)  # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«
meta_model.fit(train_blend, y_train)              # ãƒ–ãƒ¬ãƒ³ãƒ‰ç‰¹å¾´é‡ã§å­¦ç¿’
final_pred = meta_model.predict_proba(test_blend)[:, 1]  # æœ€çµ‚äºˆæ¸¬
```

---

## ğŸ”¢ 5. NumPy é«˜é€ŸåŒ–ã®å¥¥ç¾©

### ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã¨è¨ˆç®—æœ€é©åŒ–

```python
# å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿ã®åŠ¹ç‡çš„å‡¦ç†
def efficient_feature_engineering(data):
    """
    ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’è€ƒæ…®ã—ãŸç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
    """
    
    # ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–ï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰
    for col in data.select_dtypes(include=['int64']).columns:
        col_min = data[col].min()  # åˆ—ã®æœ€å°å€¤
        col_max = data[col].max()  # åˆ—ã®æœ€å¤§å€¤
        
        # é©åˆ‡ãªãƒ‡ãƒ¼ã‚¿å‹ã‚’é¸æŠ
        if col_min >= 0:  # éè² æ•´æ•°ã®å ´åˆ
            if col_max < 255:
                data[col] = data[col].astype(np.uint8)      # 8bitç¬¦å·ãªã—æ•´æ•°
            elif col_max < 65535:
                data[col] = data[col].astype(np.uint16)     # 16bitç¬¦å·ãªã—æ•´æ•°
        else:  # è² æ•°ã‚’å«ã‚€å ´åˆ
            if col_min >= -128 and col_max <= 127:
                data[col] = data[col].astype(np.int8)       # 8bitç¬¦å·ä»˜ãæ•´æ•°
            elif col_min >= -32768 and col_max <= 32767:
                data[col] = data[col].astype(np.int16)      # 16bitç¬¦å·ä»˜ãæ•´æ•°
    
    # float64ã‚’float32ã«å¤‰æ›ï¼ˆãƒ¡ãƒ¢ãƒªåŠåˆ†ï¼‰
    for col in data.select_dtypes(include=['float64']).columns:
        data[col] = data[col].astype(np.float32)
    
    return data

# é«˜é€Ÿçµ±è¨ˆè¨ˆç®—
def fast_rolling_statistics(arr, window=5):
    """
    é«˜é€Ÿãªç§»å‹•çµ±è¨ˆé‡è¨ˆç®—
    """
    # NumPyã®ç•³ã¿è¾¼ã¿ã‚’ä½¿ç”¨ã—ãŸé«˜é€Ÿç§»å‹•å¹³å‡
    kernel = np.ones(window) / window           # ç§»å‹•å¹³å‡ã‚«ãƒ¼ãƒãƒ«
    moving_avg = np.convolve(arr, kernel, mode='valid')  # ç•³ã¿è¾¼ã¿å®Ÿè¡Œ
    
    # ç§»å‹•æ¨™æº–åå·®ï¼ˆåŠ¹ç‡çš„è¨ˆç®—ï¼‰
    arr_squared = arr ** 2                      # äºŒä¹—é…åˆ—
    moving_var = np.convolve(arr_squared, kernel, mode='valid') - moving_avg ** 2  # åˆ†æ•£è¨ˆç®—
    moving_std = np.sqrt(np.maximum(moving_var, 0))  # æ¨™æº–åå·®ï¼ˆè² æ•°å›é¿ï¼‰
    
    return moving_avg, moving_std
```

### ç‰¹å¾´é‡ç”Ÿæˆã®é»„é‡‘ãƒ‘ã‚¿ãƒ¼ãƒ³

```python
def create_winning_features(df):
    """
    Kaggleä¸Šä½é™£ãŒä½¿ã†ç‰¹å¾´é‡ç”Ÿæˆãƒ‘ã‚¿ãƒ¼ãƒ³
    """
    
    # 1. çµ±è¨ˆçš„ç‰¹å¾´é‡
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    
    # è¡Œã”ã¨ã®çµ±è¨ˆé‡
    df['row_mean'] = df[numeric_cols].mean(axis=1)      # è¡Œå¹³å‡
    df['row_std'] = df[numeric_cols].std(axis=1)        # è¡Œæ¨™æº–åå·®
    df['row_max'] = df[numeric_cols].max(axis=1)        # è¡Œæœ€å¤§å€¤
    df['row_min'] = df[numeric_cols].min(axis=1)        # è¡Œæœ€å°å€¤
    df['row_range'] = df['row_max'] - df['row_min']     # è¡Œãƒ¬ãƒ³ã‚¸
    
    # 2. äº¤äº’ä½œç”¨ç‰¹å¾´é‡ï¼ˆé‡è¦ï¼ï¼‰
    important_cols = numeric_cols[:5]  # é‡è¦ãªåˆ—ã‚’é¸æŠ
    for i, col1 in enumerate(important_cols):
        for col2 in important_cols[i+1:]:
            df[f'{col1}_x_{col2}'] = df[col1] * df[col2]        # ç©
            df[f'{col1}_div_{col2}'] = df[col1] / (df[col2] + 1e-8)  # å•†ï¼ˆã‚¼ãƒ­é™¤ç®—å›é¿ï¼‰
            df[f'{col1}_diff_{col2}'] = df[col1] - df[col2]     # å·®
    
    # 3. å¤šé …å¼ç‰¹å¾´é‡
    for col in important_cols:
        df[f'{col}_squared'] = df[col] ** 2                 # äºŒä¹—
        df[f'{col}_cubed'] = df[col] ** 3                   # ä¸‰ä¹—
        df[f'{col}_sqrt'] = np.sqrt(np.abs(df[col]))        # å¹³æ–¹æ ¹
        df[f'{col}_log'] = np.log1p(np.abs(df[col]))        # å¯¾æ•°å¤‰æ›
    
    return df

# ç‰¹å¾´é‡ç”Ÿæˆå®Ÿè¡Œ
enhanced_train = create_winning_features(X_train.copy())
enhanced_test = create_winning_features(X_test.copy())
```

---

## ğŸ… 6. Kaggleæå‡ºç”¨ã‚³ãƒ¼ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

### å®Œç’§ãªæå‡ºã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```python
# å®Œå…¨ãªKaggleã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
def kaggle_solution_template():
    """
    Kaggleä¸Šä½å…¥è³ã®ãŸã‚ã®å®Œå…¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
    """
    
    # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    train = pd.read_csv('/kaggle/input/train.csv')      # è¨“ç·´ãƒ‡ãƒ¼ã‚¿
    test = pd.read_csv('/kaggle/input/test.csv')        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    submission = pd.read_csv('/kaggle/input/sample_submission.csv')  # æå‡ºå½¢å¼
    
    # 2. å‰å‡¦ç†å®Ÿè¡Œ
    X_train, X_test, y_train = kaggle_preprocessing_pipeline(train, test, 'target')
    
    # 3. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
    X_train = create_winning_features(X_train)
    X_test = create_winning_features(X_test)
    
    # 4. ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼‰
    train_blend, test_blend = create_stacking_features(base_models, X_train, y_train, X_test)
    
    # 5. æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬
    final_model = LogisticRegression(random_state=42)
    final_model.fit(train_blend, y_train)
    predictions = final_model.predict_proba(test_blend)[:, 1]
    
    # 6. æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    submission['target'] = predictions                   # äºˆæ¸¬å€¤ã‚’æ ¼ç´
    submission.to_csv('submission.csv', index=False)    # CSVã§ä¿å­˜
    
    print('æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ãŒå®Œæˆã—ã¾ã—ãŸï¼')
    return predictions

# å®Ÿè¡Œ
predictions = kaggle_solution_template()
```

---

## âš¡ ã‚¯ã‚¤ãƒƒã‚¯ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹

### scikit-learnå¿…é ˆã‚³ãƒãƒ³ãƒ‰
```python
# ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
cross_val_score(model, X, y, cv=5)          # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
classification_report(y_true, y_pred)       # åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ
confusion_matrix(y_true, y_pred)           # æ··åŒè¡Œåˆ—

# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–  
GridSearchCV(model, param_grid, cv=5)       # ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ
RandomizedSearchCV(model, param_dist, cv=5) # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒ

# å‰å‡¦ç†
StandardScaler().fit_transform(X)           # æ¨™æº–åŒ–
MinMaxScaler().fit_transform(X)            # æ­£è¦åŒ–
LabelEncoder().fit_transform(y)            # ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
```

### NumPyé«˜é€ŸåŒ–ã‚³ãƒãƒ³ãƒ‰
```python
# é«˜é€Ÿçµ±è¨ˆè¨ˆç®—
np.mean(arr, axis=0)                       # åˆ—ã”ã¨ã®å¹³å‡
np.std(arr, axis=1)                        # è¡Œã”ã¨ã®æ¨™æº–åå·®
np.percentile(arr, [25, 50, 75])           # ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«

# æ¡ä»¶å‡¦ç†
np.where(condition, value_if_true, value_if_false)  # æ¡ä»¶åˆ†å²
np.select([cond1, cond2], [val1, val2])            # è¤‡æ•°æ¡ä»¶

# é…åˆ—æ“ä½œ
np.concatenate([arr1, arr2])               # é…åˆ—çµåˆ
np.vstack([arr1, arr2])                    # ç¸¦æ–¹å‘çµåˆ
np.hstack([arr1, arr2])                    # æ¨ªæ–¹å‘çµåˆ
```

---

## ğŸ¯ å‹åˆ©ã®ãŸã‚ã®æœ€çµ‚ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### âœ… æå‡ºå‰ã®å¿…é ˆç¢ºèªé …ç›®

1. **ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ç¢ºèª**
   - æœªæ¥æƒ…å ±ãŒç‰¹å¾´é‡ã«å«ã¾ã‚Œã¦ã„ãªã„ã‹
   - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æƒ…å ±ãŒè¨“ç·´ã«æ¼ã‚Œã¦ã„ãªã„ã‹

2. **ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š**
   - æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ãªã‚‰æ™‚é–“é †åˆ†å‰²
   - å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§ã‚¯ãƒ©ã‚¹æ¯”ç‡ç¶­æŒ

3. **ç‰¹å¾´é‡é‡è¦åº¦ãƒã‚§ãƒƒã‚¯**
   - ä¸è¦ãªç‰¹å¾´é‡ã¯å‰Šé™¤
   - é‡è¦ç‰¹å¾´é‡ã®çµ„ã¿åˆã‚ã›ã‚’è¿½åŠ 

4. **ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å¤šæ§˜æ€§**
   - ç•°ãªã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®çµ„ã¿åˆã‚ã›
   - ç•°ãªã‚‹ç‰¹å¾´é‡ã‚»ãƒƒãƒˆã§ã®å­¦ç¿’

5. **äºˆæ¸¬å€¤ã®å¾Œå‡¦ç†**
   - ç¢ºç‡å€¤ã®ç¯„å›²ãƒã‚§ãƒƒã‚¯ï¼ˆ0-1ï¼‰
   - ç•°å¸¸å€¤ã®ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°

---

ã“ã®ã‚¬ã‚¤ãƒ‰ã®ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã‚’å®Ÿè·µã™ã‚Œã°ã€Kaggleã§ç¢ºå®Ÿã«ä¸Šä½å…¥è³ã‚’ç‹™ãˆã¾ã™ï¼ğŸ’ª

ã¾ãšã¯å°ã•ãªã‚³ãƒ³ãƒšã§ç·´ç¿’ã—ã¦ã€å¾ã€…ã«é«˜åº¦ãªãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã‚’èº«ã«ã¤ã‘ã¦ã„ãã¾ã—ã‚‡ã†ã€‚é ‘å¼µã£ã¦ãã ã•ã„ï¼ğŸš€
