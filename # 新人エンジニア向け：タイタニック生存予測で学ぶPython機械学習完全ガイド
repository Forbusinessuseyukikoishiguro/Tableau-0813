# 新人エンジニア向け：タイタニック生存予測で学ぶPython機械学習完全ガイド

Kaggleの有名なタイタニック生存予測問題を通じて、scikit-learn、numpy、pandasなどの重要ライブラリを一行一行丁寧に解説します。この記事を読み終える頃には、機械学習の基本的な流れを完全に理解できているはずです！

## 目次
1. [環境準備とライブラリの理解](#1-環境準備とライブラリの理解)
2. [データの読み込みと基本確認](#2-データの読み込みと基本確認)
3. [探索的データ分析（EDA）](#3-探索的データ分析eda)
4. [データ前処理](#4-データ前処理)
5. [機械学習モデルの構築](#5-機械学習モデルの構築)
6. [モデルの評価と改善](#6-モデルの評価と改善)

---

## 1. 環境準備とライブラリの理解

### ライブラリのインポート

```python
# データ操作用ライブラリ
import pandas as pd          # DataFrameでテーブルデータを扱う
import numpy as np           # 数値計算・配列操作用

# 可視化ライブラリ
import matplotlib.pyplot as plt  # グラフ作成の基本ライブラリ
import seaborn as sns           # matplotlibをより美しく・簡単にしたライブラリ

# 機械学習ライブラリ
from sklearn.model_selection import train_test_split  # データを訓練用とテスト用に分割
from sklearn.preprocessing import StandardScaler      # データの標準化（平均0、分散1）
from sklearn.preprocessing import LabelEncoder        # カテゴリデータを数値に変換
from sklearn.linear_model import LogisticRegression   # ロジスティック回帰モデル
from sklearn.ensemble import RandomForestClassifier   # ランダムフォレスト（決定木の集合）
from sklearn.svm import SVC                          # サポートベクターマシン
from sklearn.metrics import accuracy_score           # 精度を計算
from sklearn.metrics import classification_report    # 詳細な分類結果レポート
from sklearn.metrics import confusion_matrix         # 混同行列（予測と実際の比較表）

# 警告メッセージを非表示にする（コード実行時の見た目をすっきりさせる）
import warnings
warnings.filterwarnings('ignore')

print("📚 ライブラリのインポート完了！")
```

### 可視化の設定

```python
# matplotlibの設定
plt.style.use('seaborn-v0_8')    # seabornスタイルを適用（美しいグラフ）
plt.rcParams['figure.figsize'] = (10, 6)  # デフォルトのグラフサイズを設定
plt.rcParams['font.size'] = 12             # フォントサイズを設定

# seabornのカラーパレット設定
sns.set_palette("husl")  # カラフルで見やすい色合いを設定

print("🎨 可視化設定完了！")
```

---

## 2. データの読み込みと基本確認

### データの読み込み

```python
# CSVファイルを読み込んでDataFrameオブジェクトを作成
# DataFrameは表形式のデータを扱うpandasの核となるデータ構造
df = pd.read_csv('titanic.csv')

# 読み込んだデータの最初の5行を表示（データの構造を確認）
print("📄 データの最初の5行:")
print(df.head())
```

### データの基本情報を確認

```python
# データの形状（行数、列数）を取得
# shape属性は(行数, 列数)のタプルを返す
rows, columns = df.shape
print(f"📊 データサイズ: {rows}行 × {columns}列")

# 各列のデータ型と非null値の数を表示
# info()メソッドはデータの概要を一覧表示する
print("\n🔍 データの基本情報:")
df.info()

# 数値データの統計情報を表示
# describe()は平均、標準偏差、四分位数などを計算
print("\n📈 数値データの統計:")
print(df.describe())
```

### 欠損値の確認

```python
# 各列の欠損値（NaN、None）の数をカウント
# isnull()で欠損値をTrue/Falseで表現し、sum()で合計
missing_values = df.isnull().sum()

# 欠損値がある列のみを表示
# missing_values > 0で欠損値がある列を抽出
missing_columns = missing_values[missing_values > 0]

print("❌ 欠損値の確認:")
if len(missing_columns) > 0:
    for column, count in missing_columns.items():
        # 欠損値の数と割合を計算して表示
        percentage = (count / len(df)) * 100
        print(f"  {column}: {count}個 ({percentage:.1f}%)")
else:
    print("  欠損値はありません！")
```

---

## 3. 探索的データ分析（EDA）

### 目的変数の分布確認

```python
# 生存率の確認（0=死亡、1=生存）
# value_counts()で各値の出現回数をカウント
survival_counts = df['Survived'].value_counts()

print("⚰️ 生存状況:")
print(f"  死亡: {survival_counts[0]}人 ({survival_counts[0]/len(df)*100:.1f}%)")
print(f"  生存: {survival_counts[1]}人 ({survival_counts[1]/len(df)*100:.1f}%)")

# 生存率を円グラフで可視化
plt.figure(figsize=(8, 6))  # グラフのサイズを指定
# pie()で円グラフを作成、autopct='%1.1f%%'で割合を表示
plt.pie(survival_counts.values, 
        labels=['死亡', '生存'], 
        autopct='%1.1f%%',        # 割合を表示
        startangle=90,            # 開始角度
        colors=['lightcoral', 'lightblue'])  # 色を指定
plt.title('タイタニック生存率')
plt.axis('equal')  # 円を正円にする
plt.show()
```

### 性別と生存率の関係

```python
# 性別ごとの生存率をクロス集計
# crosstab()で2つの変数の組み合わせをカウント
gender_survival = pd.crosstab(df['Sex'], df['Survived'], margins=True)
print("👫 性別ごとの生存状況:")
print(gender_survival)

# 生存率を百分率で計算
# normalize='index'で行ごとの割合を計算
gender_survival_rate = pd.crosstab(df['Sex'], df['Survived'], normalize='index') * 100
print("\n📊 性別ごとの生存率 (%):")
print(gender_survival_rate.round(1))

# 棒グラフで可視化
plt.figure(figsize=(10, 6))
# groupby()でグループ化し、mean()で平均（生存率）を計算
df.groupby('Sex')['Survived'].mean().plot(kind='bar', 
                                          color=['lightblue', 'pink'],
                                          rot=0)  # rot=0で横軸ラベルを水平に
plt.title('性別ごとの生存率')
plt.ylabel('生存率')
plt.xlabel('性別')
plt.ylim(0, 1)  # y軸の範囲を0-1に設定
# グラフに数値ラベルを追加
for i, v in enumerate(df.groupby('Sex')['Survived'].mean()):
    plt.text(i, v + 0.02, f'{v:.2f}', ha='center')  # ha='center'で中央揃え
plt.show()
```

### 客室クラスと生存率の関係

```python
# 客室クラス（Pclass）ごとの生存率を分析
# 1=ファーストクラス、2=セカンドクラス、3=サードクラス
class_survival = df.groupby('Pclass')['Survived'].agg(['count', 'sum', 'mean'])
class_survival.columns = ['総乗客数', '生存者数', '生存率']

print("🎩 客室クラスごとの生存状況:")
print(class_survival)

# ヒートマップで可視化
plt.figure(figsize=(8, 6))
# pivot_table()でピボットテーブルを作成（行列の組み合わせで集計）
class_sex_survival = df.pivot_table(values='Survived', 
                                   index='Pclass', 
                                   columns='Sex', 
                                   aggfunc='mean')  # 平均値（生存率）で集計

# heatmap()でヒートマップを作成
sns.heatmap(class_sex_survival, 
            annot=True,      # セルに数値を表示
            cmap='RdYlBu',   # カラーマップ（赤→黄→青）
            fmt='.3f',       # 小数点以下3桁で表示
            cbar_kws={'label': '生存率'})  # カラーバーのラベル
plt.title('客室クラス×性別の生存率')
plt.ylabel('客室クラス')
plt.xlabel('性別')
plt.show()
```

### 年齢分布の分析

```python
# 年齢データの基本統計
age_stats = df['Age'].describe()
print("👶 年齢の基本統計:")
print(age_stats)

# 生存者と死亡者の年齢分布を比較
plt.figure(figsize=(12, 6))

# subplot()で複数のグラフを並べて表示（1行2列）
plt.subplot(1, 2, 1)  # 1行目の1列目
# hist()でヒストグラム作成、alpha=0.7で透明度設定
plt.hist(df[df['Survived'] == 0]['Age'].dropna(),  # dropna()で欠損値を除外
         bins=30,           # ビンの数
         alpha=0.7,         # 透明度
         label='死亡',      # 凡例ラベル
         color='red')
plt.hist(df[df['Survived'] == 1]['Age'].dropna(), 
         bins=30, 
         alpha=0.7, 
         label='生存', 
         color='blue')
plt.xlabel('年齢')
plt.ylabel('人数')
plt.title('年齢分布比較')
plt.legend()  # 凡例を表示

# 箱ひげ図での比較
plt.subplot(1, 2, 2)  # 1行目の2列目
# boxplot()で箱ひげ図作成
df.boxplot(column='Age', by='Survived', ax=plt.gca())  # gca()で現在の軸を取得
plt.title('生存状況別年齢分布')
plt.xlabel('生存状況 (0:死亡, 1:生存)')
plt.ylabel('年齢')

plt.tight_layout()  # レイアウトを自動調整
plt.show()

# 年齢層別の生存率
# cut()で連続値を区間に分割
df['AgeGroup'] = pd.cut(df['Age'], 
                       bins=[0, 12, 18, 30, 50, 80],  # 区間の境界
                       labels=['子供', '10代', '青年', '中年', '高齢'])  # ラベル

age_group_survival = df.groupby('AgeGroup')['Survived'].mean()
print("\n👥 年齢層別生存率:")
print(age_group_survival)
```

---

## 4. データ前処理

### 欠損値の処理

```python
print("🔧 データ前処理開始...")

# 作業用にデータをコピー（元データを保護）
df_processed = df.copy()

# 年齢の欠損値を中央値で補完
# median()で中央値を計算、fillna()で欠損値を置換
age_median = df_processed['Age'].median()
df_processed['Age'].fillna(age_median, inplace=True)  # inplace=Trueで元データを変更
print(f"✅ 年齢の欠損値を中央値 {age_median} で補完")

# 乗船港（Embarked）の欠損値を最頻値で補完
# mode()[0]で最頻値を取得（mode()は配列を返すので[0]で最初の値）
embarked_mode = df_processed['Embarked'].mode()[0]
df_processed['Embarked'].fillna(embarked_mode, inplace=True)
print(f"✅ 乗船港の欠損値を最頻値 '{embarked_mode}' で補完")

# Cabin列は欠損値が多すぎるので削除
# drop()で列を削除、axis=1で列方向を指定
df_processed = df_processed.drop('Cabin', axis=1)
print("✅ Cabin列を削除（欠損値が多すぎるため）")

# 処理後の欠損値確認
remaining_missing = df_processed.isnull().sum().sum()  # 全体の欠損値数
print(f"✅ 処理後の欠損値: {remaining_missing}個")
```

### カテゴリ変数のエンコーディング

```python
# 性別を数値に変換（male=1, female=0）
# map()で値を対応表に従って変換
df_processed['Sex'] = df_processed['Sex'].map({'male': 1, 'female': 0})
print("✅ 性別を数値化: male=1, female=0")

# 乗船港を数値に変換
# LabelEncoder()でカテゴリを自動的に数値に変換
le_embarked = LabelEncoder()
df_processed['Embarked'] = le_embarked.fit_transform(df_processed['Embarked'])
print("✅ 乗船港を数値化:", dict(zip(le_embarked.classes_, 
                                   le_embarked.transform(le_embarked.classes_))))

# 不要な列を削除
# Name, Ticket, PassengerIdは予測に直接関係ないので削除
columns_to_drop = ['Name', 'Ticket', 'PassengerId']
df_processed = df_processed.drop(columns_to_drop, axis=1)
print(f"✅ 不要な列を削除: {columns_to_drop}")
```

### 新しい特徴量の作成

```python
# 家族サイズの特徴量を作成
# SibSp（兄弟・配偶者数）+ Parch（親・子供数）+ 1（本人）
df_processed['FamilySize'] = df_processed['SibSp'] + df_processed['Parch'] + 1
print("✅ FamilySize特徴量を作成")

# 一人旅かどうかの特徴量
# FamilySize == 1なら一人旅（True=1, False=0）
df_processed['IsAlone'] = (df_processed['FamilySize'] == 1).astype(int)
print("✅ IsAlone特徴量を作成")

# 運賃を区間分割
# qcut()で等頻度分割（各区間の人数がほぼ同じ）
df_processed['FareGroup'] = pd.qcut(df_processed['Fare'], 
                                   q=4,        # 4つの区間に分割
                                   labels=['Low', 'Medium', 'High', 'VeryHigh'])

# FareGroupを数値に変換
le_fare = LabelEncoder()
df_processed['FareGroup'] = le_fare.fit_transform(df_processed['FareGroup'])
print("✅ FareGroup特徴量を作成")

# 最終的な特徴量を確認
print("\n📋 最終的な特徴量:")
print(df_processed.columns.tolist())
print(f"📊 最終データサイズ: {df_processed.shape}")
```

---

## 5. 機械学習モデルの構築

### データの分割

```python
# 特徴量（X）と目的変数（y）を分離
# drop()でSurvivedを除いた列を特徴量とする
X = df_processed.drop('Survived', axis=1)
y = df_processed['Survived']

print("🎯 特徴量と目的変数を分離:")
print(f"  特徴量の形状: {X.shape}")
print(f"  目的変数の形状: {y.shape}")

# 学習用とテスト用にデータを分割（8:2の比率）
# train_test_split()でランダムに分割
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2,      # テストデータの割合
    random_state=42,    # 乱数シードで結果を再現可能に
    stratify=y          # 目的変数の分布を維持
)

print("📊 データ分割結果:")
print(f"  学習用: {X_train.shape[0]}サンプル")
print(f"  テスト用: {X_test.shape[0]}サンプル")

# 学習用データの生存率を確認
train_survival_rate = y_train.mean()
print(f"  学習用データの生存率: {train_survival_rate:.3f}")
```

### 特徴量の標準化

```python
# 数値特徴量を標準化（平均0、分散1）
# StandardScaler()で標準化オブジェクトを作成
scaler = StandardScaler()

# fit_transform()で学習用データに標準化を適用
# 学習用データの平均・分散を記録し、それに基づいて変換
X_train_scaled = scaler.fit_transform(X_train)

# transform()でテストデータを変換（学習用データの統計を使用）
# テストデータには学習用と同じ変換を適用（データリークを防ぐ）
X_test_scaled = scaler.transform(X_test)

print("⚖️ 特徴量を標準化:")
print(f"  学習前の平均: {X_train.mean().round(2).tolist()}")
print(f"  標準化後の平均: {X_train_scaled.mean(axis=0).round(2).tolist()}")
print(f"  標準化後の標準偏差: {X_train_scaled.std(axis=0).round(2).tolist()}")
```

### 複数モデルの学習と評価

```python
# 複数のモデルを定義
models = {
    'Logistic Regression': LogisticRegression(random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'SVM': SVC(random_state=42)
}

# 各モデルの結果を保存する辞書
model_results = {}

print("🤖 機械学習モデルの学習開始...")

# 各モデルを学習・評価
for model_name, model in models.items():
    print(f"\n--- {model_name} ---")
    
    # モデルによって標準化の必要性が異なる
    if model_name in ['Logistic Regression', 'SVM']:
        # ロジスティック回帰とSVMは標準化が重要
        model.fit(X_train_scaled, y_train)           # 学習
        y_pred = model.predict(X_test_scaled)        # 予測
    else:
        # ランダムフォレストは標準化不要
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
    
    # 精度を計算
    accuracy = accuracy_score(y_test, y_pred)
    
    # 結果を保存
    model_results[model_name] = {
        'model': model,
        'accuracy': accuracy,
        'predictions': y_pred
    }
    
    print(f"✅ 精度: {accuracy:.4f} ({accuracy*100:.1f}%)")

# 最も精度の高いモデルを特定
best_model_name = max(model_results.keys(), 
                     key=lambda x: model_results[x]['accuracy'])
best_accuracy = model_results[best_model_name]['accuracy']

print(f"\n🏆 最高精度モデル: {best_model_name} (精度: {best_accuracy:.4f})")
```

---

## 6. モデルの評価と改善

### 詳細な評価指標

```python
# 最高精度モデルの詳細評価
best_model = model_results[best_model_name]['model']
best_predictions = model_results[best_model_name]['predictions']

print(f"📊 {best_model_name}の詳細評価:")

# 分類レポートを表示
# precision（適合率）、recall（再現率）、f1-score（F値）を表示
classification_rep = classification_report(y_test, best_predictions, 
                                          target_names=['死亡', '生存'])
print("\n分類レポート:")
print(classification_rep)

# 混同行列の作成と可視化
cm = confusion_matrix(y_test, best_predictions)
plt.figure(figsize=(8, 6))

# heatmap()で混同行列を可視化
sns.heatmap(cm, 
            annot=True,          # セルに数値表示
            fmt='d',             # 整数表示
            cmap='Blues',        # 青色系のカラーマップ
            xticklabels=['死亡', '生存'],    # x軸ラベル
            yticklabels=['死亡', '生存'])    # y軸ラベル

plt.title(f'{best_model_name} - 混同行列')
plt.xlabel('予測値')
plt.ylabel('実際値')
plt.show()

# 混同行列の解釈
tn, fp, fn, tp = cm.ravel()  # 混同行列を1次元配列に変換
print(f"\n混同行列の解釈:")
print(f"  真陰性 (TN): {tn} - 正しく死亡と予測")
print(f"  偽陽性 (FP): {fp} - 誤って生存と予測")
print(f"  偽陰性 (FN): {fn} - 誤って死亡と予測")
print(f"  真陽性 (TP): {tp} - 正しく生存と予測")
```

### 特徴量重要度の分析

```python
# Random Forestの特徴量重要度を可視化
if best_model_name == 'Random Forest':
    # feature_importances_属性で特徴量重要度を取得
    feature_importance = best_model.feature_importances_
    feature_names = X.columns
    
    # 重要度順にソート
    # argsort()でインデックスを取得し、[::-1]で降順に
    indices = np.argsort(feature_importance)[::-1]
    
    plt.figure(figsize=(10, 6))
    # bar()で棒グラフ作成
    plt.bar(range(len(feature_importance)), 
            feature_importance[indices],
            color='lightblue')
    
    # x軸のラベルを設定
    plt.xticks(range(len(feature_importance)), 
               [feature_names[i] for i in indices], 
               rotation=45)           # 45度回転
    
    plt.title('特徴量重要度 (Random Forest)')
    plt.xlabel('特徴量')
    plt.ylabel('重要度')
    plt.tight_layout()
    plt.show()
    
    # 重要度を数値で表示
    print("📈 特徴量重要度ランキング:")
    for i, idx in enumerate(indices):
        print(f"  {i+1:2d}. {feature_names[idx]:12s}: {feature_importance[idx]:.4f}")
```

### モデル性能の比較可視化

```python
# 全モデルの精度を比較
model_names = list(model_results.keys())
accuracies = [model_results[name]['accuracy'] for name in model_names]

plt.figure(figsize=(10, 6))
# bar()で棒グラフ作成、最高精度のモデルを強調
colors = ['gold' if acc == max(accuracies) else 'lightblue' for acc in accuracies]
bars = plt.bar(model_names, accuracies, color=colors)

# 各棒グラフの上に精度値を表示
for bar, acc in zip(bars, accuracies):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
             f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')

plt.title('モデル別予測精度比較')
plt.xlabel('モデル')
plt.ylabel('精度')
plt.ylim(0, 1)              # y軸を0-1に固定
plt.xticks(rotation=15)     # x軸ラベルを15度回転
plt.tight_layout()
plt.show()

# 精度改善のための提案
print("\n💡 さらなる精度向上のためのアイデア:")
print("1. ハイパーパラメータチューニング（GridSearchCV）")
print("2. 特徴量エンジニアリング（新しい特徴量の作成）")
print("3. アンサンブル学習（複数モデルの組み合わせ）")
print("4. 交差検証による安定性確認")
print("5. より多くのモデル（XGBoost、LightGBMなど）の試行")
```

### 実際の予測例

```python
# テストデータから数件の予測例を表示
print("🎭 実際の予測例:")
print("-" * 50)

# 最初の10件について予測と実際を比較
for i in range(min(10, len(y_test))):
    actual = y_test.iloc[i]           # 実際の値
    predicted = best_predictions[i]    # 予測値
    
    # 予測が正しいかどうかを判定
    status = "✅ 正解" if actual == predicted else "❌ 不正解"
    
    # 結果を表示
    actual_label = "生存" if actual == 1 else "死亡"
    predicted_label = "生存" if predicted == 1 else "死亡"
    
    print(f"サンプル{i+1:2d}: 実際={actual_label}, 予測={predicted_label} {status}")

# 全体の正解率を再確認
correct_predictions = (y_test == best_predictions).sum()
total_predictions = len(y_test)
overall_accuracy = correct_predictions / total_predictions

print(f"\n📊 全体結果: {correct_predictions}/{total_predictions} = {overall_accuracy:.3f}")
```

---

## まとめ：学習内容の振り返り

この記事を通じて学習した技術とライブラリ：

### 🐼 **Pandas**
- `read_csv()`: CSVファイルの読み込み
- `head()`, `info()`, `describe()`: データの基本確認
- `isnull()`, `fillna()`: 欠損値の処理
- `groupby()`, `crosstab()`: データの集計・分析
- `map()`, `cut()`, `qcut()`: データ変換

### 🔢 **NumPy**
- `np.array`: 配列操作
- `np.argsort()`: ソートインデックス取得
- 数値計算の基盤として活用

### 🎨 **Matplotlib & Seaborn**
- `plt.figure()`, `plt.subplot()`: グラフ領域設定
- `plt.bar()`, `plt.hist()`, `plt.pie()`: 各種グラフ作成
- `sns.heatmap()`, `sns.boxplot()`: 高度な可視化

### 🤖 **Scikit-learn**
- `train_test_split()`: データ分割
- `StandardScaler()`: データ標準化
- `LabelEncoder()`: カテゴリ変数エンコーディング
- `LogisticRegression()`, `RandomForestClassifier()`, `SVC()`: 各種機械学習モデル
- `accuracy_score()`, `classification_report()`, `confusion_matrix()`: モデル評価

---

## 🚀 次のステップ：さらなるスキルアップ

### レベル1：基本の復習
```python
# 今回学んだコードを自分のデータで試してみる
# 1. 自分でCSVファイルを用意
# 2. 同じ手順でデータ分析を実行
# 3. 結果を解釈してみる

# 練習用データセット例
practice_datasets = [
    "iris.csv",           # アイリス分類
    "wine.csv",           # ワイン品質予測
    "diabetes.csv",       # 糖尿病予測
    "heart_disease.csv"   # 心疾患予測
]

print("📚 練習におすすめのデータセット:")
for i, dataset in enumerate(practice_datasets, 1):
    print(f"  {i}. {dataset}")
```

### レベル2：高度な技術に挑戦
```python
# ハイパーパラメータチューニング
from sklearn.model_selection import GridSearchCV

def advanced_model_tuning():
    """高度なモデルチューニングの例"""
    
    # Random Forestのパラメータ候補
    param_grid = {
        'n_estimators': [100, 200, 300],      # 木の数
        'max_depth': [3, 5, 7, None],         # 木の深さ
        'min_samples_split': [2, 5, 10],      # 分割最小サンプル数
        'min_samples_leaf': [1, 2, 4]         # 葉の最小サンプル数
    }
    
    # GridSearchCVでベストパラメータを探索
    rf = RandomForestClassifier(random_state=42)
    grid_search = GridSearchCV(
        rf,                    # モデル
        param_grid,           # パラメータ候補
        cv=5,                 # 5分割交差検証
        scoring='accuracy',   # 評価指標
        n_jobs=-1            # 並列処理
    )
    
    # 標準化済みデータで学習
    grid_search.fit(X_train, y_train)
    
    print("🔍 ハイパーパラメータチューニング結果:")
    print(f"最適パラメータ: {grid_search.best_params_}")
    print(f"最高スコア: {grid_search.best_score_:.4f}")
    
    return grid_search.best_estimator_

# 実行例（時間がかかる場合があります）
# best_tuned_model = advanced_model_tuning()
```

### レベル3：アンサンブル学習
```python
from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import GradientBoostingClassifier
import xgboost as xgb

def ensemble_learning():
    """複数モデルを組み合わせたアンサンブル学習"""
    
    # 複数の異なるモデルを準備
    models = [
        ('lr', LogisticRegression(random_state=42)),
        ('rf', RandomForestClassifier(random_state=42)),
        ('gb', GradientBoostingClassifier(random_state=42)),
        ('xgb', xgb.XGBClassifier(random_state=42))
    ]
    
    # VotingClassifierで多数決予測
    ensemble = VotingClassifier(
        estimators=models,
        voting='soft'      # 確率ベースの投票
    )
    
    # アンサンブルモデルを学習
    ensemble.fit(X_train_scaled, y_train)
    
    # 予測と評価
    ensemble_pred = ensemble.predict(X_test_scaled)
    ensemble_accuracy = accuracy_score(y_test, ensemble_pred)
    
    print("🎯 アンサンブル学習結果:")
    print(f"アンサンブル精度: {ensemble_accuracy:.4f}")
    
    # 個別モデルとの比較
    for name, model in models:
        model.fit(X_train_scaled, y_train)
        pred = model.predict(X_test_scaled)
        acc = accuracy_score(y_test, pred)
        print(f"{name}単体精度: {acc:.4f}")
    
    return ensemble

# 実行例
# ensemble_model = ensemble_learning()
```

### レベル4：特徴量エンジニアリング
```python
def advanced_feature_engineering(df):
    """高度な特徴量エンジニアリング"""
    
    df_advanced = df.copy()
    
    # 1. タイトル抽出（Mr, Mrs, Miss等）
    df_advanced['Title'] = df_advanced['Name'].str.extract(r' ([A-Za-z]+)\.')
    
    # 稀なタイトルをOtherにまとめる
    title_counts = df_advanced['Title'].value_counts()
    rare_titles = title_counts[title_counts < 10].index
    df_advanced['Title'] = df_advanced['Title'].replace(rare_titles, 'Other')
    
    # 2. 客室デッキ情報（Cabinの最初の文字）
    df_advanced['Deck'] = df_advanced['Cabin'].str[0]
    df_advanced['Deck'].fillna('Unknown', inplace=True)
    
    # 3. 運賃の相対値（クラス内での位置）
    df_advanced['Fare_Per_Person'] = df_advanced['Fare'] / df_advanced['FamilySize']
    
    # 4. 年齢グループの詳細分類
    df_advanced['Age_Group_Detail'] = pd.cut(
        df_advanced['Age'],
        bins=[0, 5, 12, 18, 25, 35, 50, 65, 100],
        labels=['Infant', 'Child', 'Teen', 'Young_Adult', 
                'Adult', 'Middle_Age', 'Senior', 'Elderly']
    )
    
    # 5. 家族サイズカテゴリ
    df_advanced['Family_Type'] = df_advanced['FamilySize'].map({
        1: 'Single',
        2: 'Couple', 
        3: 'Small_Family',
        4: 'Medium_Family'
    })
    df_advanced['Family_Type'].fillna('Large_Family', inplace=True)
    
    print("🛠️ 高度な特徴量エンジニアリング完了:")
    print(f"新しい特徴量: {['Title', 'Deck', 'Fare_Per_Person', 'Age_Group_Detail', 'Family_Type']}")
    
    return df_advanced

# 使用例
# df_with_advanced_features = advanced_feature_engineering(df)
```

---

## 🎯 実践プロジェクト提案

### プロジェクト1：カスタムタイタニック分析
```python
def custom_titanic_project():
    """オリジナルのタイタニック分析プロジェクト"""
    
    project_ideas = [
        "1. 生存率を95%以上にするための条件分析",
        "2. 最も危険だった客室エリアの特定",
        "3. 家族構成が生存に与えた影響の詳細分析",
        "4. 運賃と生存率の関係から見る階級社会分析",
        "5. 年齢別避難行動パターンの推定"
    ]
    
    print("🎭 オリジナル分析アイデア:")
    for idea in project_ideas:
        print(f"  {idea}")
    
    return project_ideas

custom_titanic_project()
```

### プロジェクト2：他のデータセットへの応用
```python
def apply_to_new_datasets():
    """学んだ技術を他のデータセットに応用"""
    
    application_areas = {
        "ビジネス": [
            "顧客離反予測（Customer Churn）",
            "売上予測（Sales Forecasting）", 
            "価格最適化（Price Optimization）"
        ],
        "医療": [
            "疾患診断支援（Disease Diagnosis）",
            "薬効予測（Drug Efficacy）",
            "治療効果分析（Treatment Analysis）"
        ],
        "金融": [
            "信用リスク評価（Credit Risk）",
            "株価予測（Stock Prediction）",
            "不正検知（Fraud Detection）"
        ],
        "技術": [
            "システム障害予測（System Failure）",
            "品質管理（Quality Control）",
            "ユーザー行動分析（User Behavior）"
        ]
    }
    
    print("🌐 応用可能な分野:")
    for field, applications in application_areas.items():
        print(f"\n{field}分野:")
        for app in applications:
            print(f"  • {app}")
    
    return application_areas

apply_to_new_datasets()
```

---

## 📚 さらなる学習リソース

### 必読書籍
```python
recommended_books = [
    {
        "タイトル": "Pythonではじめる機械学習",
        "著者": "Andreas C. Müller, Sarah Guido",
        "レベル": "初級〜中級",
        "理由": "scikit-learnの公式的な解説書"
    },
    {
        "タイトル": "前処理大全",
        "著者": "本橋智光",
        "レベル": "中級",
        "理由": "データ前処理の実践的テクニック満載"
    },
    {
        "タイトル": "機械学習のエッセンス",
        "著者": "加藤公一",
        "レベル": "中級〜上級",
        "理由": "アルゴリズムの数学的背景を理解"
    }
]

print("📖 おすすめ書籍:")
for book in recommended_books:
    print(f"\n『{book['タイトル』}』")
    print(f"  著者: {book['著者']}")
    print(f"  レベル: {book['レベル']}")
    print(f"  推薦理由: {book['理由']}")
```

### オンライン学習プラットフォーム
```python
online_platforms = [
    {
        "プラットフォーム": "Kaggle Learn",
        "URL": "https://www.kaggle.com/learn",
        "特徴": "無料、実践的、短時間で完了",
        "おすすめコース": ["Intro to Machine Learning", "Pandas", "Data Visualization"]
    },
    {
        "プラットフォーム": "Coursera",
        "URL": "https://www.coursera.org",
        "特徴": "大学レベルの講義、認定証あり",
        "おすすめコース": ["Machine Learning Course (Andrew Ng)", "Python for Data Science"]
    },
    {
        "プラットフォーム": "Udemy", 
        "URL": "https://www.udemy.com",
        "特徴": "実践的プロジェクト中心、日本語コースも豊富",
        "おすすめコース": ["Python for Data Science and Machine Learning"]
    }
]

print("🌐 オンライン学習プラットフォーム:")
for platform in online_platforms:
    print(f"\n{platform['プラットフォーム']}:")
    print(f"  特徴: {platform['特徴']}")
    print(f"  おすすめ: {', '.join(platform['おすすめコース'])}")
```

---

## 🏁 最終チェックリスト

この記事を完了したあなたができるようになったこと：

### ✅ **基本スキル**
- [ ] pandasでCSVファイルを読み込み、基本的なデータ操作ができる
- [ ] numpyで数値計算と配列操作ができる  
- [ ] matplotlib/seabornでデータを可視化できる
- [ ] scikit-learnで機械学習モデルを構築できる

### ✅ **データ分析スキル**
- [ ] 欠損値を適切に処理できる
- [ ] カテゴリ変数を数値に変換できる
- [ ] 探索的データ分析（EDA）を実行できる
- [ ] 新しい特徴量を作成できる

### ✅ **機械学習スキル**
- [ ] データを適切に分割できる
- [ ] 複数のモデルを比較評価できる
- [ ] モデルの性能を様々な指標で評価できる
- [ ] 結果を解釈して改善提案ができる

### 🎯 **次の目標設定**
```python
def set_next_goals():
    """次の学習目標を設定"""
    
    next_goals = [
        "🥇 短期目標（1ヶ月）: Kaggleコンペに参加してみる",
        "🥈 中期目標（3ヶ月）: 自分の業務データで機械学習を適用",
        "🥉 長期目標（6ヶ月）: オリジナルのデータ分析プロジェクトを完成させる"
    ]
    
    print("🎯 あなたの次の目標:")
    for goal in next_goals:
        print(f"  {goal}")
    
    return next_goals

set_next_goals()
```

---

## 🎉 おめでとうございます！

この記事を最後まで読み、実際にコードを動かしたあなたは、もうデータサイエンティストの入り口に立っています。

**重要なポイント：**
- **継続が最も重要**：毎日少しずつでもコードを書く習慣を
- **実践あるのみ**：理論より実際に手を動かすことが大切
- **コミュニティ参加**：Kaggle、GitHub、技術ブログで他の人と交流
- **質問を恐れない**：分からないことは積極的に調べ、質問する

データサイエンスの世界へようこそ！あなたの分析で世界をより良くしていきましょう！

---

*📧 質問や感想があれば、ぜひコメントやSNSでシェアしてください。一緒に学び続けていきましょう！*

```python
print("🚀 Happy Coding & Data Science! 🚀")
```
