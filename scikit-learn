# scikit-learnå®Œå…¨ã‚¬ã‚¤ãƒ‰ - æ–°äººã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å‘ã‘

## ğŸ¯ ã“ã®è¨˜äº‹ã§èº«ã«ã¤ãã‚¹ã‚­ãƒ«

âœ… **scikit-learnã®åŸºæœ¬æ¦‚å¿µã¨ä½¿ã„æ–¹**  
âœ… **å®Ÿéš›ã®æ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å®Ÿè£…**  
âœ… **ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‹ã‚‰äºˆæ¸¬ã¾ã§ä¸€é€£ã®æµã‚Œ**  
âœ… **ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã®å¯¾å‡¦æ³•**  

---

## ğŸ“š scikit-learnã£ã¦ä½•ï¼Ÿ

### ğŸ¤– ä¸€è¨€ã§èª¬æ˜ã™ã‚‹ã¨

> **Pythonç”¨ã®æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€‚åˆå¿ƒè€…ã§ã‚‚ç°¡å˜ã«æœ¬æ ¼çš„ãªæ©Ÿæ¢°å­¦ç¿’ãŒã§ãã‚‹æœ€å¼·ãƒ„ãƒ¼ãƒ«**

### ğŸŒŸ ãªãœscikit-learnãŒäººæ°—ãªã®ã‹ï¼Ÿ

| ç†ç”± | è©³ç´° | åˆå¿ƒè€…ã¸ã®ä¾¡å€¤ |
|------|------|---------------|
| **ç°¡å˜** | 3è¡Œã§ãƒ¢ãƒ‡ãƒ«ä½œæˆå¯èƒ½ | ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°åˆå¿ƒè€…ã§ã‚‚ã™ãä½¿ãˆã‚‹ |
| **çµ±ä¸€API** | å…¨ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒåŒã˜ä½¿ã„æ–¹ | è¦šãˆã‚‹ã“ã¨ãŒå°‘ãªã„ |
| **è±Šå¯Œãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ** | åˆ†é¡ãƒ»å›å¸°ãƒ»ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å…¨ã¦ | 1ã¤ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ä½•ã§ã‚‚ã§ãã‚‹ |
| **ç„¡æ–™** | å®Œå…¨ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ | å­¦ç¿’ã‚³ã‚¹ãƒˆ0å†† |
| **æ—¥æœ¬èªæƒ…å ±è±Šå¯Œ** | è§£èª¬è¨˜äº‹ãƒ»æ›¸ç±ãŒå¤šæ•° | å›°ã£ãŸæ™‚ã«æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã‚„ã™ã„ |

### ğŸ¢ å®Ÿéš›ã®ç¾å ´ã§ã®ä½¿ã‚ã‚Œæ–¹

```
ğŸ¥ åŒ»ç™‚ï¼šç—…æ°—ã®è¨ºæ–­æ”¯æ´ã‚·ã‚¹ãƒ†ãƒ 
ğŸ¦ é‡‘èï¼šã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ã®ä¸æ­£åˆ©ç”¨æ¤œå‡º  
ğŸ›’ ECï¼šå•†å“æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ 
ğŸš— è£½é€ ï¼šå“è³ªç®¡ç†ãƒ»ç•°å¸¸æ¤œçŸ¥
ğŸ“± ITï¼šãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•åˆ†æ
```

---

## ğŸš€ ç’°å¢ƒæ§‹ç¯‰ï¼ˆ15åˆ†ã§å®Œäº†ï¼‰

### æ–¹æ³•1ï¼šGoogle Colabï¼ˆæœ€æ¨å¥¨ãƒ»åˆå¿ƒè€…å‘ã‘ï¼‰

```python
# Google Colabãªã‚‰æœ€åˆã‹ã‚‰ä½¿ãˆã‚‹ï¼
# https://colab.research.google.com/ ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã ã‘

# ç¢ºèªã‚³ãƒ¼ãƒ‰
import sklearn
print(f"scikit-learn version: {sklearn.__version__}")
print("âœ… ç’°å¢ƒæ§‹ç¯‰å®Œäº†ï¼")
```

**ãƒ¡ãƒªãƒƒãƒˆï¼š**
- è¨­å®šä¸è¦ã€ãƒ–ãƒ©ã‚¦ã‚¶ã ã‘ã§OK
- ç„¡æ–™ã§GPUä½¿ç”¨å¯èƒ½
- è‡ªå‹•ä¿å­˜æ©Ÿèƒ½ä»˜ã

### æ–¹æ³•2ï¼šãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒï¼ˆAnacondaä½¿ç”¨ï¼‰

#### Step 1: Anacondaã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

1. **[Anacondaå…¬å¼ã‚µã‚¤ãƒˆ](https://www.anaconda.com/products/distribution)ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**
2. **ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ©ãƒ¼ã‚’å®Ÿè¡Œï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§OKï¼‰**
3. **Anaconda Promptã‚’èµ·å‹•**

#### Step 2: ä»®æƒ³ç’°å¢ƒä½œæˆ

```bash
# ä»®æƒ³ç’°å¢ƒä½œæˆï¼ˆPython 3.9æŒ‡å®šï¼‰
conda create -n ml_env python=3.9

# ä»®æƒ³ç’°å¢ƒæœ‰åŠ¹åŒ–
conda activate ml_env

# scikit-learnã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
conda install scikit-learn pandas numpy matplotlib seaborn jupyter

# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª
python -c "import sklearn; print(sklearn.__version__)"
```

#### Step 3: Jupyter Notebookèµ·å‹•

```bash
# Jupyter Notebookèµ·å‹•
jupyter notebook

# ãƒ–ãƒ©ã‚¦ã‚¶ãŒè‡ªå‹•ã§é–‹ã
# æ–°ã—ã„Notebookã‚’ä½œæˆã—ã¦é–‹å§‹ï¼
```

---

## ğŸ“Š scikit-learnã®åŸºæœ¬æ§‹é€ 

### ğŸ—‚ï¸ ä¸»è¦ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

```python
from sklearn import datasets          # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
from sklearn import model_selection   # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ãƒ»æ¤œè¨¼
from sklearn import preprocessing     # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
from sklearn import linear_model      # ç·šå½¢ãƒ¢ãƒ‡ãƒ«ï¼ˆå›å¸°ãƒ»åˆ†é¡ï¼‰
from sklearn import ensemble         # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•
from sklearn import tree             # æ±ºå®šæœ¨
from sklearn import svm              # ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ãƒã‚·ãƒ³
from sklearn import neighbors        # kè¿‘å‚æ³•
from sklearn import cluster          # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
from sklearn import metrics          # è©•ä¾¡æŒ‡æ¨™
```

### ğŸ¯ çµ±ä¸€ã•ã‚ŒãŸAPIï¼ˆè¦šãˆã‚‹ã®ã¯ã“ã‚Œã ã‘ï¼ï¼‰

```python
# å…¨ã¦ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒåŒã˜ä½¿ã„æ–¹ï¼

# 1. ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from sklearn.ensemble import RandomForestClassifier

# 2. ãƒ¢ãƒ‡ãƒ«ä½œæˆ
model = RandomForestClassifier()

# 3. å­¦ç¿’
model.fit(X_train, y_train)

# 4. äºˆæ¸¬
predictions = model.predict(X_test)

# 5. è©•ä¾¡
score = model.score(X_test, y_test)
```

---

## ğŸ“ ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«1ï¼šã‚¢ã‚¤ãƒªã‚¹èŠ±åˆ†é¡ï¼ˆ30åˆ†ï¼‰

### ğŸ“š ã‚¢ã‚¤ãƒªã‚¹åˆ†é¡ã¨ã¯ï¼Ÿ

> èŠ±ã®ã€ŒãŒãç‰‡ã®é•·ã•ãƒ»å¹…ã€ã¨ã€ŒèŠ±ã³ã‚‰ã®é•·ã•ãƒ»å¹…ã€ã‹ã‚‰ã€3ç¨®é¡ã®ã‚¢ã‚¤ãƒªã‚¹ï¼ˆsetosa, versicolor, virginicaï¼‰ã‚’äºˆæ¸¬ã™ã‚‹å•é¡Œ

**ãªãœåˆå­¦è€…ã«äººæ°—ï¼Ÿ**
- ãƒ‡ãƒ¼ã‚¿ãŒå°ã•ã„ï¼ˆ150ã‚µãƒ³ãƒ—ãƒ«ï¼‰
- ç‰¹å¾´é‡ãŒ4ã¤ã ã‘ã§ç†è§£ã—ã‚„ã™ã„
- åˆ†é¡ç²¾åº¦ãŒé«˜ãã€æˆåŠŸä½“é¨“ã‚’å¾—ã‚„ã™ã„

### ğŸ” Step 1: ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨ç¢ºèª

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import datasets

# ã‚¢ã‚¤ãƒªã‚¹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
iris = datasets.load_iris()

print("ğŸŒ¸ ã‚¢ã‚¤ãƒªã‚¹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†æé–‹å§‹ï¼")
print(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {iris.data.shape}")
print(f"ç‰¹å¾´é‡: {iris.feature_names}")
print(f"ã‚¯ãƒ©ã‚¹: {iris.target_names}")

# DataFrameã«å¤‰æ›ï¼ˆè¦‹ã‚„ã™ãã™ã‚‹ãŸã‚ï¼‰
df = pd.DataFrame(iris.data, columns=iris.feature_names)
df['species'] = iris.target_names[iris.target]

print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ã®æœ€åˆã®5è¡Œ:")
print(df.head())

print("\nğŸ“ˆ åŸºæœ¬çµ±è¨ˆé‡:")
print(df.describe())
```

### ğŸ“Š Step 2: ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–

```python
# ã‚°ãƒ©ãƒ•è¨­å®š
plt.style.use('default')
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# 1. èŠ±ã³ã‚‰ã®é•·ã• vs å¹…
sns.scatterplot(data=df, x='petal length (cm)', y='petal width (cm)', 
                hue='species', ax=axes[0,0])
axes[0,0].set_title('èŠ±ã³ã‚‰ï¼šé•·ã• vs å¹…')

# 2. ãŒãç‰‡ã®é•·ã• vs å¹…  
sns.scatterplot(data=df, x='sepal length (cm)', y='sepal width (cm)', 
                hue='species', ax=axes[0,1])
axes[0,1].set_title('ãŒãç‰‡ï¼šé•·ã• vs å¹…')

# 3. ç‰¹å¾´é‡ã®åˆ†å¸ƒ
df.boxplot(column='petal length (cm)', by='species', ax=axes[1,0])
axes[1,0].set_title('èŠ±ã³ã‚‰ã®é•·ã•åˆ†å¸ƒ')

# 4. ç›¸é–¢è¡Œåˆ—
correlation = df.select_dtypes(include=[np.number]).corr()
sns.heatmap(correlation, annot=True, ax=axes[1,1])
axes[1,1].set_title('ç‰¹å¾´é‡é–“ã®ç›¸é–¢')

plt.tight_layout()
plt.show()

print("ğŸ’¡ é‡è¦ãªç™ºè¦‹:")
print("1. èŠ±ã³ã‚‰ã®ç‰¹å¾´ï¼ˆé•·ã•ãƒ»å¹…ï¼‰ã§ã‚¯ãƒ©ã‚¹ãŒã‚ˆãåˆ†ã‹ã‚Œã¦ã„ã‚‹")
print("2. setosaã¯ä»–ã®2ç¨®é¡ã¨æ˜ç¢ºã«åŒºåˆ¥ã§ãã‚‹")
print("3. versicolorã¨virginicaã¯å°‘ã—é‡è¤‡ã—ã¦ã„ã‚‹")
```

### ğŸ”§ Step 3: ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†

```python
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«åˆ†é›¢
X = iris.data  # ç‰¹å¾´é‡ï¼ˆ4ã¤ã®æ¸¬å®šå€¤ï¼‰
y = iris.target  # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆ0, 1, 2ã®3ã‚¯ãƒ©ã‚¹ï¼‰

print(f"ç‰¹å¾´é‡ã®å½¢çŠ¶: {X.shape}")
print(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®å½¢çŠ¶: {y.shape}")
print(f"ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: {np.bincount(y)}")

# ãƒ‡ãƒ¼ã‚¿ã‚’è¨“ç·´ç”¨ã¨ãƒ†ã‚¹ãƒˆç”¨ã«åˆ†å‰²ï¼ˆ8:2ã®æ¯”ç‡ï¼‰
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2,      # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å‰²åˆ
    random_state=42,    # å†ç¾æ€§ã®ãŸã‚
    stratify=y          # å„ã‚¯ãƒ©ã‚¹ã®æ¯”ç‡ã‚’ä¿æŒ
)

print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰²çµæœ:")
print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {X_train.shape[0]}ã‚µãƒ³ãƒ—ãƒ«")
print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {X_test.shape[0]}ã‚µãƒ³ãƒ—ãƒ«")

# ç‰¹å¾´é‡ã®æ­£è¦åŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼šç·šå½¢æ‰‹æ³•ã§æ¨å¥¨ï¼‰
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("âœ… å‰å‡¦ç†å®Œäº†ï¼")
```

### ğŸ¤– Step 4: è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬

```python
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report

# è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã—ã¦ã¿ã‚‹
models = {
    'ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°': LogisticRegression(random_state=42),
    'æ±ºå®šæœ¨': DecisionTreeClassifier(random_state=42),
    'ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ': RandomForestClassifier(random_state=42),
    'SVM': SVC(random_state=42),
    'kè¿‘å‚æ³•': KNeighborsClassifier()
}

print("ğŸ¤– å„ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½æ¯”è¼ƒ:")
print("-" * 50)

results = {}
for name, model in models.items():
    # å­¦ç¿’
    if name in ['ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°', 'SVM']:
        # ç·šå½¢æ‰‹æ³•ã¯æ­£è¦åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
        model.fit(X_train_scaled, y_train)
        predictions = model.predict(X_test_scaled)
    else:
        # æœ¨ç³»æ‰‹æ³•ã¯å…ƒãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
        model.fit(X_train, y_train)
        predictions = model.predict(X_test)
    
    # è©•ä¾¡
    accuracy = accuracy_score(y_test, predictions)
    results[name] = accuracy
    
    print(f"{name:15s}: {accuracy:.4f} ({accuracy*100:.1f}%)")

# æœ€é«˜æ€§èƒ½ã®ãƒ¢ãƒ‡ãƒ«
best_model = max(results, key=results.get)
print(f"\nğŸ† æœ€é«˜æ€§èƒ½: {best_model} ({results[best_model]:.4f})")
```

### ğŸ“Š Step 5: è©³ç´°ãªè©•ä¾¡

```python
from sklearn.metrics import confusion_matrix
import seaborn as sns

# æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã§è©³ç´°è©•ä¾¡
best_classifier = RandomForestClassifier(random_state=42)
best_classifier.fit(X_train, y_train)
predictions = best_classifier.predict(X_test)

# æ··åŒè¡Œåˆ—
cm = confusion_matrix(y_test, predictions)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=iris.target_names,
            yticklabels=iris.target_names)
plt.title('æ··åŒè¡Œåˆ—ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆï¼‰')
plt.ylabel('å®Ÿéš›ã®ã‚¯ãƒ©ã‚¹')
plt.xlabel('äºˆæ¸¬ã‚¯ãƒ©ã‚¹')
plt.show()

# è©³ç´°ãªåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ
print("ğŸ“‹ è©³ç´°ãªåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
print(classification_report(y_test, predictions, 
                          target_names=iris.target_names))

# ç‰¹å¾´é‡é‡è¦åº¦
feature_importance = pd.DataFrame({
    'feature': iris.feature_names,
    'importance': best_classifier.feature_importances_
}).sort_values('importance', ascending=False)

print("\nğŸ” ç‰¹å¾´é‡é‡è¦åº¦:")
print(feature_importance)
```

---

## ğŸš¢ ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«2ï¼šã‚¿ã‚¤ã‚¿ãƒ‹ãƒƒã‚¯ç”Ÿå­˜äºˆæ¸¬ï¼ˆ60åˆ†ï¼‰

### ğŸ¯ å®Ÿè·µçš„ãªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ

> ã‚ˆã‚Šè¤‡é›‘ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã€å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½“é¨“

### ğŸ“¥ Step 1: ãƒ‡ãƒ¼ã‚¿å–å¾—ã¨åŸºæœ¬ç¢ºèª

```python
# ã‚¿ã‚¤ã‚¿ãƒ‹ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆä»®æƒ³ãƒ‡ãƒ¼ã‚¿ä½œæˆï¼‰
import numpy as np
import pandas as pd

# å®Ÿè·µçš„ãªã‚¿ã‚¤ã‚¿ãƒ‹ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
# å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
# train_df = pd.read_csv('train.csv')

# ãƒ‡ãƒ¢ç”¨ãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆå®Ÿéš›ã®Kaggleãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’æ¨¡æ“¬ï¼‰
np.random.seed(42)
n_samples = 891

demo_data = {
    'PassengerId': range(1, n_samples + 1),
    'Survived': np.random.choice([0, 1], n_samples, p=[0.62, 0.38]),
    'Pclass': np.random.choice([1, 2, 3], n_samples, p=[0.24, 0.21, 0.55]),
    'Sex': np.random.choice(['male', 'female'], n_samples, p=[0.65, 0.35]),
    'Age': np.random.normal(29, 12, n_samples).clip(0, 80),
    'SibSp': np.random.choice([0, 1, 2, 3, 4, 5], n_samples, p=[0.68, 0.23, 0.05, 0.02, 0.01, 0.01]),
    'Parch': np.random.choice([0, 1, 2, 3, 4, 5, 6], n_samples, p=[0.76, 0.13, 0.08, 0.02, 0.005, 0.003, 0.002]),
    'Fare': np.random.gamma(2, 15),
    'Embarked': np.random.choice(['C', 'Q', 'S'], n_samples, p=[0.19, 0.09, 0.72])
}

# ã‚ˆã‚Šç¾å®Ÿçš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½œæˆ
for i in range(n_samples):
    # å¥³æ€§ã¨å­ä¾›ã¯ç”Ÿå­˜ç‡ãŒé«˜ã„
    if demo_data['Sex'][i] == 'female' or demo_data['Age'][i] < 16:
        demo_data['Survived'][i] = np.random.choice([0, 1], p=[0.25, 0.75])
    # 1ç­‰å®¢å®¤ã¯ç”Ÿå­˜ç‡ãŒé«˜ã„
    if demo_data['Pclass'][i] == 1:
        demo_data['Survived'][i] = np.random.choice([0, 1], p=[0.37, 0.63])

train_df = pd.DataFrame(demo_data)

# ä¸€éƒ¨ãƒ‡ãƒ¼ã‚¿ã«æ¬ æå€¤ã‚’æ„å›³çš„ã«ä½œæˆï¼ˆç¾å®Ÿçš„ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼‰
missing_age_indices = np.random.choice(train_df.index, 177, replace=False)
train_df.loc[missing_age_indices, 'Age'] = np.nan

missing_embarked_indices = np.random.choice(train_df.index, 2, replace=False)
train_df.loc[missing_embarked_indices, 'Embarked'] = np.nan

print("ğŸš¢ ã‚¿ã‚¤ã‚¿ãƒ‹ãƒƒã‚¯ç”Ÿå­˜äºˆæ¸¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé–‹å§‹ï¼")
print(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {train_df.shape}")
print("\nğŸ“‹ ãƒ‡ãƒ¼ã‚¿ã®æ¦‚è¦:")
print(train_df.head())

print("\nğŸ“Š åŸºæœ¬çµ±è¨ˆé‡:")
print(train_df.describe())

print("\nâ“ æ¬ æå€¤ã®ç¢ºèª:")
print(train_df.isnull().sum())
```

### ğŸ” Step 2: æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆEDAï¼‰

```python
# ç”Ÿå­˜ç‡ã®åŸºæœ¬åˆ†æ
survival_rate = train_df['Survived'].mean()
print(f"å…¨ä½“ã®ç”Ÿå­˜ç‡: {survival_rate:.1%}")

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# 1. æ€§åˆ¥åˆ¥ç”Ÿå­˜ç‡
sex_survival = train_df.groupby('Sex')['Survived'].mean()
sex_survival.plot(kind='bar', ax=axes[0,0], color=['lightcoral', 'lightblue'])
axes[0,0].set_title('æ€§åˆ¥åˆ¥ç”Ÿå­˜ç‡')
axes[0,0].set_ylabel('ç”Ÿå­˜ç‡')
axes[0,0].tick_params(axis='x', rotation=0)

# 2. ã‚¯ãƒ©ã‚¹åˆ¥ç”Ÿå­˜ç‡
class_survival = train_df.groupby('Pclass')['Survived'].mean()
class_survival.plot(kind='bar', ax=axes[0,1], color=['gold', 'silver', 'brown'])
axes[0,1].set_title('å®¢å®¤ã‚¯ãƒ©ã‚¹åˆ¥ç”Ÿå­˜ç‡')
axes[0,1].set_ylabel('ç”Ÿå­˜ç‡')
axes[0,1].tick_params(axis='x', rotation=0)

# 3. å¹´é½¢åˆ†å¸ƒ
train_df['Age'].hist(bins=30, ax=axes[0,2], alpha=0.7, color='skyblue')
axes[0,2].set_title('å¹´é½¢åˆ†å¸ƒ')
axes[0,2].set_xlabel('å¹´é½¢')

# 4. é‹è³ƒåˆ†å¸ƒ
train_df['Fare'].hist(bins=30, ax=axes[1,0], alpha=0.7, color='lightgreen')
axes[1,0].set_title('é‹è³ƒåˆ†å¸ƒ')
axes[1,0].set_xlabel('é‹è³ƒ')

# 5. ä¹—èˆ¹æ¸¯åˆ¥ç”Ÿå­˜ç‡
embarked_survival = train_df.groupby('Embarked')['Survived'].mean()
embarked_survival.plot(kind='bar', ax=axes[1,1], color=['red', 'green', 'blue'])
axes[1,1].set_title('ä¹—èˆ¹æ¸¯åˆ¥ç”Ÿå­˜ç‡')
axes[1,1].tick_params(axis='x', rotation=0)

# 6. å®¶æ—ã‚µã‚¤ã‚ºã¨ç”Ÿå­˜ç‡
train_df['FamilySize'] = train_df['SibSp'] + train_df['Parch'] + 1
family_survival = train_df.groupby('FamilySize')['Survived'].mean()
family_survival.plot(kind='line', ax=axes[1,2], marker='o', color='purple')
axes[1,2].set_title('å®¶æ—ã‚µã‚¤ã‚ºåˆ¥ç”Ÿå­˜ç‡')
axes[1,2].set_xlabel('å®¶æ—ã‚µã‚¤ã‚º')

plt.tight_layout()
plt.show()

# é‡è¦ãªæ´å¯Ÿ
print("ğŸ’¡ é‡è¦ãªæ´å¯Ÿ:")
print(f"ğŸ‘© å¥³æ€§ã®ç”Ÿå­˜ç‡: {sex_survival['female']:.1%}")
print(f"ğŸ‘¨ ç”·æ€§ã®ç”Ÿå­˜ç‡: {sex_survival['male']:.1%}")
print(f"ğŸ¥‡ 1ç­‰å®¢å®¤ã®ç”Ÿå­˜ç‡: {class_survival[1]:.1%}")
print(f"ğŸ¥‰ 3ç­‰å®¢å®¤ã®ç”Ÿå­˜ç‡: {class_survival[3]:.1%}")
```

### ğŸ”§ Step 3: ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†

```python
from sklearn.preprocessing import LabelEncoder

def preprocess_titanic_data(df):
    """
    ã‚¿ã‚¤ã‚¿ãƒ‹ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†é–¢æ•°
    """
    # ãƒ‡ãƒ¼ã‚¿ã®ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆï¼ˆå…ƒãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›´ã—ãªã„ãŸã‚ï¼‰
    processed_df = df.copy()
    
    print("ğŸ”§ ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†é–‹å§‹...")
    
    # 1. æ¬ æå€¤å‡¦ç†
    print("â“ æ¬ æå€¤å‡¦ç†...")
    
    # å¹´é½¢ã®æ¬ æå€¤ã‚’ä¸­å¤®å€¤ã§è£œå®Œ
    median_age = processed_df['Age'].median()
    processed_df['Age'].fillna(median_age, inplace=True)
    
    # ä¹—èˆ¹æ¸¯ã®æ¬ æå€¤ã‚’æœ€é »å€¤ã§è£œå®Œ
    mode_embarked = processed_df['Embarked'].mode()[0]
    processed_df['Embarked'].fillna(mode_embarked, inplace=True)
    
    # 2. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
    print("ğŸ¯ æ–°ã—ã„ç‰¹å¾´é‡ã‚’ä½œæˆ...")
    
    # å®¶æ—ã‚µã‚¤ã‚º
    processed_df['FamilySize'] = processed_df['SibSp'] + processed_df['Parch'] + 1
    
    # ä¸€äººæ—…ã‹ã©ã†ã‹
    processed_df['IsAlone'] = (processed_df['FamilySize'] == 1).astype(int)
    
    # å¹´é½¢ã‚°ãƒ«ãƒ¼ãƒ—
    processed_df['AgeGroup'] = pd.cut(processed_df['Age'], 
                                     bins=[0, 12, 18, 35, 60, 100],
                                     labels=['Child', 'Teen', 'Adult', 'Middle', 'Senior'])
    
    # é‹è³ƒã‚°ãƒ«ãƒ¼ãƒ—
    processed_df['FareGroup'] = pd.qcut(processed_df['Fare'], 
                                       q=4, 
                                       labels=['Low', 'Medium', 'High', 'VeryHigh'])
    
    # 3. ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    print("ğŸ”¢ ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã‚’æ•°å€¤ã«å¤‰æ›...")
    
    # æ€§åˆ¥ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    processed_df['Sex'] = processed_df['Sex'].map({'female': 1, 'male': 0})
    
    # ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    categorical_features = ['Embarked', 'AgeGroup', 'FareGroup']
    
    for feature in categorical_features:
        dummies = pd.get_dummies(processed_df[feature], prefix=feature)
        processed_df = pd.concat([processed_df, dummies], axis=1)
    
    # ä¸è¦ãªåˆ—ã‚’å‰Šé™¤
    drop_columns = ['PassengerId', 'Embarked', 'AgeGroup', 'FareGroup']
    processed_df = processed_df.drop(drop_columns, axis=1)
    
    print("âœ… å‰å‡¦ç†å®Œäº†ï¼")
    print(f"ğŸ“Š æœ€çµ‚çš„ãªç‰¹å¾´é‡æ•°: {processed_df.shape[1] - 1}")  # Survivedã‚’é™¤ã
    
    return processed_df

# å‰å‡¦ç†å®Ÿè¡Œ
processed_df = preprocess_titanic_data(train_df)

print("\nğŸ“‹ å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿:")
print(processed_df.head())
print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {processed_df.shape}")
```

### ğŸ¤– Step 4: ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ã¨è©•ä¾¡

```python
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®åˆ†é›¢
X = processed_df.drop('Survived', axis=1)
y = processed_df['Survived']

print(f"ç‰¹å¾´é‡æ•°: {X.shape[1]}")
print(f"ãƒ‡ãƒ¼ã‚¿æ•°: {X.shape[0]}")

# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒ
models = {
    'ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°': LogisticRegression(random_state=42, max_iter=1000),
    'ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ': RandomForestClassifier(n_estimators=100, random_state=42),
    'å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°': GradientBoostingClassifier(random_state=42),
    'SVM': SVC(random_state=42, probability=True)
}

print("\nğŸ¤– ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒ:")
print("-" * 60)

# äº¤å·®æ¤œè¨¼ã§ã®è©•ä¾¡
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
best_score = 0
best_model_name = ""
best_model = None

for name, model in models.items():
    # äº¤å·®æ¤œè¨¼
    cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')
    
    print(f"{name:20s}: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})")
    
    if cv_scores.mean() > best_score:
        best_score = cv_scores.mean()
        best_model_name = name
        best_model = model

print(f"\nğŸ† æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«: {best_model_name} (CV Score: {best_score:.4f})")

# æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã§è©³ç´°è©•ä¾¡
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)

print(f"\nğŸ“Š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®æœ€çµ‚è©•ä¾¡:")
print(f"ç²¾åº¦: {accuracy_score(y_test, y_pred):.4f}")

print(f"\nğŸ“‹ è©³ç´°ãªåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
print(classification_report(y_test, y_pred, target_names=['æ­»äº¡', 'ç”Ÿå­˜']))

# æ··åŒè¡Œåˆ—
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['æ­»äº¡', 'ç”Ÿå­˜'],
            yticklabels=['æ­»äº¡', 'ç”Ÿå­˜'])
plt.title(f'æ··åŒè¡Œåˆ— ({best_model_name})')
plt.ylabel('å®Ÿéš›ã®ã‚¯ãƒ©ã‚¹')
plt.xlabel('äºˆæ¸¬ã‚¯ãƒ©ã‚¹')
plt.show()
```

### ğŸ“Š Step 5: ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ

```python
# ç‰¹å¾´é‡é‡è¦åº¦ã®åˆ†æï¼ˆãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®å ´åˆï¼‰
if hasattr(best_model, 'feature_importances_'):
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': best_model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("\nğŸ” ç‰¹å¾´é‡é‡è¦åº¦ TOP 10:")
    print(feature_importance.head(10))
    
    # å¯è¦–åŒ–
    plt.figure(figsize=(10, 8))
    top_features = feature_importance.head(10)
    sns.barplot(data=top_features, x='importance', y='feature', palette='viridis')
    plt.title(f'ç‰¹å¾´é‡é‡è¦åº¦ TOP 10 ({best_model_name})')
    plt.xlabel('é‡è¦åº¦')
    plt.tight_layout()
    plt.show()

# äºˆæ¸¬ä¾‹ã®è¡¨ç¤º
print("\nğŸ¯ äºˆæ¸¬ä¾‹:")
sample_indices = [0, 1, 2, 3, 4]
for i in sample_indices:
    actual = y_test.iloc[i]
    predicted = y_pred[i]
    
    if hasattr(best_model, 'predict_proba'):
        prob = best_model.predict_proba(X_test.iloc[i:i+1])[0]
        print(f"ã‚µãƒ³ãƒ—ãƒ«{i+1}: å®Ÿéš›={actual}, äºˆæ¸¬={predicted}, "
              f"ç¢ºç‡=[æ­»äº¡:{prob[0]:.3f}, ç”Ÿå­˜:{prob[1]:.3f}]")
    else:
        print(f"ã‚µãƒ³ãƒ—ãƒ«{i+1}: å®Ÿéš›={actual}, äºˆæ¸¬={predicted}")
```

---

## ğŸ› ï¸ scikit-learnã®ä¸»è¦æ©Ÿèƒ½

### ğŸ”„ 1. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç† (preprocessing)

```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder
from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures

# æ•°å€¤ã®æ­£è¦åŒ–
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
encoder = LabelEncoder()
y_encoded = encoder.fit_transform(y_categorical)

# ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
onehot = OneHotEncoder()
X_onehot = onehot.fit_transform(X_categorical)

print("âœ… å‰å‡¦ç†æ©Ÿèƒ½ã®ä¾‹")
```

### ğŸ¯ 2. ãƒ¢ãƒ‡ãƒ«é¸æŠ (model_selection)

```python
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# äº¤å·®æ¤œè¨¼
scores = cross_val_score(model, X, y, cv=5)

# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 7]}
grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)
grid_search.fit(X_train, y_train)

print("âœ… ãƒ¢ãƒ‡ãƒ«é¸æŠæ©Ÿèƒ½ã®ä¾‹")
```

### ğŸ“ 3. è©•ä¾¡æŒ‡æ¨™ (metrics)

```python
from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.metrics import f1_score, roc_auc_score, mean_squared_error

# åˆ†é¡ã®è©•ä¾¡
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred, average='weighted')
recall = recall_score(y_true, y_pred, average='weighted')
f1 = f1_score(y_true, y_pred, average='weighted')

# å›å¸°ã®è©•ä¾¡
mse = mean_squared_error(y_true, y_pred)
rmse = np.sqrt(mse)

print("âœ… è©•ä¾¡æŒ‡æ¨™æ©Ÿèƒ½ã®ä¾‹")
```

---

## ğŸš¨ ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã¨å¯¾å‡¦æ³•

### âŒ ã‚¨ãƒ©ãƒ¼1: ImportError

```python
# ã‚¨ãƒ©ãƒ¼ä¾‹
ImportError: No module named 'sklearn'

# å¯¾å‡¦æ³•
# 1. Anacondaç’°å¢ƒã®å ´åˆ
conda install scikit-learn

# 2. pipç’°å¢ƒã®å ´åˆ
pip install scikit-learn

# 3. ç’°å¢ƒç¢ºèª
import sklearn
print(sklearn.__version__)
```

### âŒ ã‚¨ãƒ©ãƒ¼2: ValueError (ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ã‚¨ãƒ©ãƒ¼)

```python
# ã‚¨ãƒ©ãƒ¼ä¾‹
ValueError: Expected 2D array, got 1D array instead

# å•é¡Œã®ã‚ã‚‹ã‚³ãƒ¼ãƒ‰
X = [1, 2, 3, 4, 5]  # 1æ¬¡å…ƒé…åˆ—
model.fit(X, y)      # ã‚¨ãƒ©ãƒ¼ï¼

# æ­£ã—ã„ã‚³ãƒ¼ãƒ‰
X = [[1], [2], [3], [4], [5]]  # 2æ¬¡å…ƒé…åˆ—
# ã¾ãŸã¯
X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)
model.fit(X, y)  # OKï¼

print("âœ… å½¢çŠ¶ã‚¨ãƒ©ãƒ¼ã®å¯¾å‡¦æ³•")
```

### âŒ ã‚¨ãƒ©ãƒ¼3: æ¬ æå€¤ã‚¨ãƒ©ãƒ¼

```python
# ã‚¨ãƒ©ãƒ¼ä¾‹  
ValueError: Input contains NaN, infinity or a value too large

# å¯¾å‡¦æ³•1: æ¬ æå€¤ç¢ºèª
print("æ¬ æå€¤ãƒã‚§ãƒƒã‚¯:", df.isnull().sum())

# å¯¾å‡¦æ³•2: æ¬ æå€¤å‡¦ç†
# æ•°å€¤åˆ—ã¯ä¸­å¤®å€¤ã§è£œå®Œ
df['Age'].fillna(df['Age'].median(), inplace=True)

# ã‚«ãƒ†ã‚´ãƒªåˆ—ã¯æœ€é »å€¤ã§è£œå®Œ
df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)

# å¯¾å‡¦æ³•3: æ¬ æå€¤ã‚’å«ã‚€è¡Œã‚’å‰Šé™¤
df_clean = df.dropna()

print("âœ… æ¬ æå€¤ã‚¨ãƒ©ãƒ¼ã®å¯¾å‡¦æ³•")
```

### âŒ ã‚¨ãƒ©ãƒ¼4: æ¬¡å…ƒä¸ä¸€è‡´ã‚¨ãƒ©ãƒ¼

```python
# ã‚¨ãƒ©ãƒ¼ä¾‹
ValueError: X has 10 features, but model was trained with 12 features

# åŸå› : å­¦ç¿’æ™‚ã¨ãƒ†ã‚¹ãƒˆæ™‚ã§ç‰¹å¾´é‡æ•°ãŒç•°ãªã‚‹

# å¯¾å‡¦æ³•: å‰å‡¦ç†ã‚’çµ±ä¸€ã™ã‚‹
def preprocess_data(df, is_training=True):
    if is_training:
        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®å ´åˆï¼šfit_transform
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        return X_scaled, scaler
    else:
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å ´åˆï¼štransform ã®ã¿
        X_scaled = scaler.transform(X)
        return X_scaled

print("âœ… æ¬¡å…ƒä¸ä¸€è‡´ã‚¨ãƒ©ãƒ¼ã®å¯¾å‡¦æ³•")
```

---

## ğŸ“ˆ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã®ã‚³ãƒ„

### ğŸ¯ 1. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°

```python
# åŠ¹æœçš„ãªç‰¹å¾´é‡ä½œæˆä¾‹

# 1. æ—¢å­˜ç‰¹å¾´é‡ã®çµ„ã¿åˆã‚ã›
df['BMI'] = df['Weight'] / (df['Height'] ** 2)

# 2. ãƒ“ãƒ‹ãƒ³ã‚°ï¼ˆé€£ç¶šå€¤ã‚’é›¢æ•£åŒ–ï¼‰
df['AgeGroup'] = pd.cut(df['Age'], bins=[0, 18, 35, 60, 100], 
                       labels=['Young', 'Adult', 'Middle', 'Senior'])

# 3. æ™‚ç³»åˆ—ç‰¹å¾´é‡
df['DayOfWeek'] = df['Date'].dt.dayofweek
df['Month'] = df['Date'].dt.month

# 4. é›†ç´„ç‰¹å¾´é‡
df['AvgPurchase'] = df.groupby('CustomerID')['Amount'].transform('mean')

print("âœ… ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®ã‚³ãƒ„")
```

### âš™ï¸ 2. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´

```python
from sklearn.model_selection import RandomizedSearchCV

# ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒã§åŠ¹ç‡çš„ã«æœ€é©åŒ–
param_distributions = {
    'n_estimators': [50, 100, 200, 300],
    'max_depth': [3, 5, 7, 10, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

random_search = RandomizedSearchCV(
    RandomForestClassifier(random_state=42),
    param_distributions,
    n_iter=20,  # è©¦è¡Œå›æ•°
    cv=5,
    random_state=42,
    n_jobs=-1  # ä¸¦åˆ—å‡¦ç†
)

random_search.fit(X_train, y_train)
print(f"æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {random_search.best_params_}")
print(f"æœ€é«˜ã‚¹ã‚³ã‚¢: {random_search.best_score_:.4f}")
```

### ğŸ­ 3. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•

```python
from sklearn.ensemble import VotingClassifier

# è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®çµ„ã¿åˆã‚ã›
rf = RandomForestClassifier(random_state=42)
gb = GradientBoostingClassifier(random_state=42)
lr = LogisticRegression(random_state=42)

# æŠ•ç¥¨åˆ†é¡å™¨
voting_clf = VotingClassifier(
    estimators=[('rf', rf), ('gb', gb), ('lr', lr)],
    voting='soft'  # ç¢ºç‡ãƒ™ãƒ¼ã‚¹ã®æŠ•ç¥¨
)

voting_clf.fit(X_train, y_train)
ensemble_score = voting_clf.score(X_test, y_test)
print(f"ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚¹ã‚³ã‚¢: {ensemble_score:.4f}")
```

---

## ğŸ“ å­¦ç¿’ã®æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

### ğŸ“š 1. ç™ºå±•çš„ãªãƒˆãƒ”ãƒƒã‚¯

```python
# å­¦ç¿’ã™ã¹ãåˆ†é‡
advanced_topics = {
    'ç‰¹å¾´é‡é¸æŠ': 'SelectKBest, RFE, LASSO',
    'ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³': 'Pipeline, ColumnTransformer',
    'ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿': 'SMOTE, class_weight',
    'æ™‚ç³»åˆ—': 'TimeSeriesSplit, ç‰¹æ®Šãªå‰å‡¦ç†',
    'ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°': 'TensorFlow, PyTorch ã¨ã®é€£æº',
    'MLOps': 'joblib, pickle ã‚’ä½¿ã£ãŸãƒ¢ãƒ‡ãƒ«ä¿å­˜'
}

for topic, details in advanced_topics.items():
    print(f"{topic}: {details}")
```

### ğŸ¯ 2. ãŠã™ã™ã‚å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ

```python
projects = [
    "1. ä½å®…ä¾¡æ ¼äºˆæ¸¬ï¼ˆå›å¸°å•é¡Œï¼‰",
    "2. æ‰‹æ›¸ãæ•°å­—èªè­˜ï¼ˆç”»åƒåˆ†é¡ï¼‰", 
    "3. æ˜ ç”»ãƒ¬ãƒ“ãƒ¥ãƒ¼æ„Ÿæƒ…åˆ†æï¼ˆè‡ªç„¶è¨€èªå‡¦ç†ï¼‰",
    "4. é¡§å®¢ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼‰",
    "5. ç•°å¸¸æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå¤–ã‚Œå€¤æ¤œå‡ºï¼‰",
    "6. æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰"
]

print("ğŸ“‹ ãŠã™ã™ã‚ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ:")
for project in projects:
    print(f"  {project}")
```

### ğŸ“– 3. å­¦ç¿’ãƒªã‚½ãƒ¼ã‚¹

```markdown
ğŸ“š æ›¸ç±ï¼š
- ã€ŒPythonæ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã€Sebastian Raschka
- ã€Œscikit-learn ãƒ‡ãƒ¼ã‚¿åˆ†æå®Ÿè£…ãƒãƒ³ãƒ‰ãƒ–ãƒƒã‚¯ã€åŠ è—¤å…¬ä¸€

ğŸŒ ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ï¼š
- scikit-learn å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
- Kaggle Learnï¼ˆç„¡æ–™ã‚³ãƒ¼ã‚¹ï¼‰
- YouTubeã€ŒPythonæ©Ÿæ¢°å­¦ç¿’ã€ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«

ğŸ¤ ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ï¼š
- Kaggleï¼ˆç«¶æŠ€ãƒ»ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ï¼‰
- Stack Overflowï¼ˆæŠ€è¡“çš„ãªè³ªå•ï¼‰
- Qiitaï¼ˆæ—¥æœ¬èªæŠ€è¡“è¨˜äº‹ï¼‰
```

---

## ğŸ‰ ã¾ã¨ã‚

### âœ… ä»Šå›ç¿’å¾—ã—ãŸã‚¹ã‚­ãƒ«

```
ğŸ¯ scikit-learnã®åŸºæœ¬æ¦‚å¿µã¨ä½¿ã„æ–¹
ğŸ”§ ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã®å®Ÿè·µçš„æ‰‹æ³•
ğŸ¤– è¤‡æ•°ã®æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ¯”è¼ƒ
ğŸ“Š ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã¨æ€§èƒ½åˆ†æ
ğŸ› ï¸ ã‚¨ãƒ©ãƒ¼å¯¾å‡¦ã¨ãƒ‡ãƒãƒƒã‚°æŠ€è¡“
```

### ğŸš€ æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³

```
ä»Šæ—¥ä¸­ã«ï¼š
âœ… Google Colabã§ã‚¢ã‚¤ãƒªã‚¹åˆ†é¡ã‚’å®Ÿéš›ã«å®Ÿè¡Œ
âœ… scikit-learnå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯

ä»Šé€±ä¸­ã«ï¼š
âœ… ã‚¿ã‚¤ã‚¿ãƒ‹ãƒƒã‚¯äºˆæ¸¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’å®Œæˆã•ã›ã‚‹
âœ… ä»–ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆSVMã€k-NNï¼‰ã‚’è©¦ã—ã¦ã¿ã‚‹

ä»Šæœˆä¸­ã«ï¼š
âœ… Kaggleã®ä»–ã®ç«¶æŠ€ã«å‚åŠ 
âœ… è‡ªåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§æ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå®Ÿè¡Œ
```

**ğŸŒŸ ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼ã‚ãªãŸã¯ä»Šã€scikit-learnã‚’ä½¿ã£ã¦å®Ÿéš›ã®æ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’å®Ÿè£…ã§ãã‚‹ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã™ã€‚**

**ç¶™ç¶šçš„ãªå­¦ç¿’ã¨å®Ÿè·µã‚’é€šã˜ã¦ã€ã•ã‚‰ã«ã‚¹ã‚­ãƒ«ã‚’ç£¨ã„ã¦ã„ãã¾ã—ã‚‡ã†ï¼**

---

*åˆ†ã‹ã‚‰ãªã„ã“ã¨ãŒã‚ã‚Œã°ã€é æ…®ãªãè³ªå•ã—ã¦ãã ã•ã„ã€‚ä¸€ç·’ã«å•é¡Œã‚’è§£æ±ºã—ã¦ã„ãã¾ã—ã‚‡ã†ï¼* ğŸš€ğŸ“Š
