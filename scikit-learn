# scikit-learn完全ガイド - 新人エンジニア向け

## 🎯 この記事で身につくスキル

✅ **scikit-learnの基本概念と使い方**  
✅ **実際の機械学習プロジェクトの実装**  
✅ **データ前処理から予測まで一連の流れ**  
✅ **よくあるエラーの対処法**  

---

## 📚 scikit-learnって何？

### 🤖 一言で説明すると

> **Python用の機械学習ライブラリ。初心者でも簡単に本格的な機械学習ができる最強ツール**

### 🌟 なぜscikit-learnが人気なのか？

| 理由 | 詳細 | 初心者への価値 |
|------|------|---------------|
| **簡単** | 3行でモデル作成可能 | プログラミング初心者でもすぐ使える |
| **統一API** | 全アルゴリズムが同じ使い方 | 覚えることが少ない |
| **豊富なアルゴリズム** | 分類・回帰・クラスタリング全て | 1つのライブラリで何でもできる |
| **無料** | 完全オープンソース | 学習コスト0円 |
| **日本語情報豊富** | 解説記事・書籍が多数 | 困った時に情報が見つかりやすい |

### 🏢 実際の現場での使われ方

```
🏥 医療：病気の診断支援システム
🏦 金融：クレジットカードの不正利用検出  
🛒 EC：商品推薦システム
🚗 製造：品質管理・異常検知
📱 IT：ユーザー行動分析
```

---

## 🚀 環境構築（15分で完了）

### 方法1：Google Colab（最推奨・初心者向け）

```python
# Google Colabなら最初から使える！
# https://colab.research.google.com/ にアクセスするだけ

# 確認コード
import sklearn
print(f"scikit-learn version: {sklearn.__version__}")
print("✅ 環境構築完了！")
```

**メリット：**
- 設定不要、ブラウザだけでOK
- 無料でGPU使用可能
- 自動保存機能付き

### 方法2：ローカル環境（Anaconda使用）

#### Step 1: Anacondaインストール

1. **[Anaconda公式サイト](https://www.anaconda.com/products/distribution)からダウンロード**
2. **インストーラーを実行（デフォルト設定でOK）**
3. **Anaconda Promptを起動**

#### Step 2: 仮想環境作成

```bash
# 仮想環境作成（Python 3.9指定）
conda create -n ml_env python=3.9

# 仮想環境有効化
conda activate ml_env

# scikit-learnインストール
conda install scikit-learn pandas numpy matplotlib seaborn jupyter

# インストール確認
python -c "import sklearn; print(sklearn.__version__)"
```

#### Step 3: Jupyter Notebook起動

```bash
# Jupyter Notebook起動
jupyter notebook

# ブラウザが自動で開く
# 新しいNotebookを作成して開始！
```

---

## 📊 scikit-learnの基本構造

### 🗂️ 主要モジュール

```python
from sklearn import datasets          # サンプルデータセット
from sklearn import model_selection   # データ分割・検証
from sklearn import preprocessing     # データ前処理
from sklearn import linear_model      # 線形モデル（回帰・分類）
from sklearn import ensemble         # アンサンブル手法
from sklearn import tree             # 決定木
from sklearn import svm              # サポートベクターマシン
from sklearn import neighbors        # k近傍法
from sklearn import cluster          # クラスタリング
from sklearn import metrics          # 評価指標
```

### 🎯 統一されたAPI（覚えるのはこれだけ！）

```python
# 全てのアルゴリズムが同じ使い方！

# 1. インポート
from sklearn.ensemble import RandomForestClassifier

# 2. モデル作成
model = RandomForestClassifier()

# 3. 学習
model.fit(X_train, y_train)

# 4. 予測
predictions = model.predict(X_test)

# 5. 評価
score = model.score(X_test, y_test)
```

---

## 🎓 チュートリアル1：アイリス花分類（30分）

### 📚 アイリス分類とは？

> 花の「がく片の長さ・幅」と「花びらの長さ・幅」から、3種類のアイリス（setosa, versicolor, virginica）を予測する問題

**なぜ初学者に人気？**
- データが小さい（150サンプル）
- 特徴量が4つだけで理解しやすい
- 分類精度が高く、成功体験を得やすい

### 🔍 Step 1: データの読み込みと確認

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import datasets

# アイリスデータセット読み込み
iris = datasets.load_iris()

print("🌸 アイリスデータセット分析開始！")
print(f"データサイズ: {iris.data.shape}")
print(f"特徴量: {iris.feature_names}")
print(f"クラス: {iris.target_names}")

# DataFrameに変換（見やすくするため）
df = pd.DataFrame(iris.data, columns=iris.feature_names)
df['species'] = iris.target_names[iris.target]

print("\n📊 データの最初の5行:")
print(df.head())

print("\n📈 基本統計量:")
print(df.describe())
```

### 📊 Step 2: データの可視化

```python
# グラフ設定
plt.style.use('default')
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# 1. 花びらの長さ vs 幅
sns.scatterplot(data=df, x='petal length (cm)', y='petal width (cm)', 
                hue='species', ax=axes[0,0])
axes[0,0].set_title('花びら：長さ vs 幅')

# 2. がく片の長さ vs 幅  
sns.scatterplot(data=df, x='sepal length (cm)', y='sepal width (cm)', 
                hue='species', ax=axes[0,1])
axes[0,1].set_title('がく片：長さ vs 幅')

# 3. 特徴量の分布
df.boxplot(column='petal length (cm)', by='species', ax=axes[1,0])
axes[1,0].set_title('花びらの長さ分布')

# 4. 相関行列
correlation = df.select_dtypes(include=[np.number]).corr()
sns.heatmap(correlation, annot=True, ax=axes[1,1])
axes[1,1].set_title('特徴量間の相関')

plt.tight_layout()
plt.show()

print("💡 重要な発見:")
print("1. 花びらの特徴（長さ・幅）でクラスがよく分かれている")
print("2. setosaは他の2種類と明確に区別できる")
print("3. versicolorとvirginicaは少し重複している")
```

### 🔧 Step 3: データの前処理

```python
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 特徴量とターゲットに分離
X = iris.data  # 特徴量（4つの測定値）
y = iris.target  # ターゲット（0, 1, 2の3クラス）

print(f"特徴量の形状: {X.shape}")
print(f"ターゲットの形状: {y.shape}")
print(f"クラス分布: {np.bincount(y)}")

# データを訓練用とテスト用に分割（8:2の比率）
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2,      # テストデータの割合
    random_state=42,    # 再現性のため
    stratify=y          # 各クラスの比率を保持
)

print(f"\n📊 データ分割結果:")
print(f"訓練データ: {X_train.shape[0]}サンプル")
print(f"テストデータ: {X_test.shape[0]}サンプル")

# 特徴量の正規化（オプション：線形手法で推奨）
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("✅ 前処理完了！")
```

### 🤖 Step 4: 複数モデルで予測

```python
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report

# 複数のモデルを試してみる
models = {
    'ロジスティック回帰': LogisticRegression(random_state=42),
    '決定木': DecisionTreeClassifier(random_state=42),
    'ランダムフォレスト': RandomForestClassifier(random_state=42),
    'SVM': SVC(random_state=42),
    'k近傍法': KNeighborsClassifier()
}

print("🤖 各モデルの性能比較:")
print("-" * 50)

results = {}
for name, model in models.items():
    # 学習
    if name in ['ロジスティック回帰', 'SVM']:
        # 線形手法は正規化データを使用
        model.fit(X_train_scaled, y_train)
        predictions = model.predict(X_test_scaled)
    else:
        # 木系手法は元データを使用
        model.fit(X_train, y_train)
        predictions = model.predict(X_test)
    
    # 評価
    accuracy = accuracy_score(y_test, predictions)
    results[name] = accuracy
    
    print(f"{name:15s}: {accuracy:.4f} ({accuracy*100:.1f}%)")

# 最高性能のモデル
best_model = max(results, key=results.get)
print(f"\n🏆 最高性能: {best_model} ({results[best_model]:.4f})")
```

### 📊 Step 5: 詳細な評価

```python
from sklearn.metrics import confusion_matrix
import seaborn as sns

# 最高性能モデルで詳細評価
best_classifier = RandomForestClassifier(random_state=42)
best_classifier.fit(X_train, y_train)
predictions = best_classifier.predict(X_test)

# 混同行列
cm = confusion_matrix(y_test, predictions)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=iris.target_names,
            yticklabels=iris.target_names)
plt.title('混同行列（ランダムフォレスト）')
plt.ylabel('実際のクラス')
plt.xlabel('予測クラス')
plt.show()

# 詳細な分類レポート
print("📋 詳細な分類レポート:")
print(classification_report(y_test, predictions, 
                          target_names=iris.target_names))

# 特徴量重要度
feature_importance = pd.DataFrame({
    'feature': iris.feature_names,
    'importance': best_classifier.feature_importances_
}).sort_values('importance', ascending=False)

print("\n🔝 特徴量重要度:")
print(feature_importance)
```

---

## 🚢 チュートリアル2：タイタニック生存予測（60分）

### 🎯 実践的なプロジェクト

> より複雑なデータセットで、実際のデータサイエンスプロジェクトを体験

### 📥 Step 1: データ取得と基本確認

```python
# タイタニックデータセット（仮想データ作成）
import numpy as np
import pandas as pd

# 実践的なタイタニックデータの読み込み
# 実際のプロジェクトではCSVファイルから読み込み
# train_df = pd.read_csv('train.csv')

# デモ用データ作成（実際のKaggleデータ構造を模擬）
np.random.seed(42)
n_samples = 891

demo_data = {
    'PassengerId': range(1, n_samples + 1),
    'Survived': np.random.choice([0, 1], n_samples, p=[0.62, 0.38]),
    'Pclass': np.random.choice([1, 2, 3], n_samples, p=[0.24, 0.21, 0.55]),
    'Sex': np.random.choice(['male', 'female'], n_samples, p=[0.65, 0.35]),
    'Age': np.random.normal(29, 12, n_samples).clip(0, 80),
    'SibSp': np.random.choice([0, 1, 2, 3, 4, 5], n_samples, p=[0.68, 0.23, 0.05, 0.02, 0.01, 0.01]),
    'Parch': np.random.choice([0, 1, 2, 3, 4, 5, 6], n_samples, p=[0.76, 0.13, 0.08, 0.02, 0.005, 0.003, 0.002]),
    'Fare': np.random.gamma(2, 15),
    'Embarked': np.random.choice(['C', 'Q', 'S'], n_samples, p=[0.19, 0.09, 0.72])
}

# より現実的なパターンを作成
for i in range(n_samples):
    # 女性と子供は生存率が高い
    if demo_data['Sex'][i] == 'female' or demo_data['Age'][i] < 16:
        demo_data['Survived'][i] = np.random.choice([0, 1], p=[0.25, 0.75])
    # 1等客室は生存率が高い
    if demo_data['Pclass'][i] == 1:
        demo_data['Survived'][i] = np.random.choice([0, 1], p=[0.37, 0.63])

train_df = pd.DataFrame(demo_data)

# 一部データに欠損値を意図的に作成（現実的なデータセット）
missing_age_indices = np.random.choice(train_df.index, 177, replace=False)
train_df.loc[missing_age_indices, 'Age'] = np.nan

missing_embarked_indices = np.random.choice(train_df.index, 2, replace=False)
train_df.loc[missing_embarked_indices, 'Embarked'] = np.nan

print("🚢 タイタニック生存予測プロジェクト開始！")
print(f"データサイズ: {train_df.shape}")
print("\n📋 データの概要:")
print(train_df.head())

print("\n📊 基本統計量:")
print(train_df.describe())

print("\n❓ 欠損値の確認:")
print(train_df.isnull().sum())
```

### 🔍 Step 2: 探索的データ分析（EDA）

```python
# 生存率の基本分析
survival_rate = train_df['Survived'].mean()
print(f"全体の生存率: {survival_rate:.1%}")

# 可視化
fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# 1. 性別別生存率
sex_survival = train_df.groupby('Sex')['Survived'].mean()
sex_survival.plot(kind='bar', ax=axes[0,0], color=['lightcoral', 'lightblue'])
axes[0,0].set_title('性別別生存率')
axes[0,0].set_ylabel('生存率')
axes[0,0].tick_params(axis='x', rotation=0)

# 2. クラス別生存率
class_survival = train_df.groupby('Pclass')['Survived'].mean()
class_survival.plot(kind='bar', ax=axes[0,1], color=['gold', 'silver', 'brown'])
axes[0,1].set_title('客室クラス別生存率')
axes[0,1].set_ylabel('生存率')
axes[0,1].tick_params(axis='x', rotation=0)

# 3. 年齢分布
train_df['Age'].hist(bins=30, ax=axes[0,2], alpha=0.7, color='skyblue')
axes[0,2].set_title('年齢分布')
axes[0,2].set_xlabel('年齢')

# 4. 運賃分布
train_df['Fare'].hist(bins=30, ax=axes[1,0], alpha=0.7, color='lightgreen')
axes[1,0].set_title('運賃分布')
axes[1,0].set_xlabel('運賃')

# 5. 乗船港別生存率
embarked_survival = train_df.groupby('Embarked')['Survived'].mean()
embarked_survival.plot(kind='bar', ax=axes[1,1], color=['red', 'green', 'blue'])
axes[1,1].set_title('乗船港別生存率')
axes[1,1].tick_params(axis='x', rotation=0)

# 6. 家族サイズと生存率
train_df['FamilySize'] = train_df['SibSp'] + train_df['Parch'] + 1
family_survival = train_df.groupby('FamilySize')['Survived'].mean()
family_survival.plot(kind='line', ax=axes[1,2], marker='o', color='purple')
axes[1,2].set_title('家族サイズ別生存率')
axes[1,2].set_xlabel('家族サイズ')

plt.tight_layout()
plt.show()

# 重要な洞察
print("💡 重要な洞察:")
print(f"👩 女性の生存率: {sex_survival['female']:.1%}")
print(f"👨 男性の生存率: {sex_survival['male']:.1%}")
print(f"🥇 1等客室の生存率: {class_survival[1]:.1%}")
print(f"🥉 3等客室の生存率: {class_survival[3]:.1%}")
```

### 🔧 Step 3: データ前処理

```python
from sklearn.preprocessing import LabelEncoder

def preprocess_titanic_data(df):
    """
    タイタニックデータの前処理関数
    """
    # データのコピーを作成（元データを変更しないため）
    processed_df = df.copy()
    
    print("🔧 データ前処理開始...")
    
    # 1. 欠損値処理
    print("❓ 欠損値処理...")
    
    # 年齢の欠損値を中央値で補完
    median_age = processed_df['Age'].median()
    processed_df['Age'].fillna(median_age, inplace=True)
    
    # 乗船港の欠損値を最頻値で補完
    mode_embarked = processed_df['Embarked'].mode()[0]
    processed_df['Embarked'].fillna(mode_embarked, inplace=True)
    
    # 2. 特徴量エンジニアリング
    print("🎯 新しい特徴量を作成...")
    
    # 家族サイズ
    processed_df['FamilySize'] = processed_df['SibSp'] + processed_df['Parch'] + 1
    
    # 一人旅かどうか
    processed_df['IsAlone'] = (processed_df['FamilySize'] == 1).astype(int)
    
    # 年齢グループ
    processed_df['AgeGroup'] = pd.cut(processed_df['Age'], 
                                     bins=[0, 12, 18, 35, 60, 100],
                                     labels=['Child', 'Teen', 'Adult', 'Middle', 'Senior'])
    
    # 運賃グループ
    processed_df['FareGroup'] = pd.qcut(processed_df['Fare'], 
                                       q=4, 
                                       labels=['Low', 'Medium', 'High', 'VeryHigh'])
    
    # 3. カテゴリカル変数のエンコーディング
    print("🔢 カテゴリカル変数を数値に変換...")
    
    # 性別をエンコーディング
    processed_df['Sex'] = processed_df['Sex'].map({'female': 1, 'male': 0})
    
    # ワンホットエンコーディング
    categorical_features = ['Embarked', 'AgeGroup', 'FareGroup']
    
    for feature in categorical_features:
        dummies = pd.get_dummies(processed_df[feature], prefix=feature)
        processed_df = pd.concat([processed_df, dummies], axis=1)
    
    # 不要な列を削除
    drop_columns = ['PassengerId', 'Embarked', 'AgeGroup', 'FareGroup']
    processed_df = processed_df.drop(drop_columns, axis=1)
    
    print("✅ 前処理完了！")
    print(f"📊 最終的な特徴量数: {processed_df.shape[1] - 1}")  # Survivedを除く
    
    return processed_df

# 前処理実行
processed_df = preprocess_titanic_data(train_df)

print("\n📋 処理後のデータ:")
print(processed_df.head())
print(f"\n📊 データ形状: {processed_df.shape}")
```

### 🤖 Step 4: モデル構築と評価

```python
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 特徴量とターゲットの分離
X = processed_df.drop('Survived', axis=1)
y = processed_df['Survived']

print(f"特徴量数: {X.shape[1]}")
print(f"データ数: {X.shape[0]}")

# データ分割
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 複数モデルの比較
models = {
    'ロジスティック回帰': LogisticRegression(random_state=42, max_iter=1000),
    'ランダムフォレスト': RandomForestClassifier(n_estimators=100, random_state=42),
    '勾配ブースティング': GradientBoostingClassifier(random_state=42),
    'SVM': SVC(random_state=42, probability=True)
}

print("\n🤖 モデル性能比較:")
print("-" * 60)

# 交差検証での評価
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
best_score = 0
best_model_name = ""
best_model = None

for name, model in models.items():
    # 交差検証
    cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')
    
    print(f"{name:20s}: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})")
    
    if cv_scores.mean() > best_score:
        best_score = cv_scores.mean()
        best_model_name = name
        best_model = model

print(f"\n🏆 最高性能モデル: {best_model_name} (CV Score: {best_score:.4f})")

# 最高性能モデルで詳細評価
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)

print(f"\n📊 テストデータでの最終評価:")
print(f"精度: {accuracy_score(y_test, y_pred):.4f}")

print(f"\n📋 詳細な分類レポート:")
print(classification_report(y_test, y_pred, target_names=['死亡', '生存']))

# 混同行列
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['死亡', '生存'],
            yticklabels=['死亡', '生存'])
plt.title(f'混同行列 ({best_model_name})')
plt.ylabel('実際のクラス')
plt.xlabel('予測クラス')
plt.show()
```

### 📊 Step 5: 特徴量重要度分析

```python
# 特徴量重要度の分析（ランダムフォレストの場合）
if hasattr(best_model, 'feature_importances_'):
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': best_model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("\n🔝 特徴量重要度 TOP 10:")
    print(feature_importance.head(10))
    
    # 可視化
    plt.figure(figsize=(10, 8))
    top_features = feature_importance.head(10)
    sns.barplot(data=top_features, x='importance', y='feature', palette='viridis')
    plt.title(f'特徴量重要度 TOP 10 ({best_model_name})')
    plt.xlabel('重要度')
    plt.tight_layout()
    plt.show()

# 予測例の表示
print("\n🎯 予測例:")
sample_indices = [0, 1, 2, 3, 4]
for i in sample_indices:
    actual = y_test.iloc[i]
    predicted = y_pred[i]
    
    if hasattr(best_model, 'predict_proba'):
        prob = best_model.predict_proba(X_test.iloc[i:i+1])[0]
        print(f"サンプル{i+1}: 実際={actual}, 予測={predicted}, "
              f"確率=[死亡:{prob[0]:.3f}, 生存:{prob[1]:.3f}]")
    else:
        print(f"サンプル{i+1}: 実際={actual}, 予測={predicted}")
```

---

## 🛠️ scikit-learnの主要機能

### 🔄 1. データ前処理 (preprocessing)

```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder
from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures

# 数値の正規化
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# カテゴリカル変数のエンコーディング
encoder = LabelEncoder()
y_encoded = encoder.fit_transform(y_categorical)

# ワンホットエンコーディング
onehot = OneHotEncoder()
X_onehot = onehot.fit_transform(X_categorical)

print("✅ 前処理機能の例")
```

### 🎯 2. モデル選択 (model_selection)

```python
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

# データ分割
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# 交差検証
scores = cross_val_score(model, X, y, cv=5)

# ハイパーパラメータ最適化
param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 7]}
grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)
grid_search.fit(X_train, y_train)

print("✅ モデル選択機能の例")
```

### 📏 3. 評価指標 (metrics)

```python
from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.metrics import f1_score, roc_auc_score, mean_squared_error

# 分類の評価
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred, average='weighted')
recall = recall_score(y_true, y_pred, average='weighted')
f1 = f1_score(y_true, y_pred, average='weighted')

# 回帰の評価
mse = mean_squared_error(y_true, y_pred)
rmse = np.sqrt(mse)

print("✅ 評価指標機能の例")
```

---

## 🚨 よくあるエラーと対処法

### ❌ エラー1: ImportError

```python
# エラー例
ImportError: No module named 'sklearn'

# 対処法
# 1. Anaconda環境の場合
conda install scikit-learn

# 2. pip環境の場合
pip install scikit-learn

# 3. 環境確認
import sklearn
print(sklearn.__version__)
```

### ❌ エラー2: ValueError (データ形状エラー)

```python
# エラー例
ValueError: Expected 2D array, got 1D array instead

# 問題のあるコード
X = [1, 2, 3, 4, 5]  # 1次元配列
model.fit(X, y)      # エラー！

# 正しいコード
X = [[1], [2], [3], [4], [5]]  # 2次元配列
# または
X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)
model.fit(X, y)  # OK！

print("✅ 形状エラーの対処法")
```

### ❌ エラー3: 欠損値エラー

```python
# エラー例  
ValueError: Input contains NaN, infinity or a value too large

# 対処法1: 欠損値確認
print("欠損値チェック:", df.isnull().sum())

# 対処法2: 欠損値処理
# 数値列は中央値で補完
df['Age'].fillna(df['Age'].median(), inplace=True)

# カテゴリ列は最頻値で補完
df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)

# 対処法3: 欠損値を含む行を削除
df_clean = df.dropna()

print("✅ 欠損値エラーの対処法")
```

### ❌ エラー4: 次元不一致エラー

```python
# エラー例
ValueError: X has 10 features, but model was trained with 12 features

# 原因: 学習時とテスト時で特徴量数が異なる

# 対処法: 前処理を統一する
def preprocess_data(df, is_training=True):
    if is_training:
        # 学習データの場合：fit_transform
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        return X_scaled, scaler
    else:
        # テストデータの場合：transform のみ
        X_scaled = scaler.transform(X)
        return X_scaled

print("✅ 次元不一致エラーの対処法")
```

---

## 📈 パフォーマンス向上のコツ

### 🎯 1. 特徴量エンジニアリング

```python
# 効果的な特徴量作成例

# 1. 既存特徴量の組み合わせ
df['BMI'] = df['Weight'] / (df['Height'] ** 2)

# 2. ビニング（連続値を離散化）
df['AgeGroup'] = pd.cut(df['Age'], bins=[0, 18, 35, 60, 100], 
                       labels=['Young', 'Adult', 'Middle', 'Senior'])

# 3. 時系列特徴量
df['DayOfWeek'] = df['Date'].dt.dayofweek
df['Month'] = df['Date'].dt.month

# 4. 集約特徴量
df['AvgPurchase'] = df.groupby('CustomerID')['Amount'].transform('mean')

print("✅ 特徴量エンジニアリングのコツ")
```

### ⚙️ 2. ハイパーパラメータ調整

```python
from sklearn.model_selection import RandomizedSearchCV

# ランダムサーチで効率的に最適化
param_distributions = {
    'n_estimators': [50, 100, 200, 300],
    'max_depth': [3, 5, 7, 10, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

random_search = RandomizedSearchCV(
    RandomForestClassifier(random_state=42),
    param_distributions,
    n_iter=20,  # 試行回数
    cv=5,
    random_state=42,
    n_jobs=-1  # 並列処理
)

random_search.fit(X_train, y_train)
print(f"最適パラメータ: {random_search.best_params_}")
print(f"最高スコア: {random_search.best_score_:.4f}")
```

### 🎭 3. アンサンブル手法

```python
from sklearn.ensemble import VotingClassifier

# 複数モデルの組み合わせ
rf = RandomForestClassifier(random_state=42)
gb = GradientBoostingClassifier(random_state=42)
lr = LogisticRegression(random_state=42)

# 投票分類器
voting_clf = VotingClassifier(
    estimators=[('rf', rf), ('gb', gb), ('lr', lr)],
    voting='soft'  # 確率ベースの投票
)

voting_clf.fit(X_train, y_train)
ensemble_score = voting_clf.score(X_test, y_test)
print(f"アンサンブルスコア: {ensemble_score:.4f}")
```

---

## 🎓 学習の次のステップ

### 📚 1. 発展的なトピック

```python
# 学習すべき分野
advanced_topics = {
    '特徴量選択': 'SelectKBest, RFE, LASSO',
    'パイプライン': 'Pipeline, ColumnTransformer',
    '不均衡データ': 'SMOTE, class_weight',
    '時系列': 'TimeSeriesSplit, 特殊な前処理',
    'ディープラーニング': 'TensorFlow, PyTorch との連携',
    'MLOps': 'joblib, pickle を使ったモデル保存'
}

for topic, details in advanced_topics.items():
    print(f"{topic}: {details}")
```

### 🎯 2. おすすめ実践プロジェクト

```python
projects = [
    "1. 住宅価格予測（回帰問題）",
    "2. 手書き数字認識（画像分類）", 
    "3. 映画レビュー感情分析（自然言語処理）",
    "4. 顧客セグメンテーション（クラスタリング）",
    "5. 異常検知システム（外れ値検出）",
    "6. 推薦システム（協調フィルタリング）"
]

print("📋 おすすめプロジェクト:")
for project in projects:
    print(f"  {project}")
```

### 📖 3. 学習リソース

```markdown
📚 書籍：
- 「Python機械学習プログラミング」Sebastian Raschka
- 「scikit-learn データ分析実装ハンドブック」加藤公一

🌐 オンライン：
- scikit-learn 公式ドキュメント
- Kaggle Learn（無料コース）
- YouTube「Python機械学習」チュートリアル

🤝 コミュニティ：
- Kaggle（競技・ディスカッション）
- Stack Overflow（技術的な質問）
- Qiita（日本語技術記事）
```

---

## 🎉 まとめ

### ✅ 今回習得したスキル

```
🎯 scikit-learnの基本概念と使い方
🔧 データ前処理の実践的手法
🤖 複数の機械学習アルゴリズムの比較
📊 モデル評価と性能分析
🛠️ エラー対処とデバッグ技術
```

### 🚀 次のアクション

```
今日中に：
✅ Google Colabでアイリス分類を実際に実行
✅ scikit-learn公式ドキュメントをブックマーク

今週中に：
✅ タイタニック予測プロジェクトを完成させる
✅ 他のアルゴリズム（SVM、k-NN）を試してみる

今月中に：
✅ Kaggleの他の競技に参加
✅ 自分のデータセットで機械学習プロジェクト実行
```

**🌟 おめでとうございます！あなたは今、scikit-learnを使って実際の機械学習プロジェクトを実装できるエンジニアです。**

**継続的な学習と実践を通じて、さらにスキルを磨いていきましょう！**

---

*分からないことがあれば、遠慮なく質問してください。一緒に問題を解決していきましょう！* 🚀📊
