# ã‚¿ã‚¤ã‚¿ãƒ‹ãƒƒã‚¯å·ç”Ÿå­˜äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«
# Kaggleã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ç”¨

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import warnings
warnings.filterwarnings('ignore')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = 'DejaVu Sans'
sns.set_style("whitegrid")

class TitanicPredictor:
    def __init__(self):
        self.model = None
        self.le_sex = LabelEncoder()
        self.le_embarked = LabelEncoder()
        
    def load_data(self, train_path='train.csv', test_path='test.csv'):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        self.train_df = pd.read_csv(train_path)
        self.test_df = pd.read_csv(test_path)
        
        print("=== ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ ===")
        print(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {self.train_df.shape}")
        print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {self.test_df.shape}")
        print("\n=== å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®æœ€åˆã®5è¡Œ ===")
        print(self.train_df.head())
        
        return self.train_df, self.test_df
    
    def explore_data(self):
        """ãƒ‡ãƒ¼ã‚¿æ¢ç´¢ãƒ»å¯è¦–åŒ–"""
        print("\n=== åŸºæœ¬çµ±è¨ˆæƒ…å ± ===")
        print(self.train_df.describe())
        
        print("\n=== æ¬ æå€¤ç¢ºèª ===")
        print(self.train_df.isnull().sum())
        
        # ç”Ÿå­˜ç‡ã®åŸºæœ¬åˆ†æ
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # æ€§åˆ¥ã«ã‚ˆã‚‹ç”Ÿå­˜ç‡
        sns.barplot(data=self.train_df, x='Sex', y='Survived', ax=axes[0,0])
        axes[0,0].set_title('æ€§åˆ¥ã«ã‚ˆã‚‹ç”Ÿå­˜ç‡')
        
        # å®¢å®¤ç­‰ç´šã«ã‚ˆã‚‹ç”Ÿå­˜ç‡
        sns.barplot(data=self.train_df, x='Pclass', y='Survived', ax=axes[0,1])
        axes[0,1].set_title('å®¢å®¤ç­‰ç´šã«ã‚ˆã‚‹ç”Ÿå­˜ç‡')
        
        # å¹´é½¢åˆ†å¸ƒ
        axes[1,0].hist(self.train_df['Age'].dropna(), bins=30, alpha=0.7)
        axes[1,0].set_title('å¹´é½¢åˆ†å¸ƒ')
        axes[1,0].set_xlabel('å¹´é½¢')
        
        # é‹è³ƒåˆ†å¸ƒ
        axes[1,1].hist(self.train_df['Fare'].dropna(), bins=30, alpha=0.7)
        axes[1,1].set_title('é‹è³ƒåˆ†å¸ƒ')
        axes[1,1].set_xlabel('é‹è³ƒ')
        
        plt.tight_layout()
        plt.show()
        
        # ç›¸é–¢è¡Œåˆ—
        plt.figure(figsize=(10, 8))
        # æ•°å€¤ã‚«ãƒ©ãƒ ã®ã¿é¸æŠ
        numeric_cols = self.train_df.select_dtypes(include=[np.number]).columns
        correlation_matrix = self.train_df[numeric_cols].corr()
        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
        plt.title('ç‰¹å¾´é‡é–“ã®ç›¸é–¢')
        plt.show()
    
    def preprocess_data(self):
        """ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†"""
        # ä¸¡æ–¹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’çµåˆã—ã¦ä¸€æ‹¬å‡¦ç†
        all_data = pd.concat([self.train_df, self.test_df], ignore_index=True)
        
        print("\n=== ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†é–‹å§‹ ===")
        
        # 1. å¹´é½¢ã®æ¬ æå€¤ã‚’ä¸­å¤®å€¤ã§è£œå®Œ
        age_median = all_data['Age'].median()
        all_data['Age'].fillna(age_median, inplace=True)
        
        # 2. ä¹—èˆ¹æ¸¯ã®æ¬ æå€¤ã‚’æœ€é »å€¤ã§è£œå®Œ
        embarked_mode = all_data['Embarked'].mode()[0]
        all_data['Embarked'].fillna(embarked_mode, inplace=True)
        
        # 3. é‹è³ƒã®æ¬ æå€¤ã‚’ä¸­å¤®å€¤ã§è£œå®Œ
        fare_median = all_data['Fare'].median()
        all_data['Fare'].fillna(fare_median, inplace=True)
        
        # 4. æ–°ã—ã„ç‰¹å¾´é‡ã‚’ä½œæˆ
        # å®¶æ—ã‚µã‚¤ã‚º
        all_data['FamilySize'] = all_data['SibSp'] + all_data['Parch'] + 1
        
        # ä¸€äººæ—…ã‹ã©ã†ã‹
        all_data['IsAlone'] = (all_data['FamilySize'] == 1).astype(int)
        
        # æ•¬ç§°ã‚’æŠ½å‡º
        all_data['Title'] = all_data['Name'].str.extract(' ([A-Za-z]+)\.', expand=False)
        all_data['Title'] = all_data['Title'].replace(['Lady', 'Countess','Capt', 'Col',
                                                     'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')
        all_data['Title'] = all_data['Title'].replace('Mlle', 'Miss')
        all_data['Title'] = all_data['Title'].replace('Ms', 'Miss')
        all_data['Title'] = all_data['Title'].replace('Mme', 'Mrs')
        
        # å¹´é½¢ã‚°ãƒ«ãƒ¼ãƒ—
        all_data['AgeGroup'] = pd.cut(all_data['Age'], bins=[0, 12, 18, 35, 60, 100], 
                                    labels=['Child', 'Teen', 'Adult', 'Middle', 'Senior'])
        
        # é‹è³ƒã‚°ãƒ«ãƒ¼ãƒ—
        all_data['FareGroup'] = pd.qcut(all_data['Fare'], q=4, labels=['Low', 'Medium', 'High', 'VeryHigh'])
        
        # 5. ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        # æ€§åˆ¥
        all_data['Sex_encoded'] = self.le_sex.fit_transform(all_data['Sex'])
        
        # ä¹—èˆ¹æ¸¯
        all_data['Embarked_encoded'] = self.le_embarked.fit_transform(all_data['Embarked'])
        
        # æ•¬ç§°ã‚’ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        title_dummies = pd.get_dummies(all_data['Title'], prefix='Title')
        all_data = pd.concat([all_data, title_dummies], axis=1)
        
        # å¹´é½¢ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        age_group_dummies = pd.get_dummies(all_data['AgeGroup'], prefix='AgeGroup')
        all_data = pd.concat([all_data, age_group_dummies], axis=1)
        
        # é‹è³ƒã‚°ãƒ«ãƒ¼ãƒ—ã‚’ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        fare_group_dummies = pd.get_dummies(all_data['FareGroup'], prefix='FareGroup')
        all_data = pd.concat([all_data, fare_group_dummies], axis=1)
        
        # 6. å­¦ç¿’ç”¨ã¨ãƒ†ã‚¹ãƒˆç”¨ã«åˆ†å‰²
        train_len = len(self.train_df)
        self.processed_train = all_data[:train_len].copy()
        self.processed_test = all_data[train_len:].copy()
        
        # 7. ç‰¹å¾´é‡é¸æŠ
        feature_columns = [
            'Pclass', 'Sex_encoded', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_encoded',
            'FamilySize', 'IsAlone'
        ] + [col for col in all_data.columns if col.startswith(('Title_', 'AgeGroup_', 'FareGroup_'))]
        
        self.X = self.processed_train[feature_columns]
        self.y = self.processed_train['Survived']
        self.X_test = self.processed_test[feature_columns]
        
        print(f"ç‰¹å¾´é‡æ•°: {len(feature_columns)}")
        print("ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡:", feature_columns)
        print("å‰å‡¦ç†å®Œäº†ï¼")
        
        return self.X, self.y, self.X_test
    
    def train_model(self):
        """ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
        print("\n=== ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹ ===")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’è¨“ç·´ç”¨ã¨æ¤œè¨¼ç”¨ã«åˆ†å‰²
        X_train, X_val, y_train, y_val = train_test_split(
            self.X, self.y, test_size=0.2, random_state=42, stratify=self.y
        )
        
        # è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™
        models = {
            'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=5),
            'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000)
        }
        
        best_score = 0
        best_model_name = ""
        
        for name, model in models.items():
            # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
            
            # è¨“ç·´
            model.fit(X_train, y_train)
            
            # æ¤œè¨¼
            val_pred = model.predict(X_val)
            val_accuracy = accuracy_score(y_val, val_pred)
            
            print(f"\n{name}:")
            print(f"  ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å¹³å‡: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
            print(f"  æ¤œè¨¼ç²¾åº¦: {val_accuracy:.4f}")
            
            if val_accuracy > best_score:
                best_score = val_accuracy
                best_model_name = name
                self.model = model
        
        print(f"\næœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {best_model_name} (ç²¾åº¦: {best_score:.4f})")
        
        # ç‰¹å¾´é‡é‡è¦åº¦ã®è¡¨ç¤ºï¼ˆRandomForestã®å ´åˆï¼‰
        if isinstance(self.model, RandomForestClassifier):
            feature_importance = pd.DataFrame({
                'feature': self.X.columns,
                'importance': self.model.feature_importances_
            }).sort_values('importance', ascending=False)
            
            print("\n=== ç‰¹å¾´é‡é‡è¦åº¦ Top 10 ===")
            print(feature_importance.head(10))
            
            # é‡è¦åº¦ã®å¯è¦–åŒ–
            plt.figure(figsize=(10, 6))
            sns.barplot(data=feature_importance.head(10), x='importance', y='feature')
            plt.title('ç‰¹å¾´é‡é‡è¦åº¦ Top 10')
            plt.xlabel('é‡è¦åº¦')
            plt.show()
        
        return self.model
    
    def evaluate_model(self):
        """ãƒ¢ãƒ‡ãƒ«è©•ä¾¡"""
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§å†åˆ†å‰²ã—ã¦è©³ç´°è©•ä¾¡
        X_train, X_val, y_train, y_val = train_test_split(
            self.X, self.y, test_size=0.2, random_state=42, stratify=self.y
        )
        
        val_pred = self.model.predict(X_val)
        
        print("\n=== ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ ===")
        print("ç²¾åº¦:", accuracy_score(y_val, val_pred))
        print("\nåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
        print(classification_report(y_val, val_pred))
        
        # æ··åŒè¡Œåˆ—
        plt.figure(figsize=(8, 6))
        cm = confusion_matrix(y_val, val_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.title('æ··åŒè¡Œåˆ—')
        plt.ylabel('å®Ÿéš›')
        plt.xlabel('äºˆæ¸¬')
        plt.show()
    
    def predict_and_submit(self, submission_filename='submission.csv'):
        """äºˆæ¸¬ã¨æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ"""
        print("\n=== äºˆæ¸¬å®Ÿè¡Œ ===")
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬
        test_predictions = self.model.predict(self.X_test)
        
        # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        submission = pd.DataFrame({
            'PassengerId': self.test_df['PassengerId'],
            'Survived': test_predictions
        })
        
        submission.to_csv(submission_filename, index=False)
        print(f"æå‡ºãƒ•ã‚¡ã‚¤ãƒ« '{submission_filename}' ã‚’ä½œæˆã—ã¾ã—ãŸï¼")
        
        # äºˆæ¸¬çµæœã®æ¦‚è¦
        survival_rate = test_predictions.mean()
        print(f"äºˆæ¸¬ç”Ÿå­˜ç‡: {survival_rate:.3f}")
        print(f"äºˆæ¸¬ç”Ÿå­˜è€…æ•°: {test_predictions.sum()}/{len(test_predictions)}")
        
        return submission

# ä½¿ç”¨ä¾‹
def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš¢ ã‚¿ã‚¤ã‚¿ãƒ‹ãƒƒã‚¯å·ç”Ÿå­˜äºˆæ¸¬ãƒ¢ãƒ‡ãƒ« ğŸš¢")
    print("=" * 50)
    
    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    predictor = TitanicPredictor()
    
    try:
        # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        train_df, test_df = predictor.load_data()
        
        # 2. ãƒ‡ãƒ¼ã‚¿æ¢ç´¢
        predictor.explore_data()
        
        # 3. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
        X, y, X_test = predictor.preprocess_data()
        
        # 4. ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        model = predictor.train_model()
        
        # 5. ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
        predictor.evaluate_model()
        
        # 6. äºˆæ¸¬ã¨æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        submission = predictor.predict_and_submit()
        
        print("\nğŸ‰ ãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†ï¼")
        print("submission.csvãƒ•ã‚¡ã‚¤ãƒ«ã‚’Kaggleã«æå‡ºã—ã¦ãã ã•ã„ã€‚")
        
    except FileNotFoundError:
        print("âŒ ã‚¨ãƒ©ãƒ¼: train.csvã¾ãŸã¯test.csvãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        print("Kaggleã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ã€åŒã˜ãƒ•ã‚©ãƒ«ãƒ€ã«é…ç½®ã—ã¦ãã ã•ã„ã€‚")
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        print("\nğŸ“ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ãƒ‡ãƒ¢ã‚’å®Ÿè¡Œã—ã¾ã™...")
        demo_with_sample_data()

def demo_with_sample_data():
    """ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆä¸­...")
    
    # ã‚µãƒ³ãƒ—ãƒ«å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ä½œæˆ
    np.random.seed(42)
    n_samples = 800
    
    sample_train = pd.DataFrame({
        'PassengerId': range(1, n_samples + 1),
        'Pclass': np.random.choice([1, 2, 3], n_samples, p=[0.2, 0.2, 0.6]),
        'Name': [f'Sample Name {i}' for i in range(n_samples)],
        'Sex': np.random.choice(['male', 'female'], n_samples, p=[0.65, 0.35]),
        'Age': np.random.normal(30, 12, n_samples),
        'SibSp': np.random.choice([0, 1, 2, 3], n_samples, p=[0.7, 0.2, 0.07, 0.03]),
        'Parch': np.random.choice([0, 1, 2], n_samples, p=[0.8, 0.15, 0.05]),
        'Ticket': [f'TICKET{i}' for i in range(n_samples)],
        'Fare': np.random.lognormal(3, 1, n_samples),
        'Cabin': [''] * n_samples,  # å¤§éƒ¨åˆ†ãŒæ¬ æ
        'Embarked': np.random.choice(['S', 'C', 'Q'], n_samples, p=[0.7, 0.2, 0.1])
    })
    
    # ç”Ÿå­˜ç‡ã‚’ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã§è¨­å®šï¼ˆå¥³æ€§ã€ä¸Šç´šã‚¯ãƒ©ã‚¹ã€è‹¥å¹´è€…ãŒç”Ÿå­˜ã—ã‚„ã™ã„ï¼‰
    survival_prob = 0.3  # ãƒ™ãƒ¼ã‚¹ç”Ÿå­˜ç‡
    survival_prob += (sample_train['Sex'] == 'female') * 0.4  # å¥³æ€§ãƒœãƒ¼ãƒŠã‚¹
    survival_prob += (sample_train['Pclass'] == 1) * 0.2  # 1ç­‰ã‚¯ãƒ©ã‚¹ãƒœãƒ¼ãƒŠã‚¹
    survival_prob += (sample_train['Age'] < 15) * 0.2  # å­ä¾›ãƒœãƒ¼ãƒŠã‚¹
    
    sample_train['Survived'] = np.random.binomial(1, survival_prob)
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    n_test = 200
    sample_test = pd.DataFrame({
        'PassengerId': range(n_samples + 1, n_samples + n_test + 1),
        'Pclass': np.random.choice([1, 2, 3], n_test, p=[0.2, 0.2, 0.6]),
        'Name': [f'Test Name {i}' for i in range(n_test)],
        'Sex': np.random.choice(['male', 'female'], n_test, p=[0.65, 0.35]),
        'Age': np.random.normal(30, 12, n_test),
        'SibSp': np.random.choice([0, 1, 2, 3], n_test, p=[0.7, 0.2, 0.07, 0.03]),
        'Parch': np.random.choice([0, 1, 2], n_test, p=[0.8, 0.15, 0.05]),
        'Ticket': [f'TEST_TICKET{i}' for i in range(n_test)],
        'Fare': np.random.lognormal(3, 1, n_test),
        'Cabin': [''] * n_test,
        'Embarked': np.random.choice(['S', 'C', 'Q'], n_test, p=[0.7, 0.2, 0.1])
    })
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    sample_train.to_csv('train.csv', index=False)
    sample_test.to_csv('test.csv', index=False)
    
    print("ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã—ã¾ã—ãŸ: train.csv, test.csv")
    
    # ãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œ
    predictor = TitanicPredictor()
    predictor.train_df = sample_train
    predictor.test_df = sample_test
    
    print("\n=== ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ã®å­¦ç¿’é–‹å§‹ ===")
    predictor.preprocess_data()
    predictor.train_model()
    predictor.evaluate_model()
    predictor.predict_and_submit('sample_submission.csv')

if __name__ == "__main__":
    main()
